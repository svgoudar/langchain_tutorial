{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59836631",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Custom Tools\n",
    "\n",
    "### What Custom Tools Are\n",
    "\n",
    "**Custom tools** are **user-defined functions** that you expose to an LLM/agent so it can **perform deterministic actions** such as:\n",
    "\n",
    "* Querying databases\n",
    "* Calling internal APIs\n",
    "* Running business logic\n",
    "* Reading files or metrics\n",
    "\n",
    "> Custom tools let the LLM **decide *when* to act**, while your code **controls *how* the action runs**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Custom Tools Are Needed\n",
    "\n",
    "LLMs alone cannot:\n",
    "\n",
    "* Access live systems\n",
    "* Perform reliable calculations\n",
    "* Enforce business rules\n",
    "* Execute side effects safely\n",
    "\n",
    "Custom tools enable:\n",
    "\n",
    "* Deterministic behavior\n",
    "* Enterprise integration\n",
    "* Auditable actions\n",
    "* Safer agent workflows\n",
    "\n",
    "---\n",
    "\n",
    "### Where Custom Tools Fit\n",
    "\n",
    "```\n",
    "User Input\n",
    "   ↓\n",
    "LLM / Agent (reasoning)\n",
    "   ↓\n",
    "Custom Tool (your code)\n",
    "   ↓\n",
    "Tool Result\n",
    "   ↓\n",
    "LLM Final Answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### What Makes a Tool “Custom”\n",
    "\n",
    "A custom tool is:\n",
    "\n",
    "* Written by **you**\n",
    "* Has a **clear purpose**\n",
    "* Uses a **strict input schema**\n",
    "* Returns a **deterministic output**\n",
    "\n",
    "---\n",
    "\n",
    "### Tool vs Function vs API\n",
    "\n",
    "| Concept     | Meaning                         |\n",
    "| ----------- | ------------------------------- |\n",
    "| Function    | Python function                 |\n",
    "| Tool        | Function + schema + description |\n",
    "| API         | Remote service                  |\n",
    "| Custom Tool | Your function exposed as a tool |\n",
    "\n",
    "LangChain turns **functions into tools**.\n",
    "\n",
    "---\n",
    "\n",
    "### How LangChain Understands a Tool\n",
    "\n",
    "Each tool has:\n",
    "\n",
    "1. **Name**\n",
    "2. **Description** (critical for tool selection)\n",
    "3. **Input schema**\n",
    "4. **Return value**\n",
    "\n",
    "The LLM sees **only the schema and description**, not your code.\n",
    "\n",
    "---\n",
    "\n",
    "### Method 1: Custom Tool Using `@tool` Decorator (Recommended)\n",
    "\n",
    "#### Step 1: Define the Tool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fc9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_classic.tools import tool\n",
    "\n",
    "@tool\n",
    "def ticket_count(source: str) -> int:\n",
    "    \"\"\"Return total number of tickets for a given source (e.g., jira, servicenow).\"\"\"\n",
    "    if source.lower() == \"jira\":\n",
    "        return 128\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23efef86",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "What LangChain derives automatically:\n",
    "\n",
    "* Tool name → `ticket_count`\n",
    "* Input schema → `{ source: string }`\n",
    "* Output type → `int`\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Bind Tool to LLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a676eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([ticket_count])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec23d2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c98ebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM:\n",
      "content='' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 64, 'total_tokens': 78, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_ee69c2ef48', 'id': 'chatcmpl-CpcKpxqkbNoiSx3kZs9kN9e6zmdnI', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--019b46b9-687b-7f83-a7e9-33922d7488a4-0' tool_calls=[{'name': 'ticket_count', 'args': {'source': 'jira'}, 'id': 'call_OZotasFbqQWbdJQ56TmvZFuI', 'type': 'tool_call'}] usage_metadata={'input_tokens': 64, 'output_tokens': 14, 'total_tokens': 78, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Tool calls: [{'name': 'ticket_count', 'args': {'source': 'jira'}, 'id': 'call_OZotasFbqQWbdJQ56TmvZFuI', 'type': 'tool_call'}]\n",
      "\n",
      "Tool result: 128\n",
      "\n",
      "Final answer: There are 128 tickets in Jira.\n"
     ]
    }
   ],
   "source": [
    "# First invoke - LLM decides to call the tool\n",
    "response = llm_with_tools.invoke(\n",
    "    \"How many tickets are there in Jira?\"\n",
    ")\n",
    "\n",
    "print(\"Response from LLM:\")\n",
    "print(response)\n",
    "print(\"\\nTool calls:\", response.tool_calls)\n",
    "\n",
    "# Execute the tool manually\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "tool_call = response.tool_calls[0]\n",
    "tool_result = ticket_count.invoke(tool_call[\"args\"])\n",
    "\n",
    "print(f\"\\nTool result: {tool_result}\")\n",
    "\n",
    "# Send tool result back to LLM for final answer\n",
    "messages = [\n",
    "    (\"human\", \"How many tickets are there in Jira?\"),\n",
    "    response,\n",
    "    ToolMessage(\n",
    "        content=str(tool_result),\n",
    "        tool_call_id=tool_call[\"id\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "final_response = llm_with_tools.invoke(messages)\n",
    "print(f\"\\nFinal answer: {final_response.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d813e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Easier Approach: Use an Agent\n",
    "\n",
    "Instead of manually handling tool calls, use an agent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d136d2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sangouda\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ticket_count` with `{'source': 'jira'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m128\u001b[0m\u001b[32;1m\u001b[1;3mThere are 128 tickets in Jira.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Final output: There are 128 tickets in Jira.\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create agent prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Use tools when needed.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")\n",
    "])\n",
    "\n",
    "# Create agent\n",
    "agent = create_tool_calling_agent(llm, [ticket_count], prompt)\n",
    "\n",
    "# Create executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=[ticket_count],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Now just invoke - tool execution is automatic!\n",
    "result = agent_executor.invoke({\n",
    "    \"input\": \"How many tickets are there in Jira?\"\n",
    "})\n",
    "\n",
    "print(f\"\\nFinal output: {result['output']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f4b94",
   "metadata": {},
   "source": [
    "**Key Difference:**\n",
    "\n",
    "* `bind_tools()` → LLM requests tool calls (you handle execution)\n",
    "* `AgentExecutor` → Automatic tool execution loop\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
