{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b609845c",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Self-Reflection in LLM Systems \n",
    "\n",
    "\n",
    "**Self-reflection** is a mechanism where an AI system **evaluates its own output**, detects weaknesses or errors, and then **improves the response before final delivery**.\n",
    "\n",
    "Instead of:\n",
    "\n",
    "```\n",
    "Generate → Return\n",
    "```\n",
    "\n",
    "It becomes:\n",
    "\n",
    "```\n",
    "Generate → Critique → Revise → Validate → Return\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why Self-Reflection Matters\n",
    "\n",
    "| Without Self-Reflection | With Self-Reflection |\n",
    "| ----------------------- | -------------------- |\n",
    "| More hallucinations     | Fewer hallucinations |\n",
    "| Shallow answers         | Deeper reasoning     |\n",
    "| Higher error rate       | Higher accuracy      |\n",
    "| Inconsistent quality    | Stable quality       |\n",
    "\n",
    "---\n",
    "\n",
    "### Self-Reflection Architecture\n",
    "\n",
    "```\n",
    "User Query\n",
    "   ↓\n",
    "Draft Generator\n",
    "   ↓\n",
    "Critic / Reviewer\n",
    "   ↓\n",
    "Improver\n",
    "   ↓\n",
    "Final Answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Core Reflection Roles\n",
    "\n",
    "| Agent     | Role                    |\n",
    "| --------- | ----------------------- |\n",
    "| Generator | Produces initial answer |\n",
    "| Critic    | Identifies issues       |\n",
    "| Refiner   | Fixes problems          |\n",
    "| Verifier  | Confirms correctness    |\n",
    "\n",
    "---\n",
    "\n",
    "### Self-Reflection Implementation\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "def generate_answer(query):\n",
    "    return llm.invoke(query).content\n",
    "\n",
    "def critique(answer):\n",
    "    review_prompt = f\"Review the following answer and list errors or gaps:\\n{answer}\"\n",
    "    return llm.invoke(review_prompt).content\n",
    "\n",
    "def refine(answer, critique):\n",
    "    improve_prompt = f\"Improve the answer based on this critique:\\n{critique}\\n\\nOriginal:\\n{answer}\"\n",
    "    return llm.invoke(improve_prompt).content\n",
    "\n",
    "def self_reflect(query):\n",
    "    draft = generate_answer(query)\n",
    "    feedback = critique(draft)\n",
    "    final = refine(draft, feedback)\n",
    "    return final\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Verification Layer\n",
    "\n",
    "```python\n",
    "def verify(answer):\n",
    "    check = llm.invoke(f\"Verify factual correctness:\\n{answer}\")\n",
    "    return check.content\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Applied Example\n",
    "\n",
    "```python\n",
    "response = self_reflect(\"Explain quantum computing\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Benefits in RAG & Agents\n",
    "\n",
    "| Area                 | Impact                 |\n",
    "| -------------------- | ---------------------- |\n",
    "| RAG                  | Reduces hallucinations |\n",
    "| Agents               | Improves planning      |\n",
    "| Multi-step reasoning | More robust            |\n",
    "| Safety               | Catches violations     |\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "Self-Reflection = AI proofreading its own work\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Dramatically improves output quality\n",
    "* Adds minimal cost relative to benefits\n",
    "* Essential for high-reliability AI systems\n",
    "* Core building block for agentic workflows"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
