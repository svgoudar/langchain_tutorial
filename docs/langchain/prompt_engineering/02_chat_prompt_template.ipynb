{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "491d9c95",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate (LangChain)\n",
    "\n",
    "\n",
    "In **LangChain**, a **ChatPromptTemplate** is a **structured prompt abstraction** designed for **chat-based LLMs**.\n",
    "\n",
    "> It converts structured inputs into a **sequence of role-based messages** (`system`, `human`, `assistant`) that are sent to a ChatModel.\n",
    "\n",
    "This is the **primary and recommended prompt type** for all modern LangChain systems.\n",
    "\n",
    "---\n",
    "\n",
    "### Why ChatPromptTemplate Exists\n",
    "\n",
    "Chat models require:\n",
    "\n",
    "* Role separation\n",
    "* Instruction hierarchy\n",
    "* Tool awareness\n",
    "* Memory injection\n",
    "\n",
    "Raw strings cannot express this reliably.\n",
    "\n",
    "ChatPromptTemplate:\n",
    "\n",
    "* Preserves message roles\n",
    "* Enables agents and tool calling\n",
    "* Supports memory and RAG\n",
    "* Composes cleanly with LCEL\n",
    "\n",
    "---\n",
    "\n",
    "### ChatPromptTemplate vs PromptTemplate\n",
    "\n",
    "#### Key Difference\n",
    "\n",
    "| Aspect       | PromptTemplate | ChatPromptTemplate  |\n",
    "| ------------ | -------------- | ------------------- |\n",
    "| Input        | Plain text     | Structured messages |\n",
    "| Output       | String         | List of messages    |\n",
    "| Agents       | ❌              | ✅                   |\n",
    "| Tool calling | ❌              | ✅                   |\n",
    "| Memory       | ❌              | ✅                   |\n",
    "| RAG          | Limited        | Native              |\n",
    "\n",
    "---\n",
    "\n",
    "### Basic ChatPromptTemplate Demonstration\n",
    "\n",
    "#### Creating a ChatPromptTemplate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad416144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6abc8e",
   "metadata": {},
   "source": [
    "### Formatting Messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b51eb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You are a helpful assistant.' additional_kwargs={} response_metadata={}\n",
      "content='What is LangChain?' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = prompt.format_messages(\n",
    "    input=\"What is LangChain?\"\n",
    ")\n",
    "\n",
    "for msg in messages:\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68b2ab6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "SystemMessage(...)\n",
    "HumanMessage(...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ChatPromptTemplate with Chat Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70e601c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG stands for Retrieve and Generate, which is a framework commonly used in natural language processing (NLP) for tasks such as question answering and dialogue generation. It combines two key capabilities of AI models: retrieval and generation.\n",
      "\n",
      "1. **Retrieve**: This component involves searching through a large database or corpus of text to find relevant information that can answer a specific question or provide context for a conversation. This is often done using techniques like vector similarity, where queries are compared to a set of documents to find the most relevant matches.\n",
      "\n",
      "2. **Generate**: After retrieving the relevant information, the model generates a response based on the retrieved data. This is typically done using a generative model, such as a sequence-to-sequence model or transformer architecture, which can formulate coherent and contextually appropriate responses.\n",
      "\n",
      "### Advantages of the RAG Framework:\n",
      "- **Improved Accuracy**: By combining retrieval and generation, RAG can produce more accurate responses, as it grounds its answers in existing knowledge and data rather than relying solely on its pre-trained knowledge.\n",
      "- **Dynamism**: It can adapt to new information more easily, as the retrieval component allows it to access up-to-date data that may not have been part of the training set.\n",
      "- **Efficiency**: RAG can handle a wide array of queries more effectively by leveraging a mixture of fixed information (retrieved) and dynamic generation.\n",
      "\n",
      "### Use Cases:\n",
      "- **Question Answering Systems**: RAG can be employed in chatbots or virtual assistants that need to provide users with accurate answers based on large knowledge bases.\n",
      "- **Summarization**: In applications where concise overviews of larger texts are needed, RAG can first pull relevant sections and then generate a summary.\n",
      "- **Interactive Dialogue Systems**: It’s used in conversational AI to create more engaging and relevant dialogue by grounding responses in retrieved facts.\n",
      "\n",
      "In summary, RAG combines the strengths of information retrieval and text generation to create robust and contextually aware AI systems capable of providing well-informed responses in various applications.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"Explain RAG\"})\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802367d9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Variables and Validation\n",
    "\n",
    "#### Declaring Variables Implicitly\n",
    "\n",
    "```python\n",
    "(\"human\", \"{question}\")\n",
    "```\n",
    "\n",
    "LangChain automatically:\n",
    "\n",
    "* Detects required variables\n",
    "* Raises error if missing\n",
    "\n",
    "---\n",
    "\n",
    "### Partial Variables (Static Instructions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4db832dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG stands for \"Red, Amber, Green,\" and it is a color-coding system often used for project management, reporting, and performance monitoring. Each color represents a status or level of progress:\n",
      "\n",
      "1. **Red**: Indicates significant issues that need immediate attention. This status suggests that a project or task is off track, facing serious risks, or not meeting its objectives.\n",
      "\n",
      "2. **Amber** (or Yellow): Represents caution. This status indicates that while the project or task is not currently in jeopardy, there are potential issues or concerns that could escalate if not addressed. It may require monitoring or some corrective actions.\n",
      "\n",
      "3. **Green**: Signifies that everything is on track. The project or task is proceeding as planned, meeting its deadlines, and objectives are being achieved.\n",
      "\n",
      "The RAG status is often presented in the form of dashboards or reports to provide a quick visual reference for stakeholders to understand the health of a project or initiative. It allows for easier decision-making and prioritization of resources based on the urgency of issues indicated by each color. \n",
      "\n",
      "In addition to project management, RAG ratings may also be used in various fields, including finance, risk assessment, and performance evaluation, to convey the urgency or health of various activities or metrics.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        (\"system\", \"{role}\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ],\n",
    "    partial_variables={\n",
    "        \"role\": \"You are an IT support assistant.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"question\": \"Explain RAG\"})\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45388186",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### MessagesPlaceholder (Critical Concept)\n",
    "\n",
    "#### Injecting Memory or Scratchpad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a60568ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c89fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Used for:\n",
    "\n",
    "* Conversation memory\n",
    "* Agent scratchpads\n",
    "* Tool call history\n",
    "\n",
    "---\n",
    "\n",
    "### ChatPromptTemplate in Agents\n",
    "\n",
    "#### Required Agent Parameters\n",
    "\n",
    "For OpenAI tools agents:\n",
    "\n",
    "* `{input}` – user query\n",
    "* `{agent_scratchpad}` – tool call history\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Use tools if needed.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"assistant\", \"{agent_scratchpad}\")\n",
    "])\n",
    "```\n",
    "\n",
    "⚠️ `agent_scratchpad` must be a MessagesPlaceholder or assistant slot.\n",
    "\n",
    "---\n",
    "\n",
    "### ChatPromptTemplate in RAG\n",
    "\n",
    "#### Context Injection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "918227a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: Answer using the context only.\\nHuman: Context:\\nLangChain is a framework for building applications with LLMs.\\n\\nQuestion:\\nWhat is LangChain?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer using the context only.\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{input}\")\n",
    "])\n",
    "\n",
    "prompt.format(\n",
    "    context=\"LangChain is a framework for building applications with LLMs.\",\n",
    "    input=\"What is LangChain?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ff57a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Retriever output populates `{context}`.\n",
    "\n",
    "---\n",
    "\n",
    "### ChatPromptTemplate with Structured Output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f4a3f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category='Service Outage' priority='High'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Ticket(BaseModel):\n",
    "    category: str\n",
    "    priority: str\n",
    "\n",
    "structured_llm = llm.with_structured_output(Ticket)\n",
    "\n",
    "chain = prompt | structured_llm\n",
    "\n",
    "result = chain.invoke({\"input\": \"Email service is down\", \"context\": \"The email service is experiencing an outage affecting multiple users.\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e65443",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Advanced Composition with LCEL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3273958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"question\"]\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d355a5e4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ChatPromptTemplate behaves as a Runnable.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "#### Using PromptTemplate for chat models\n",
    "\n",
    "❌ Breaks agents and tools\n",
    "\n",
    "#### Missing required variables\n",
    "\n",
    "❌ Runtime KeyError\n",
    "\n",
    "#### Hard-coding chat history\n",
    "\n",
    "❌ Breaks memory\n",
    "\n",
    "#### Referencing internal state directly\n",
    "\n",
    "❌ Agent failures\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Always use ChatPromptTemplate for new systems\n",
    "* Keep system prompts concise\n",
    "* Inject dynamic content via placeholders\n",
    "* Never expose chain-of-thought\n",
    "* Version prompts like code\n",
    "\n",
    "---\n",
    "\n",
    "### When NOT to Use ChatPromptTemplate\n",
    "\n",
    "* Legacy text-only models\n",
    "* Simple batch text generation\n",
    "* Non-chat inference pipelines\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “ChatPromptTemplate is LangChain’s primary prompt abstraction for chat-based LLMs. It produces structured, role-based messages, enabling agents, tools, memory, and RAG pipelines while enforcing input validation and composability.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **Chat / RAG / Agents → ChatPromptTemplate**\n",
    "* **Text-only → PromptTemplate**\n",
    "* **Production → Always structured prompts**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25d65b7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
