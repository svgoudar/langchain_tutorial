{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468a780a",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Chain-of-Thought (CoT) \n",
    "\n",
    "\n",
    "**Chain-of-Thought (CoT)** is a prompting technique where the model is encouraged to **reason step by step** before producing a final answer.\n",
    "\n",
    "> The goal is to improve **reasoning quality**, not to expose reasoning verbatim.\n",
    "\n",
    "From a **LangChain perspective**, CoT is **controlled reasoning orchestration**, not free-form explanation dumping.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Chain-of-Thought Exists\n",
    "\n",
    "LLMs struggle with:\n",
    "\n",
    "* Multi-step logic\n",
    "* Math and reasoning\n",
    "* Conditional decisions\n",
    "\n",
    "CoT improves:\n",
    "\n",
    "* Accuracy\n",
    "* Logical consistency\n",
    "* Complex task handling\n",
    "\n",
    "---\n",
    "\n",
    "### Critical Safety Rule (Very Important)\n",
    "\n",
    "> **Do NOT expose raw chain-of-thought to users in production.**\n",
    "\n",
    "LangChain follows **reasoning safety best practices**:\n",
    "\n",
    "* Reason internally\n",
    "* Return **concise answers**\n",
    "* Optionally return **summaries of reasoning**\n",
    "\n",
    "---\n",
    "\n",
    "### Chain-of-Thought vs Explanation\n",
    "\n",
    "| Concept          | Purpose                   |\n",
    "| ---------------- | ------------------------- |\n",
    "| Chain-of-Thought | Internal reasoning        |\n",
    "| Explanation      | User-facing justification |\n",
    "\n",
    "LangChain encourages **hidden CoT + summarized output**.\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Chain-of-Thought\n",
    "\n",
    "### Zero-shot CoT\n",
    "\n",
    "Encourages reasoning without examples.\n",
    "\n",
    "```text\n",
    "Let's think step by step.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Few-shot CoT\n",
    "\n",
    "Provides examples **with reasoning steps**.\n",
    "\n",
    "---\n",
    "\n",
    "### Structured CoT\n",
    "\n",
    "Reasoning is guided but **not exposed**.\n",
    "\n",
    "---\n",
    "\n",
    "### Zero-shot CoT Demonstration (Simple)\n",
    "\n",
    "#### Prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b580e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the total time for 3 tickets when each ticket takes 2 hours, we can follow these steps:\n",
      "\n",
      "1. Determine the time for one ticket: 2 hours.\n",
      "2. Multiply the time for one ticket by the number of tickets: \n",
      "   \\( 2 \\text{ hours/ticket} \\times 3 \\text{ tickets} = 6 \\text{ hours} \\).\n",
      "\n",
      "Final answer: 6 hours.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a logical reasoning assistant.\"),\n",
    "    (\"human\", \"Solve the problem step by step, then give the final answer only.\\n\\nQuestion: {input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"If a ticket takes 2 hours and there are 3 tickets, how long?\"})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924ea31",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Model Behavior\n",
    "\n",
    "* Performs internal reasoning\n",
    "* Outputs final answer only\n",
    "\n",
    "---\n",
    "\n",
    "### Few-shot Chain-of-Thought Demonstration\n",
    "\n",
    "#### Prompt with Examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd313263",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You solve problems logically.\"),\n",
    "    (\"human\", \"If one ticket takes 1 hour and there are 2 tickets, how long?\"),\n",
    "    (\"assistant\", \"First determine time per ticket. Then multiply by count. Final answer: 2 hours.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1722bb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This teaches:\n",
    "\n",
    "* Reasoning pattern\n",
    "* Answer structure\n",
    "\n",
    "---\n",
    "\n",
    "### Chain-of-Thought with Output Parsers (Best Practice)\n",
    "\n",
    "#### Reason Internally, Return Structured Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f56b4a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result='5h'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    result: str\n",
    "\n",
    "structured_llm = llm.with_structured_output(Answer)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Reason internally. Return only the final result.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | structured_llm\n",
    "\n",
    "result = chain.invoke({\"input\": \"If SLA is 4h and delay is 1h, total?\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d60ff3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "No reasoning leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### Chain-of-Thought in Agents\n",
    "\n",
    "### Internal Agent Reasoning\n",
    "\n",
    "Agents **always use CoT internally**:\n",
    "\n",
    "```\n",
    "Thought → Tool → Observation → Thought → Answer\n",
    "```\n",
    "\n",
    "But LangChain:\n",
    "\n",
    "* Stores it in `intermediate_steps`\n",
    "* Does NOT expose it by default\n",
    "\n",
    "---\n",
    "\n",
    "### Chain-of-Thought in LangGraph\n",
    "\n",
    "LangGraph nodes:\n",
    "\n",
    "* Perform reasoning internally\n",
    "* Pass state forward\n",
    "* Return only safe outputs\n",
    "\n",
    "This is the **production-grade CoT approach**.\n",
    "\n",
    "---\n",
    "\n",
    "### What NOT to Do (Anti-patterns)\n",
    "\n",
    "- Exposing raw CoT\n",
    "\n",
    "```text\n",
    "Here is my reasoning step by step...\n",
    "```\n",
    "\n",
    "- Logging CoT to users\n",
    "\n",
    "Security + compliance risk.\n",
    "\n",
    "- Relying on CoT for determinism\n",
    "\n",
    "Use **structured outputs** instead.\n",
    "\n",
    "---\n",
    "\n",
    "### Correct Production Pattern\n",
    "\n",
    "```\n",
    "User Input\n",
    "  ↓\n",
    "Hidden CoT\n",
    "  ↓\n",
    "Validation / Parsing\n",
    "  ↓\n",
    "Concise Answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Chain-of-Thought\n",
    "\n",
    "| Scenario            | Use CoT           |\n",
    "| ------------------- | ----------------- |\n",
    "| Math / logic        | Yes               |\n",
    "| Planning            | Yes               |\n",
    "| Agents              | Always (internal) |\n",
    "| Simple Q&A          | No                |\n",
    "| RAG factual answers | Minimal           |\n",
    "\n",
    "---\n",
    "\n",
    "### Chain-of-Thought vs Related Techniques\n",
    "\n",
    "### CoT vs ReAct\n",
    "\n",
    "| Aspect           | CoT      | ReAct       |\n",
    "| ---------------- | -------- | ----------- |\n",
    "| Reasoning        | Internal | Interleaved |\n",
    "| Tools            | ❌        | ✅           |\n",
    "| LangChain agents | Internal | Preferred   |\n",
    "\n",
    "---\n",
    "\n",
    "### CoT vs Tree of Thoughts\n",
    "\n",
    "| Aspect     | CoT    | ToT      |\n",
    "| ---------- | ------ | -------- |\n",
    "| Paths      | Single | Multiple |\n",
    "| Cost       | Lower  | Higher   |\n",
    "| Complexity | Medium | High     |\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices (LangChain)\n",
    "\n",
    "* Let the model reason internally\n",
    "* Use structured outputs\n",
    "* Use agents for tool reasoning\n",
    "* Never expose raw reasoning\n",
    "* Log reasoning only for debugging (restricted)\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “Chain-of-thought is an internal reasoning technique that improves LLM accuracy on multi-step problems. In LangChain, CoT is used implicitly in agents and reasoning chains, while only concise, safe outputs are returned to users.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **Reason internally**\n",
    "* **Respond concisely**\n",
    "* **Structure outputs**\n",
    "* **Never leak chain-of-thought**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
