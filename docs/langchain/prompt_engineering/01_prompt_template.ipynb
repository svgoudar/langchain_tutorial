{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d103d68a",
   "metadata": {},
   "source": [
    "## PromptTemplate (LangChain)\n",
    "\n",
    "\n",
    "In **LangChain**, a **PromptTemplate** is a **parameterized text prompt** designed for **text-completion style models**.\n",
    "\n",
    "> It converts structured inputs into a **single formatted string** that is sent to an LLM.\n",
    "\n",
    "This is the **foundational prompt abstraction**, but it is **legacy-oriented** compared to `ChatPromptTemplate`.\n",
    "\n",
    "---\n",
    "\n",
    "### Why PromptTemplate Exists\n",
    "\n",
    "PromptTemplate was created to:\n",
    "\n",
    "* Avoid hard-coded prompt strings\n",
    "* Enforce input variables\n",
    "* Enable reuse and testing\n",
    "* Standardize prompt formatting\n",
    "\n",
    "It is still useful for:\n",
    "\n",
    "* Legacy LLMs\n",
    "* Simple, deterministic pipelines\n",
    "* Non-chat use cases\n",
    "\n",
    "---\n",
    "\n",
    "### PromptTemplate vs ChatPromptTemplate\n",
    "\n",
    "###  Key Difference\n",
    "\n",
    "| Aspect       | PromptTemplate | ChatPromptTemplate  |\n",
    "| ------------ | -------------- | ------------------- |\n",
    "| Input        | Plain text     | Structured messages |\n",
    "| Output       | String         | List of messages    |\n",
    "| Agents       | ❌              | ✅                   |\n",
    "| Tool calling | ❌              | ✅                   |\n",
    "| Modern usage | Limited        | Recommended         |\n",
    "\n",
    "> **Use PromptTemplate only when your model expects plain text.**\n",
    "\n",
    "---\n",
    "\n",
    "### Basic PromptTemplate Demonstration\n",
    "\n",
    "###  Creating a PromptTemplate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a3481e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain RAG in one sentence.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Explain {topic} in one sentence.\"\n",
    ")\n",
    "prompt.format(topic=\"RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3794650b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  Formatting the Prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78d6af1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain RAG in one sentence.\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = prompt.format(topic=\"RAG\")\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d5cade",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Explain RAG in one sentence.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### PromptTemplate with LLM\n",
    "\n",
    "####  Simple Execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0499f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LangChain is a decentralized platform that allows users to create and manage their own blockchain-based languages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"topic\": \"LangChain\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325be6f6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here:\n",
    "\n",
    "* PromptTemplate → string\n",
    "* LLM → string\n",
    "\n",
    "---\n",
    "\n",
    "### Input Variables\n",
    "\n",
    "####  Declaring Variables Explicitly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b3bea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Severity: {severity}\\nIssue: {issue}\",\n",
    "    input_variables=[\"severity\", \"issue\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da2bd764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Severity: High\\nIssue: API not responding'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(severity=\"High\", issue=\"API not responding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eede030",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "LangChain validates:\n",
    "\n",
    "* Missing variables → error\n",
    "* Extra variables → ignored\n",
    "\n",
    "---\n",
    "\n",
    "### Partial Variables\n",
    "\n",
    "####  Static Instructions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feae54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "        template=\"{role}\\nQuestion: {question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\n",
    "        \"role\": \"You are a cybersecurity expert.\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3e20c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Used for:\n",
    "\n",
    "* Environment-specific instructions\n",
    "* Reusable system context\n",
    "\n",
    "---\n",
    "\n",
    "### PromptTemplate in Classification Tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6aaf05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nHigh'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Classify severity (Low, Medium, High): {text}\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"text\": \"Database is down\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7357653",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### PromptTemplate + Output Parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5302e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09723f42",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Parsing is **manual** for PromptTemplate pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations of PromptTemplate\n",
    "\n",
    "**What PromptTemplate Cannot Do**\n",
    "\n",
    "* ❌ Message roles (system / human / assistant)\n",
    "* ❌ Tool calling\n",
    "* ❌ Agents\n",
    "* ❌ Memory injection\n",
    "* ❌ Structured outputs without extra work\n",
    "\n",
    "These limitations are why `ChatPromptTemplate` exists.\n",
    "\n",
    "---\n",
    "\n",
    "**When PromptTemplate Is Still Appropriate**\n",
    "\n",
    "* Legacy completion models\n",
    "* One-off scripts\n",
    "* Batch jobs\n",
    "* Deterministic classification\n",
    "* Minimal dependencies\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "**Using PromptTemplate with chat models**\n",
    "\n",
    "❌ Causes silent failures or poor behavior\n",
    "\n",
    "**Expecting agent features**\n",
    "\n",
    "❌ Not supported\n",
    "\n",
    "**Hard-coding variables**\n",
    "\n",
    "❌ Reduces reusability\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Keep PromptTemplate **simple**\n",
    "* Validate variables explicitly\n",
    "* Version prompts like code\n",
    "* Prefer ChatPromptTemplate for new systems\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “PromptTemplate in LangChain is a legacy text-based prompt abstraction that formats structured inputs into a single string for completion-style models. It is useful for simple pipelines but is superseded by ChatPromptTemplate for modern, agentic systems.”\n",
    "\n",
    "---\n",
    "\n",
    "###  Rule of Thumb\n",
    "\n",
    "* **Text-only model → PromptTemplate**\n",
    "* **Chat / RAG / Agents → ChatPromptTemplate**\n",
    "* **Production systems → Avoid PromptTemplate unless required**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
