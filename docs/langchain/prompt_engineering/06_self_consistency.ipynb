{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d584c9",
   "metadata": {},
   "source": [
    "## Self-Consistency (LangChain Perspective)\n",
    "\n",
    "\n",
    "**Self-consistency** is a reasoning technique where **multiple independent reasoning paths** are generated for the same input, and the **most consistent final answer** is selected.\n",
    "\n",
    "> Instead of trusting one chain-of-thought, we sample **many chains** and aggregate their conclusions.\n",
    "\n",
    "In LangChain, self-consistency is implemented as a **controlled orchestration pattern**, not a native model feature.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Self-Consistency Exists\n",
    "\n",
    "Single model generations can be:\n",
    "\n",
    "* Overconfident\n",
    "* Incorrect due to sampling noise\n",
    "* Sensitive to prompt phrasing\n",
    "\n",
    "Self-consistency improves:\n",
    "\n",
    "* Accuracy on reasoning tasks\n",
    "* Robustness\n",
    "* Confidence calibration\n",
    "\n",
    "---\n",
    "\n",
    "### Self-Consistency vs Chain-of-Thought\n",
    "\n",
    "| Aspect          | Chain-of-Thought | Self-Consistency |\n",
    "| --------------- | ---------------- | ---------------- |\n",
    "| Reasoning paths | Single           | Multiple         |\n",
    "| Sampling        | One              | Many             |\n",
    "| Accuracy        | Medium           | Higher           |\n",
    "| Cost            | Lower            | Higher           |\n",
    "\n",
    "Self-consistency **builds on CoT**.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Idea (Conceptual Flow)\n",
    "\n",
    "```\n",
    "Input\n",
    " ↓\n",
    "Generate N reasoning paths (temperature > 0)\n",
    " ↓\n",
    "Extract final answers\n",
    " ↓\n",
    "Majority vote / aggregation\n",
    " ↓\n",
    "Final answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Self-Consistency Without Reasoning Leakage\n",
    "\n",
    "> **Do not expose raw reasoning.**\n",
    "> Only extract the **final answers** for voting.\n",
    "\n",
    "---\n",
    "\n",
    "### Manual Self-Consistency Demonstration\n",
    "\n",
    "#### Step 1: Prompt for Internal Reasoning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de0149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Solve the problem internally. Return only the final answer.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed18e4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Generate Multiple Samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1649725e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6 hours.', '6 hours.', '6 hours.', 'The total time for 3 tickets is 6 hours.', '6 hours.']\n"
     ]
    }
   ],
   "source": [
    "def generate_answers(question, k=5):\n",
    "    answers = []\n",
    "    for _ in range(k):\n",
    "        response = llm.invoke(prompt.format_messages(input=question))\n",
    "        answers.append(response.content.strip())\n",
    "    return answers\n",
    "\n",
    "samples = generate_answers(\n",
    "    \"If a ticket takes 2 hours and there are 3 tickets, how long?\",\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340a781",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Example output:\n",
    "\n",
    "```text\n",
    "['6 hours', '6 hours', '5 hours', '6 hours', '6 hours']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Majority Vote\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1da930b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 hours.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "final_answer = Counter(samples).most_common(1)[0][0]\n",
    "print(final_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d31e67f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "```text\n",
    "6 hours\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Self-Consistency Using LCEL (Cleaner)\n",
    "\n",
    "#### Runnable-based Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4178f817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If the Service Level Agreement (SLA) is 4 hours and there is a delay of 1 hour, you can think of it in terms of how much time is left within the SLA after the delay.\\n\\n1. **Original SLA**: 4 hours\\n2. **Delay**: 1 hour\\n\\nTo find the total time remaining after the delay, you would subtract the delay from the SLA:\\n\\n\\\\[ \\\\text{Remaining Time} = \\\\text{SLA} - \\\\text{Delay} = 4 \\\\text{ hours} - 1 \\\\text{ hour} = 3 \\\\text{ hours} \\\\]\\n\\nSo, after a 1-hour delay, there would be 3 hours remaining within the SLA. However, if you're asking for the total time considering the delay, then it would still be 4 hours, as the SLA is defined as the total time allowed, regardless of delays.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def self_consistent(question):\n",
    "    samples = llm.batch([question] * 5)\n",
    "    answers = [r.content for r in samples]\n",
    "    return Counter(answers).most_common(1)[0][0]\n",
    "\n",
    "sc_chain = RunnableLambda(self_consistent)\n",
    "\n",
    "sc_chain.invoke(\"If SLA is 4h and delay is 1h, total?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af88906",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Self-Consistency with Structured Output (Best Practice)\n",
    "\n",
    "#### Extract Only Final Answer Field\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216d9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Result(BaseModel):\n",
    "    answer: str\n",
    "\n",
    "structured_llm = llm.with_structured_output(Result)\n",
    "\n",
    "def structured_self_consistency(question):\n",
    "    results = structured_llm.batch([question] * 5)\n",
    "    answers = [r.answer for r in results]\n",
    "    return Counter(answers).most_common(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25468d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This avoids:\n",
    "\n",
    "* Parsing errors\n",
    "* Reasoning leakage\n",
    "\n",
    "---\n",
    "\n",
    "### Self-Consistency in Agents\n",
    "\n",
    "#### When Agents Use Self-Consistency\n",
    "\n",
    "* Planning agents\n",
    "* Decision-making agents\n",
    "* Confidence-critical workflows\n",
    "\n",
    "LangChain typically combines:\n",
    "\n",
    "* Self-consistency\n",
    "* Tool verification\n",
    "* Validation steps\n",
    "\n",
    "---\n",
    "\n",
    "### Self-Consistency in RAG\n",
    "\n",
    "Self-consistency helps:\n",
    "\n",
    "* Reduce hallucinations\n",
    "* Verify answers against context\n",
    "* Improve factual robustness\n",
    "\n",
    "Example strategy:\n",
    "\n",
    "* Generate multiple grounded answers\n",
    "* Discard inconsistent ones\n",
    "\n",
    "---\n",
    "\n",
    "### Cost and Latency Trade-offs\n",
    "\n",
    "| Factor      | Impact        |\n",
    "| ----------- | ------------- |\n",
    "| Cost        | k × tokens    |\n",
    "| Latency     | k × inference |\n",
    "| Accuracy    | Improves      |\n",
    "| Determinism | Improves      |\n",
    "\n",
    "Use selectively.\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Self-Consistency\n",
    "\n",
    "| Scenario             | Recommended |\n",
    "| -------------------- | ----------- |\n",
    "| Math / logic         | Yes         |\n",
    "| Planning             | Yes         |\n",
    "| Safety-critical      | Yes         |\n",
    "| Simple Q&A           | No          |\n",
    "| High-throughput APIs | No          |\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "- Using temperature = 0\n",
    "\n",
    "❌ No diversity → no benefit\n",
    "\n",
    "- Exposing reasoning\n",
    "\n",
    "❌ Security risk\n",
    "\n",
    "- Large k unnecessarily\n",
    "\n",
    "❌ High cost\n",
    "\n",
    "- Using self-consistency everywhere\n",
    "\n",
    "❌ Overkill\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Use `temperature ≈ 0.6–0.8`\n",
    "* Use `k = 3–5`\n",
    "* Aggregate final answers only\n",
    "* Combine with structured outputs\n",
    "* Log results internally only\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “Self-consistency improves LLM reasoning by sampling multiple independent reasoning paths and selecting the most frequent final answer. In LangChain, it is implemented as an orchestration pattern using batching and aggregation, not as a model parameter.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **Reasoning task → Self-consistency**\n",
    "* **Simple task → Single pass**\n",
    "* **Production → Structured aggregation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d46565",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
