{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5cad706",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Guardrails (LangChain Perspective)\n",
    "\n",
    "\n",
    "**Guardrails** are **controls applied before, during, and after LLM execution** to ensure outputs are **safe, correct, compliant, and reliable**.\n",
    "\n",
    "> Guardrails do not improve creativity; they **reduce risk**.\n",
    "\n",
    "In LangChain, guardrails are implemented as **composable layers** using prompts, validators, output parsers, retries, filters, and routing.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Guardrails Are Necessary\n",
    "\n",
    "LLMs can:\n",
    "\n",
    "* Hallucinate facts\n",
    "* Ignore instructions\n",
    "* Leak sensitive data\n",
    "* Produce unsafe content\n",
    "* Return invalid formats\n",
    "\n",
    "Guardrails:\n",
    "\n",
    "* Enforce structure\n",
    "* Validate outputs\n",
    "* Block unsafe inputs\n",
    "* Provide fallbacks\n",
    "* Increase determinism\n",
    "\n",
    "---\n",
    "\n",
    "### Where Guardrails Sit in the Pipeline\n",
    "\n",
    "```\n",
    "User Input\n",
    "  ↓\n",
    "Input Guardrails\n",
    "  ↓\n",
    "Prompt + LLM\n",
    "  ↓\n",
    "Output Guardrails\n",
    "  ↓\n",
    "Validated Response\n",
    "```\n",
    "\n",
    "LangChain lets you place guardrails **at every stage**.\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Guardrails\n",
    "\n",
    "#### Input Guardrails\n",
    "\n",
    "* Prompt injection detection\n",
    "* Input sanitization\n",
    "* Length / schema validation\n",
    "\n",
    "#### Generation Guardrails\n",
    "\n",
    "* Low temperature\n",
    "* Tool-only responses\n",
    "* Restricted instructions\n",
    "\n",
    "#### Output Guardrails\n",
    "\n",
    "* Structured output validation\n",
    "* Content filters\n",
    "* Auto-fixing and retries\n",
    "* Fallback routing\n",
    "\n",
    "---\n",
    "\n",
    "### Input Guardrails (Demonstration)\n",
    "\n",
    "#### Basic Input Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f24d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_input(text: str):\n",
    "    if \"ignore previous instructions\" in text.lower():\n",
    "        raise ValueError(\"Potential prompt injection detected\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103d799",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Using RunnableLambda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad83325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "input_guard = RunnableLambda(validate_input)\n",
    "\n",
    "safe_chain = input_guard | prompt | llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b0c5b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Prompt-Level Guardrails\n",
    "\n",
    "#### Strong System Message\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Follow system rules. Do not reveal internal reasoning.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9361ec",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "System messages act as **soft guardrails**.\n",
    "\n",
    "---\n",
    "\n",
    "### Output Guardrails with Structured Output (Primary)\n",
    "\n",
    "#### Schema Enforcement (Best Practice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455636fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Ticket(BaseModel):\n",
    "    category: str\n",
    "    priority: str\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "guarded_llm = llm.with_structured_output(Ticket)\n",
    "\n",
    "result = guarded_llm.invoke(\"Email service is down for finance team\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5efb108",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Guarantees:\n",
    "\n",
    "* Correct fields\n",
    "* Correct types\n",
    "* Retry on failure\n",
    "\n",
    "---\n",
    "\n",
    "### Output Validation + Auto-Fix\n",
    "\n",
    "#### OutputFixingParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9295c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Ticket)\n",
    "fixing_parser = OutputFixingParser.from_llm(llm, parser)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a53b7c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If the model returns invalid JSON:\n",
    "\n",
    "* LangChain re-prompts\n",
    "* Repairs output automatically\n",
    "\n",
    "---\n",
    "\n",
    "### Content Safety Guardrails\n",
    "\n",
    "#### Simple Keyword Filter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71f022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safety_filter(output: str):\n",
    "    blocked = [\"password\", \"secret\"]\n",
    "    if any(b in output.lower() for b in blocked):\n",
    "        raise ValueError(\"Sensitive content detected\")\n",
    "    return output\n",
    "\n",
    "output_guard = RunnableLambda(safety_filter)\n",
    "safe_chain = prompt | llm | output_guard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d136fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Guardrails with Retry & Fallback\n",
    "\n",
    "#### Retry on Failure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_chain = (prompt | llm).with_retry(max_attempts=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2cd009",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Fallback Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13f9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_chain = (prompt | llm).with_fallbacks([backup_chain])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a846c39",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Used when:\n",
    "\n",
    "* Model fails validation\n",
    "* Rate limits occur\n",
    "* Output is unsafe\n",
    "\n",
    "---\n",
    "\n",
    "### Guardrails in RAG\n",
    "\n",
    "#### Context-Only Answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer only using the provided context.\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30450c2c",
   "metadata": {},
   "source": [
    "Combined with:\n",
    "\n",
    "* Low temperature\n",
    "* Source citation schema\n",
    "\n",
    "---\n",
    "\n",
    "### Guardrails in Agents\n",
    "\n",
    "#### Tool-Only Execution\n",
    "\n",
    "* Tools have strict JSON schemas\n",
    "* Agent cannot invent tool arguments\n",
    "* Invalid tool calls are rejected\n",
    "\n",
    "Agents rely on:\n",
    "\n",
    "* Structured tool schemas\n",
    "* AgentExecutor validation\n",
    "* Hidden chain-of-thought\n",
    "\n",
    "---\n",
    "\n",
    "### Common Guardrail Patterns\n",
    "\n",
    "#### Allowlist\n",
    "\n",
    "* Allowed tools\n",
    "* Allowed output labels\n",
    "* Allowed fields\n",
    "\n",
    "#### Denylist\n",
    "\n",
    "* Prompt injection phrases\n",
    "* Sensitive keywords\n",
    "\n",
    "#### Validation\n",
    "\n",
    "* Schema checks\n",
    "* Range checks\n",
    "* Enum checks\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "#### Relying only on prompts\n",
    "\n",
    "❌ Soft guardrails are not enough\n",
    "\n",
    "#### Overly complex schemas\n",
    "\n",
    "❌ Increases failure rate\n",
    "\n",
    "#### High temperature with guardrails\n",
    "\n",
    "❌ More retries and cost\n",
    "\n",
    "#### No fallback strategy\n",
    "\n",
    "❌ Fragile systems\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Use **structured output** as the primary guardrail\n",
    "* Keep temperature low for deterministic tasks\n",
    "* Validate both input and output\n",
    "* Add retries and fallbacks\n",
    "* Never expose chain-of-thought\n",
    "* Log guardrail failures internally\n",
    "\n",
    "---\n",
    "\n",
    "### When Guardrails Are Mandatory\n",
    "\n",
    "| Scenario      | Guardrails |\n",
    "| ------------- | ---------- |\n",
    "| APIs          | Required   |\n",
    "| Agents        | Required   |\n",
    "| RAG           | Required   |\n",
    "| Automation    | Required   |\n",
    "| Creative chat | Optional   |\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “Guardrails in LangChain are layered controls—input validation, prompt constraints, structured output enforcement, retries, and fallbacks—that ensure LLM outputs are safe, valid, and production-ready.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **If a machine consumes the output → guard it**\n",
    "* **If it’s production → guard it twice**\n",
    "* **If it’s safety-critical → validate everything**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
