{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc62d61c",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Refine Chain\n",
    "\n",
    "### What a Refine Chain Is\n",
    "\n",
    "A **Refine Chain** is a LangChain abstraction used to **process large inputs incrementally**, where the model:\n",
    "\n",
    "* Generates an initial answer from the **first chunk**\n",
    "* **Refines** that answer step by step as **new chunks** are introduced\n",
    "\n",
    "> Instead of summarizing chunks independently (MapReduce), the model **maintains and improves a single evolving answer**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Refine Chain Exists\n",
    "\n",
    "MapReduce can:\n",
    "\n",
    "* Lose global context\n",
    "* Produce shallow summaries\n",
    "\n",
    "Refine chain improves:\n",
    "\n",
    "* Coherence\n",
    "* Accuracy\n",
    "* Context preservation\n",
    "\n",
    "Especially useful when **order matters**.\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Flow\n",
    "\n",
    "```\n",
    "Chunk 1 → Initial Answer\n",
    "Chunk 2 → Refine Answer\n",
    "Chunk 3 → Refine Answer\n",
    "...\n",
    "Final Answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### How Refine Chain Works Internally\n",
    "\n",
    "1. Process first document chunk\n",
    "2. Generate an initial response\n",
    "3. For each subsequent chunk:\n",
    "\n",
    "   * Provide the previous answer\n",
    "   * Ask the model to improve/refine it\n",
    "\n",
    "---\n",
    "\n",
    "### Refine vs MapReduce\n",
    "\n",
    "| Aspect            | Refine Chain | MapReduce  |\n",
    "| ----------------- | ------------ | ---------- |\n",
    "| Parallelism       | ❌ Sequential | ✅ Parallel |\n",
    "| Accuracy          | Higher       | Medium     |\n",
    "| Latency           | Higher       | Lower      |\n",
    "| Context retention | Strong       | Weak       |\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Refine Chain Demonstration\n",
    "\n",
    "#### Step 1: Initial Prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73692391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "initial_prompt = PromptTemplate.from_template(\n",
    "    \"Write a summary of the following text:\\n{text}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c813e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Step 2: Refine Prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "748117a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Existing summary:\n",
    "{existing_answer}\n",
    "\n",
    "New context:\n",
    "{text}\n",
    "\n",
    "Refine the summary using the new context.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd95bd0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Create Refine Chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7d9f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sangouda\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains.summarize import load_summarize_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=initial_prompt,\n",
    "    refine_prompt=refine_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006a306",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Invoke the Chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274715b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence is revolutionizing various industries globally by utilizing machine learning algorithms, enabling computers to learn from data and improve their performance over time without explicit programming. Key areas of AI include Natural Language Processing, which allows machines to understand and generate human language, powering applications such as chatbots, translation services, and sentiment analysis. Additionally, computer vision enables machines to interpret and understand visual information, driving technologies like facial recognition, autonomous vehicles, and medical image analysis. Together, these advancements enhance the capabilities of AI in real-world scenarios.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create sample documents\n",
    "documents = [\n",
    "    Document(page_content=\"Artificial Intelligence is transforming industries worldwide. Machine learning algorithms enable computers to learn from data and improve their performance over time without explicit programming.\"),\n",
    "    Document(page_content=\"Natural Language Processing allows machines to understand and generate human language. Applications include chatbots, translation services, and sentiment analysis.\"),\n",
    "    Document(page_content=\"Computer vision enables machines to interpret and understand visual information from the world. It powers technologies like facial recognition, autonomous vehicles, and medical image analysis.\")\n",
    "]\n",
    "\n",
    "result = chain.invoke({\"input_documents\": documents})\n",
    "print(result[\"output_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb704797",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Each document chunk refines the answer.\n",
    "\n",
    "---\n",
    "\n",
    "### Internal Execution (Simplified)\n",
    "\n",
    "```\n",
    "answer = LLM(initial_prompt(chunk1))\n",
    "for chunk in remaining_chunks:\n",
    "    answer = LLM(refine_prompt(answer, chunk))\n",
    "return answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### When Refine Chain Is Best\n",
    "\n",
    "* Long documents where order matters\n",
    "* Reports, books, transcripts\n",
    "* Progressive summarization\n",
    "* Detailed synthesis tasks\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations of Refine Chain\n",
    "\n",
    "* ❌ Sequential (no parallelism)\n",
    "* ❌ Higher latency\n",
    "* ❌ More fragile to early mistakes\n",
    "* ❌ Legacy abstraction\n",
    "\n",
    "---\n",
    "\n",
    "### Refine Chain vs Stuff Chain\n",
    "\n",
    "| Aspect | Refine | Stuff |\n",
    "|------|-------|\n",
    "Scalability | High | Low |\n",
    "Context retention | High | Low |\n",
    "Latency | Higher | Low |\n",
    "\n",
    "---\n",
    "\n",
    "### Refine Chain vs LCEL (Modern)\n",
    "\n",
    "Refine logic can be expressed using LCEL with:\n",
    "\n",
    "* Stateful accumulation\n",
    "* Custom reducers\n",
    "* Streaming control\n",
    "\n",
    "LCEL provides:\n",
    "\n",
    "* Better observability\n",
    "* Retry & fallback\n",
    "* Custom refinement strategies\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Use deterministic parameters\n",
    "* Keep initial answer concise\n",
    "* Limit number of chunks\n",
    "* Monitor hallucinations\n",
    "* Validate final output\n",
    "\n",
    "---\n",
    "\n",
    "### When NOT to Use Refine Chain\n",
    "\n",
    "* Small inputs\n",
    "* Real-time APIs\n",
    "* Highly parallel workloads\n",
    "* New projects (prefer LCEL)\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “A Refine Chain incrementally improves an answer by feeding each new document chunk along with the previous response back into the model. It preserves context better than MapReduce but has higher latency and is a legacy LangChain abstraction.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **Order matters → Refine**\n",
    "* **Scale matters → MapReduce**\n",
    "* **Small input → Stuff**\n",
    "* **New systems → LCEL**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
