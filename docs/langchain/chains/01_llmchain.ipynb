{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebef00d4",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## LLMChain \n",
    "\n",
    "### What LLMChain Is\n",
    "\n",
    "**LLMChain** is a **legacy LangChain abstraction** that combines:\n",
    "\n",
    "* a **PromptTemplate**\n",
    "* an **LLM / ChatModel**\n",
    "* optional **output parsing**\n",
    "\n",
    "into a **single executable unit**.\n",
    "\n",
    "> LLMChain = Prompt + Model (+ Parser)\n",
    "\n",
    "It was the original way to build pipelines before **LCEL (Runnable-based composition)** became the standard.\n",
    "\n",
    "---\n",
    "\n",
    "### Why LLMChain Exists\n",
    "\n",
    "LLMChain was introduced to:\n",
    "\n",
    "* Reduce boilerplate\n",
    "* Bind prompts and models together\n",
    "* Provide a simple mental model for beginners\n",
    "\n",
    "At the time, this was the easiest way to do:\n",
    "\n",
    "```\n",
    "input → prompt → model → output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Important Status (Read This First)\n",
    "\n",
    "### LLMChain Is Legacy\n",
    "\n",
    "* ✅ Still supported\n",
    "* ❌ Not recommended for new systems\n",
    "* ✅ Replaced by **LCEL (Runnable pipelines)**\n",
    "\n",
    "LangChain now recommends:\n",
    "\n",
    "```python\n",
    "prompt | llm | parser\n",
    "```\n",
    "\n",
    "instead of `LLMChain`.\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Model\n",
    "\n",
    "```\n",
    "User Input\n",
    "   ↓\n",
    "PromptTemplate\n",
    "   ↓\n",
    "LLM / ChatModel\n",
    "   ↓\n",
    "Text Output\n",
    "```\n",
    "\n",
    "LLMChain hides this wiring internally.\n",
    "\n",
    "---\n",
    "\n",
    "### Basic LLMChain Demonstration\n",
    "\n",
    "#### Step 1: Create a PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "399d2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in one sentence.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b848ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Create an LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0ea6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb74230",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Create an LLMChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba7dcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangouda\\AppData\\Local\\Temp\\ipykernel_27216\\3057665129.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use `RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains.llm import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321a13bd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Invoke the Chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8dde18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LangChain is a decentralized platform that enables users to create and manage their own blockchain-based languages.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"topic\": \"LangChain\"})\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea51d09",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "LangChain is a framework for building applications powered by language models.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### What LLMChain Returns\n",
    "\n",
    "LLMChain returns a **dictionary**, not a raw string.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"text\": \"Generated output here\"\n",
    "}\n",
    "```\n",
    "\n",
    "This is one reason it is less ergonomic than LCEL.\n",
    "\n",
    "---\n",
    "\n",
    "### LLMChain with Chat Models\n",
    "\n",
    "#### Using ChatOpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1105f5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'RAG',\n",
       " 'text': 'RAG, or Retrieval-Augmented Generation, is a natural language processing technique that combines retrieval of relevant documents from a knowledge base with generative models to produce more accurate and contextually relevant responses.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "chain.invoke({\"topic\": \"RAG\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da02fa9c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Internally:\n",
    "\n",
    "* PromptTemplate → string\n",
    "* Chat model → message\n",
    "* Output → text\n",
    "\n",
    "This mismatch is another reason LLMChain is discouraged.\n",
    "\n",
    "---\n",
    "\n",
    "### LLMChain with Output Parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90954946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    output_parser=parser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1e724",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Parsing support is **limited** compared to LCEL.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Use Cases (Today)\n",
    "\n",
    "LLMChain is still used for:\n",
    "\n",
    "* Legacy codebases\n",
    "* Tutorials and older blogs\n",
    "* Simple scripts\n",
    "* Migration scenarios\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations of LLMChain\n",
    "\n",
    "### Structural Limitations\n",
    "\n",
    "* ❌ Not fully Runnable-based\n",
    "* ❌ Weak streaming support\n",
    "* ❌ Poor async ergonomics\n",
    "* ❌ Limited composability\n",
    "* ❌ No native parallelism\n",
    "\n",
    "---\n",
    "\n",
    "### LLMChain vs LCEL (Critical Comparison)\n",
    "\n",
    "### LLMChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f5316df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'LCEL',\n",
       " 'text': '\\n\\nLCEL stands for \"Language for Communicating Effectively and Learning,\" and it is a framework for developing and improving communication skills.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.invoke({\"topic\": \"LCEL\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae49545",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### LCEL (Recommended)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33fba821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLCEL stands for \"Language, Culture, Education, and Literacy\" and refers to the interconnectedness of these four elements in shaping an individual\\'s identity and understanding of the world.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain.invoke({\"topic\": \"LCEL\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e555fecf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Aspect         | LLMChain | LCEL    |\n",
    "| -------------- | -------- | ------- |\n",
    "| Composability  | Low      | High    |\n",
    "| Streaming      | Limited  | Native  |\n",
    "| Async          | Awkward  | Native  |\n",
    "| Parallelism    | ❌        | ✅       |\n",
    "| Future support | Legacy   | Primary |\n",
    "\n",
    "---\n",
    "\n",
    "### Migrating from LLMChain to LCEL\n",
    "\n",
    "#### Before (LLMChain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90c0534f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Agents',\n",
       " 'text': '\\n\\nAgents are autonomous entities that act on behalf of a user or system to perform specific tasks or make decisions.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.invoke({\"topic\": \"Agents\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e1997",
   "metadata": {},
   "source": [
    "\n",
    "### After (LCEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fcbcc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nAgents are autonomous entities that act on behalf of a principal to achieve a specific goal or task.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain.invoke({\"topic\": \"Agents\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f54593",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Same behavior, better flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "### When You Might Still Use LLMChain\n",
    "\n",
    "* Maintaining old projects\n",
    "* Following legacy tutorials\n",
    "* Extremely simple demos\n",
    "* Avoiding refactors temporarily\n",
    "\n",
    "---\n",
    "\n",
    "### When You Should NOT Use LLMChain\n",
    "\n",
    "* Agents\n",
    "* RAG pipelines\n",
    "* Streaming UIs\n",
    "* Production APIs\n",
    "* Multi-step workflows\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices (If You Must Use It)\n",
    "\n",
    "* Keep prompts simple\n",
    "* Avoid agents and tools\n",
    "* Use deterministic parameters\n",
    "* Plan migration to LCEL\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “LLMChain is a legacy LangChain abstraction that binds a prompt and an LLM into a single callable unit. It has largely been superseded by LCEL, which provides better composability, streaming, async support, and production readiness.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **New code → LCEL**\n",
    "* **Old code → Maintain LLMChain**\n",
    "* **Production → Avoid LLMChain**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
