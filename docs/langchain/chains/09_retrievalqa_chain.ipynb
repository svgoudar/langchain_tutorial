{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261dc2cb",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## RetrievalQA Chain (LangChain)\n",
    "\n",
    "### What a RetrievalQA Chain Is\n",
    "\n",
    "A **RetrievalQA chain** is a LangChain abstraction that combines:\n",
    "\n",
    "* **Retrieval** (searching relevant documents)\n",
    "* **Generation** (answering using an LLM)\n",
    "\n",
    "to answer questions **grounded in external data**.\n",
    "\n",
    "> RetrievalQA = **Retriever + Prompt + LLM**\n",
    "\n",
    "It is one of the earliest and most common **RAG (Retrieval-Augmented Generation)** patterns in LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "### Why RetrievalQA Exists\n",
    "\n",
    "LLMs alone:\n",
    "\n",
    "* Hallucinate\n",
    "* Have stale knowledge\n",
    "* Cannot access private data\n",
    "\n",
    "RetrievalQA solves this by:\n",
    "\n",
    "* Fetching relevant context at runtime\n",
    "* Grounding answers in source documents\n",
    "* Reducing hallucinations\n",
    "* Enabling enterprise/private data QA\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Flow\n",
    "\n",
    "```\n",
    "User Question\n",
    "   ↓\n",
    "Retriever (Vector DB / Search)\n",
    "   ↓\n",
    "Relevant Documents\n",
    "   ↓\n",
    "Prompt (stuffed with context)\n",
    "   ↓\n",
    "LLM\n",
    "   ↓\n",
    "Answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Core Components of RetrievalQA\n",
    "\n",
    "### Retriever\n",
    "\n",
    "Responsible for fetching relevant documents.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* VectorStore retriever (FAISS, Chroma, Pinecone)\n",
    "* BM25 / keyword retriever\n",
    "* Hybrid retriever\n",
    "\n",
    "---\n",
    "\n",
    "### Prompt\n",
    "\n",
    "Defines how retrieved documents are injected into the LLM.\n",
    "\n",
    "Typical pattern:\n",
    "\n",
    "```\n",
    "Answer the question using the following context:\n",
    "{context}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### LLM\n",
    "\n",
    "Generates the final answer using the provided context.\n",
    "\n",
    "---\n",
    "\n",
    "### Basic RetrievalQA Demonstration\n",
    "\n",
    "#### Step 1: Create a Retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62230c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    texts=[\"LangChain is a framework for LLM apps\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa01eb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Create the RetrievalQA Chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3089a6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sangouda\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e2020",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Ask a Question\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7391ef75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed for building applications that utilize large language models (LLMs). It provides tools and components to facilitate the development of LLM-based applications, making it easier for developers to integrate and manage these models in their projects.\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain.invoke(\n",
    "    {\"query\": \"What is LangChain?\"}\n",
    ")\n",
    "\n",
    "print(result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ed368",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "```text\n",
    "LangChain is a framework for building applications using language models.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### What RetrievalQA Returns\n",
    "\n",
    "By default:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"query\": \"...\",\n",
    "  \"result\": \"final answer\"\n",
    "}\n",
    "```\n",
    "\n",
    "Optional:\n",
    "\n",
    "* Source documents\n",
    "* Metadata\n",
    "\n",
    "---\n",
    "\n",
    "### RetrievalQA with Source Documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8248106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='dedc0e08-e26e-481d-84ee-154d9279f09b', metadata={}, page_content='LangChain is a framework for LLM apps')]\n"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke({\"query\": \"What is LangChain?\"})\n",
    "\n",
    "print(result[\"source_documents\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c87014",
   "metadata": {},
   "source": [
    "\n",
    "Useful for:\n",
    "\n",
    "* Debugging\n",
    "* Citations\n",
    "* Trust & explainability\n",
    "\n",
    "---\n",
    "\n",
    "### Chain Types in RetrievalQA\n",
    "\n",
    "RetrievalQA internally uses **document combination chains**:\n",
    "\n",
    "| chain_type | Description                                |\n",
    "| ---------- | ------------------------------------------ |\n",
    "| stuff      | Stuff all docs into one prompt             |\n",
    "| map_reduce | Summarize docs independently, then combine |\n",
    "| refine     | Incrementally refine answer                |\n",
    "| map_rerank | Score answers and pick best                |\n",
    "\n",
    "---\n",
    "\n",
    "### RetrievalQA vs ConversationalRetrievalChain\n",
    "\n",
    "| Aspect       | RetrievalQA | ConversationalRetrievalChain |\n",
    "| ------------ | ----------- | ---------------------------- |\n",
    "| Chat history | ❌           | ✅                            |\n",
    "| Multi-turn   | ❌           | ✅                            |\n",
    "| Memory       | ❌           | ✅                            |\n",
    "\n",
    "Use RetrievalQA for **single-turn QA**.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations of RetrievalQA\n",
    "\n",
    "* ❌ Legacy abstraction\n",
    "* ❌ Limited customization\n",
    "* ❌ Not fully Runnable-based\n",
    "* ❌ Harder to debug\n",
    "* ❌ Less control than LCEL\n",
    "\n",
    "---\n",
    "\n",
    "### RetrievalQA vs LCEL-based RAG (Modern)\n",
    "\n",
    "#### RetrievalQA (Legacy)\n",
    "\n",
    "```python\n",
    "RetrievalQA.from_chain_type(...)\n",
    "```\n",
    "\n",
    "#### LCEL RAG (Recommended)\n",
    "\n",
    "```python\n",
    "chain = (\n",
    "    {\"context\": retriever, \"input\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "```\n",
    "\n",
    "LCEL provides:\n",
    "\n",
    "* Better observability\n",
    "* Streaming & async\n",
    "* Custom guards\n",
    "* Flexible composition\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use RetrievalQA\n",
    "\n",
    "* Learning RAG concepts\n",
    "* Quick prototypes\n",
    "* Legacy codebases\n",
    "* Simple QA over documents\n",
    "\n",
    "---\n",
    "\n",
    "### When NOT to Use RetrievalQA\n",
    "\n",
    "* Production RAG systems\n",
    "* Multi-turn chat\n",
    "* Agentic workflows\n",
    "* Advanced reranking\n",
    "* Streaming APIs\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices (If You Use It)\n",
    "\n",
    "* Use low temperature (0–0.2)\n",
    "* Limit retrieved documents\n",
    "* Enable source documents\n",
    "* Validate token size\n",
    "* Plan migration to LCEL\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “RetrievalQA is a LangChain chain that answers questions by retrieving relevant documents and passing them to an LLM. It implements a classic RAG pattern but is a legacy abstraction superseded by LCEL-based pipelines.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **Simple single-turn QA → RetrievalQA**\n",
    "* **Conversational QA → ConversationalRetrievalChain**\n",
    "* **Production RAG → LCEL or LangGraph**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd03090",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
