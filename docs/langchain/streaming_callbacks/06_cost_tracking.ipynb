{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5081aa",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Cost Tracking\n",
    "\n",
    "\n",
    "**Cost tracking** is the process of **measuring and attributing LLM usage cost** based on:\n",
    "\n",
    "* Tokens consumed (input + output)\n",
    "* Model used\n",
    "* Number of calls\n",
    "* Retries / fallbacks\n",
    "* Tools invoked\n",
    "\n",
    "In LLM systems, cost tracking answers:\n",
    "\n",
    "* *How much did this request cost?*\n",
    "* *Which user / feature is expensive?*\n",
    "* *Where are tokens being wasted?*\n",
    "\n",
    "Cost tracking is commonly implemented using:\n",
    "\n",
    "* Callback handlers\n",
    "* Tracing metadata\n",
    "* Token usage APIs\n",
    "\n",
    "Supported natively in LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Cost Tracking Is Critical\n",
    "\n",
    "Without cost tracking:\n",
    "\n",
    "* Bills are unpredictable\n",
    "* Token waste goes unnoticed\n",
    "* Scaling becomes risky\n",
    "\n",
    "With cost tracking:\n",
    "\n",
    "* Budget control\n",
    "* Per-user / per-feature accounting\n",
    "* Cost optimization\n",
    "* SLA and quota enforcement\n",
    "\n",
    "---\n",
    "\n",
    "### What Contributes to LLM Cost\n",
    "\n",
    "| Factor        | Description               |\n",
    "| ------------- | ------------------------- |\n",
    "| Input tokens  | Prompt + context          |\n",
    "| Output tokens | Generated text            |\n",
    "| Model         | GPT-4 > GPT-3.5           |\n",
    "| Retries       | Each retry costs          |\n",
    "| Fallbacks     | Secondary models add cost |\n",
    "| Streaming     | Same cost, better UX      |\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture View\n",
    "\n",
    "![Image](https://blog.promptlayer.com/content/images/2024/11/How-a-Prompt-Engineering-Tool-Improves-AI-Model-Performance--24-.png)\n",
    "\n",
    "![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AUzR4Qr__-TOsZ63BgLnqww.png)\n",
    "\n",
    "![Image](https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?auto=format\\&fit=max\\&n=Tf5b6pnNY9Uj6Vtl\\&q=85\\&s=50c5f4d966f8fe4f8ae0be0beaf11bc4)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Built-in Token & Cost Tracking (Callback)\n",
    "\n",
    "#### Using `get_openai_callback`\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    response = llm.invoke(\"Explain cost tracking in LLMs\")\n",
    "    \n",
    "    print(\"Prompt tokens:\", cb.prompt_tokens)\n",
    "    print(\"Completion tokens:\", cb.completion_tokens)\n",
    "    print(\"Total tokens:\", cb.total_tokens)\n",
    "    print(\"Total cost ($):\", cb.total_cost)\n",
    "```\n",
    "\n",
    "**Output (example)**\n",
    "\n",
    "```\n",
    "Prompt tokens: 23\n",
    "Completion tokens: 42\n",
    "Total tokens: 65\n",
    "Total cost ($): 0.00013\n",
    "```\n",
    "\n",
    "This is the **simplest and most common** approach.\n",
    "\n",
    "---\n",
    "\n",
    "#### Cost Tracking in a RunnableSequence\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain {topic} in simple terms\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    chain.invoke({\"topic\": \"cost tracking\"})\n",
    "    print(\"Cost:\", cb.total_cost)\n",
    "```\n",
    "\n",
    "Tracks cost across the **entire chain**, not just the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "#### Cost Tracking with RAG Pipelines\n",
    "\n",
    "```python\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": lambda x: x,\n",
    "        \"context\": retriever\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    chain.invoke(\"What is RAG?\")\n",
    "    print(\"Tokens used:\", cb.total_tokens)\n",
    "    print(\"Cost:\", cb.total_cost)\n",
    "```\n",
    "\n",
    "This reveals:\n",
    "\n",
    "* How much retrieval context inflates prompt tokens\n",
    "* Why chunk size matters\n",
    "\n",
    "---\n",
    "\n",
    "#### Cost Tracking with Retries & Fallbacks\n",
    "\n",
    "```python\n",
    "primary = ChatOpenAI(model=\"gpt-4\").with_retry(2)\n",
    "backup = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "llm = primary.with_fallbacks([backup])\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    llm.invoke(\"Explain retry and fallback\")\n",
    "    print(\"Total cost with retries/fallbacks:\", cb.total_cost)\n",
    "```\n",
    "\n",
    "Cost includes:\n",
    "\n",
    "* All retries\n",
    "* Fallback model usage\n",
    "\n",
    "---\n",
    "\n",
    "#### Cost Tracking per Request (FastAPI)\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def chat(q: str):\n",
    "    with get_openai_callback() as cb:\n",
    "        answer = await llm.ainvoke(q)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer.content,\n",
    "            \"tokens\": cb.total_tokens,\n",
    "            \"cost\": cb.total_cost\n",
    "        }\n",
    "```\n",
    "\n",
    "Enables:\n",
    "\n",
    "* Per-request billing\n",
    "* User-level quotas\n",
    "* Usage dashboards\n",
    "\n",
    "---\n",
    "\n",
    "#### Cost Tracking via Tracing (Production)\n",
    "\n",
    "When tracing is enabled:\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=true\n",
    "```\n",
    "\n",
    "Each trace automatically records:\n",
    "\n",
    "* Token counts\n",
    "* Cost per step\n",
    "* Model usage\n",
    "* Retry/fallback impact\n",
    "\n",
    "Viewed in:\n",
    "\n",
    "* LangSmith UI\n",
    "* Cost dashboards\n",
    "\n",
    "---\n",
    "\n",
    "### Cost Tracking vs Token Tracking\n",
    "\n",
    "| Aspect        | Token Tracking | Cost Tracking |\n",
    "| ------------- | -------------- | ------------- |\n",
    "| Measures      | Tokens         | Money         |\n",
    "| Model-aware   | ❌              | ✅             |\n",
    "| Billing-ready | ❌              | ✅             |\n",
    "| Optimization  | Partial        | Full          |\n",
    "\n",
    "Cost = **tokens × model price**.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Cost Pitfalls\n",
    "\n",
    "* Large retrieval context\n",
    "* Overlapping chunks\n",
    "* Using GPT-4 unnecessarily\n",
    "* Unlimited retries\n",
    "* Verbose prompts\n",
    "\n",
    "---\n",
    "\n",
    "### Cost Optimization Hooks\n",
    "\n",
    "Typical hooks:\n",
    "\n",
    "* Log cost per request\n",
    "* Enforce max tokens\n",
    "* Route to cheaper model\n",
    "* Abort when budget exceeded\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "if cb.total_cost > 0.01:\n",
    "    raise Exception(\"Cost limit exceeded\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "Cost tracking is a **meter** attached to every LLM call.\n",
    "\n",
    "```\n",
    "LLM runs → tokens counted → cost calculated → budget enforced\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Cost tracking is mandatory for production LLMs\n",
    "* LangChain provides built-in callbacks\n",
    "* Works across chains, RAG, retries, fallbacks\n",
    "* Enables budgeting, optimization, and governance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21e902",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
