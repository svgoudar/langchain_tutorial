{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbf3056",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Observability Hooks\n",
    "\n",
    "**Observability hooks** are **integration points** that let you **capture signals** (logs, metrics, traces, events) from inside an LLM pipeline **at runtime**.\n",
    "They allow you to *see what the system is doing while it’s doing it*.\n",
    "\n",
    "In LangChain-based systems, observability hooks are implemented using:\n",
    "\n",
    "* Callback handlers\n",
    "* Tracing hooks\n",
    "* Metrics hooks\n",
    "* Streaming hooks\n",
    "\n",
    "They are foundational to **production-grade AI systems**.\n",
    "\n",
    "```\n",
    "Runnable Execution\n",
    "   ├── Observability Hooks\n",
    "   │     ├── Logs\n",
    "   │     ├── Metrics\n",
    "   │     └── Traces\n",
    "   ↓\n",
    "Monitoring / Debugging / Alerting\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why Observability Hooks Matter\n",
    "\n",
    "Without hooks:\n",
    "\n",
    "* You don’t know why answers are wrong\n",
    "* You can’t debug latency\n",
    "* Failures are invisible\n",
    "* Costs are opaque\n",
    "\n",
    "With hooks:\n",
    "\n",
    "* Full execution visibility\n",
    "* Root-cause analysis\n",
    "* SLA monitoring\n",
    "* Safe production rollout\n",
    "\n",
    "---\n",
    "\n",
    "### What Observability Hooks Capture\n",
    "\n",
    "| Signal  | Purpose              |\n",
    "| ------- | -------------------- |\n",
    "| Logs    | What happened        |\n",
    "| Metrics | How often / how long |\n",
    "| Traces  | Where time was spent |\n",
    "| Events  | Lifecycle changes    |\n",
    "| Streams | Partial outputs      |\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture View\n",
    "\n",
    "![Image](https://last9.ghost.io/content/images/2024/12/monitoring_stack.webp)\n",
    "\n",
    "![Image](https://canada1.discourse-cdn.com/flex007/uploads/langchain/optimized/2X/8/8107939d30e448eea2471b6d89f60f681dc20a86_2_1024x434.png)\n",
    "\n",
    "![Image](https://framerusercontent.com/images/H9paF4dK6nmB3pyEm9263g8KY.png?height=702\\&width=1200)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Logging Hook (Simple Observability)\n",
    "\n",
    "#### Logging Hook via RunnableLambda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5b90ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Input received: observability\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OBSERVABILITY'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def log_input(x):\n",
    "    logging.info(\"Input received: %s\", x)\n",
    "    return x\n",
    "\n",
    "chain = (\n",
    "    RunnableLambda(log_input)\n",
    "    | RunnableLambda(lambda x: x.upper())\n",
    ")\n",
    "\n",
    "chain.invoke(\"observability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f327c985",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Observed**\n",
    "\n",
    "* Input logged\n",
    "* Output produced normally\n",
    "\n",
    "This is a **manual observability hook**.\n",
    "\n",
    "---\n",
    "\n",
    "### Callback Handler Hook (Most Common)\n",
    "\n",
    "#### Custom Observability Callback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79599808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.callbacks.base import BaseCallbackHandler\n",
    "import time\n",
    "\n",
    "class ObservabilityHook(BaseCallbackHandler):\n",
    "    def on_chain_start(self, serialized, inputs, **kwargs):\n",
    "        self.start = time.time()\n",
    "        print(\"Chain started\")\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        print(\"LLM finished\")\n",
    "\n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        duration = time.time() - self.start\n",
    "        print(f\"Chain completed in {duration:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def146b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Attach Hook to a Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231d447a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.prompts'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[32m      4\u001b[39m prompt = ChatPromptTemplate.from_template(\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mExplain \u001b[39m\u001b[38;5;132;01m{topic}\u001b[39;00m\u001b[33m briefly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m llm = ChatOpenAI(callbacks=[ObservabilityHook()])\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.prompts'"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain {topic} briefly\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(callbacks=[ObservabilityHook()])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"topic\": \"observability hooks\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1f0498",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Observed**\n",
    "\n",
    "* Chain start\n",
    "* LLM completion\n",
    "* Total execution time\n",
    "\n",
    "---\n",
    "\n",
    "## C. Token Streaming Hook (Real-Time Observability)\n",
    "\n",
    "#### Streaming Token Hook\n",
    "\n",
    "```python\n",
    "class TokenStreamHook(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    streaming=True,\n",
    "    callbacks=[TokenStreamHook()]\n",
    ")\n",
    "\n",
    "llm.invoke(\"Explain observability hooks\")\n",
    "```\n",
    "\n",
    "**Observed**\n",
    "\n",
    "* Tokens emitted live\n",
    "* Real-time UX + observability\n",
    "\n",
    "---\n",
    "\n",
    "## D. Tracing Hook (Automatic Observability)\n",
    "\n",
    "#### Enable Tracing Hooks\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=true\n",
    "export LANGCHAIN_API_KEY=your_key\n",
    "export LANGCHAIN_PROJECT=observability-demo\n",
    "```\n",
    "\n",
    "```python\n",
    "chain.invoke({\"topic\": \"hooks\"})\n",
    "```\n",
    "\n",
    "**Observed**\n",
    "\n",
    "* Full execution trace\n",
    "* Step-by-step timing\n",
    "* Token usage\n",
    "* Retry/fallback visibility\n",
    "\n",
    "This is **automatic observability**.\n",
    "\n",
    "---\n",
    "\n",
    "## E. Metrics Hook (Latency / Counters)\n",
    "\n",
    "#### Metrics Hook Example\n",
    "\n",
    "```python\n",
    "class MetricsHook(BaseCallbackHandler):\n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        print(\"Metric: chain.success = 1\")\n",
    "\n",
    "    def on_error(self, error, **kwargs):\n",
    "        print(\"Metric: chain.failure = 1\")\n",
    "```\n",
    "\n",
    "Used to export metrics to:\n",
    "\n",
    "* Prometheus\n",
    "* CloudWatch\n",
    "* Datadog\n",
    "\n",
    "---\n",
    "\n",
    "### Where Observability Hooks Attach\n",
    "\n",
    "| Layer     | Hook Point       |\n",
    "| --------- | ---------------- |\n",
    "| Runnable  | Input/output     |\n",
    "| Chain     | Start/end        |\n",
    "| LLM       | Tokens / latency |\n",
    "| Tool      | Invocation       |\n",
    "| Retriever | Query time       |\n",
    "| Agent     | Decisions        |\n",
    "\n",
    "---\n",
    "\n",
    "### Observability Hooks vs Callbacks vs Tracing\n",
    "\n",
    "| Concept            | Role             |\n",
    "| ------------------ | ---------------- |\n",
    "| Callback Handler   | Mechanism        |\n",
    "| Observability Hook | Purpose          |\n",
    "| Tracing            | Visualization    |\n",
    "| Logging            | Human-readable   |\n",
    "| Metrics            | Machine-readable |\n",
    "\n",
    "Hooks are the **bridge**.\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Example (Production Agent)\n",
    "\n",
    "**IT Support Assistant**\n",
    "\n",
    "* Hook logs every ticket query\n",
    "* Hook traces RAG latency\n",
    "* Hook streams tokens to UI\n",
    "* Hook counts fallback usage\n",
    "* Hook alerts on error spikes\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Keep hooks lightweight\n",
    "* Never block execution\n",
    "* Redact sensitive data\n",
    "* Separate logs vs metrics\n",
    "* Enable tracing selectively in prod\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "Observability hooks are **sensors** inside your LLM system.\n",
    "\n",
    "```\n",
    "System runs → hooks observe → signals emitted → insight gained\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Observability hooks expose internal behavior\n",
    "* Implemented via callbacks, tracing, streaming\n",
    "* Essential for debugging, performance, reliability\n",
    "* Mandatory for production LLM systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
