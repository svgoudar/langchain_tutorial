{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b98b46a",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Token Streaming\n",
    "\n",
    "\n",
    "**Token streaming** is the ability of an LLM to **emit tokens incrementally as they are generated**, instead of waiting for the full response to complete.\n",
    "This enables **real-time output**, similar to how ChatGPT types responses word by word.\n",
    "\n",
    "In LangChain, token streaming is supported through the runnable interface of LangChain.\n",
    "\n",
    "```\n",
    "Prompt\n",
    "  ↓\n",
    "LLM\n",
    "  ↓\n",
    "Token → Token → Token → Done\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why Token Streaming Matters\n",
    "\n",
    "Without streaming:\n",
    "\n",
    "* User waits for the entire response\n",
    "* High perceived latency\n",
    "* Poor UX for long answers\n",
    "\n",
    "With streaming:\n",
    "\n",
    "* Immediate feedback\n",
    "* Better responsiveness\n",
    "* Ideal for chat UIs, copilots, terminals\n",
    "\n",
    "---\n",
    "\n",
    "### How Token Streaming Works Internally\n",
    "\n",
    "1. LLM starts generating tokens\n",
    "2. Each token is yielded as soon as it’s ready\n",
    "3. Client consumes tokens in a loop\n",
    "4. Final completion signal is sent\n",
    "\n",
    "This is **push-based generation**, not batch output.\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture View\n",
    "\n",
    "![Image](https://miro.medium.com/0%2AkNsNAIa_9n4z0qGU)\n",
    "\n",
    "![Image](https://miro.medium.com/1%2A-l2jXGB7FIv2-bRiEoaoOQ.png)\n",
    "\n",
    "![Image](https://langtail-web.vercel.app/images/blog/token-flow.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Token Streaming (LLM Only)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63fb908d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token streaming is a method used to transfer small amounts of data or information, piece by piece, in a continuous flow. Instead of transferring all the data at once, tokens or small packets of information are sent one after the other, allowing for a more efficient and continuous transfer of data. This method is often used in online media streaming services, where content like videos or music is broken down into smaller chunks (tokens) and streamed to the user in real-time, ensuring a smooth and uninterrupted playback experience."
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(streaming=True)\n",
    "\n",
    "for chunk in llm.stream(\"Explain token streaming in simple terms\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622279d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**What happens**\n",
    "\n",
    "* Tokens arrive one by one\n",
    "* `chunk.content` contains partial text\n",
    "* Output appears immediately\n",
    "\n",
    "---\n",
    "\n",
    "### Token Streaming in a RunnableSequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826d7868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token streaming refers to the process of dividing a piece of content, such as a video or audio file, into smaller segments called tokens. These tokens can then be accessed and streamed individually, allowing for faster and more efficient playback, especially in cases where internet connections or device capabilities may limit the ability to stream larger files continuously. By breaking down the content into tokens, users can access smaller chunks of data at a time, reducing loading times and improving overall streaming performance."
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain the following topic briefly:\\n{topic}\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnableLambda(lambda x: {\"topic\": x})\n",
    "    | prompt\n",
    "    | ChatOpenAI(streaming=True)\n",
    ")\n",
    "\n",
    "for chunk in chain.stream(\"Token streaming\"):\n",
    "    print(chunk.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a2fb9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Key point:\n",
    "\n",
    "* **Only the final LLM step streams**\n",
    "* Earlier steps run normally\n",
    "\n",
    "---\n",
    "\n",
    "### Async Token Streaming (`astream`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ded5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async token streaming enables the data to be sent and received in a non-blocking manner. This means that the data is transmitted in small chunks (tokens) asynchronously, allowing the sending and receiving processes to continue with other tasks while the data is being streamed. This can improve performance and efficiency by reducing the need to wait for the entire data set to be transmitted before processing begins. Additionally, async token streaming can help handle large amounts of data more effectively, as it allows for the continuous flow of data without overloading the system."
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run():\n",
    "    async for chunk in llm.astream(\"Explain async token streaming\"):\n",
    "        print(chunk.content, end=\"\")\n",
    "\n",
    "await run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf99712",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Used in:\n",
    "\n",
    "* FastAPI\n",
    "* WebSockets\n",
    "* SSE endpoints\n",
    "\n",
    "---\n",
    "\n",
    "### Token Streaming with FastAPI (Real-World)\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/chat\")\n",
    "def chat(q: str):\n",
    "    def token_generator():\n",
    "        for chunk in llm.stream(q):\n",
    "            yield chunk.content\n",
    "    return StreamingResponse(token_generator(), media_type=\"text/plain\")\n",
    "```\n",
    "\n",
    "This enables:\n",
    "\n",
    "* Browser-based streaming\n",
    "* Chat-like experience\n",
    "* Non-blocking responses\n",
    "\n",
    "---\n",
    "\n",
    "### Token Streaming vs Normal Invocation\n",
    "\n",
    "| Aspect     | Token Streaming | Normal Invoke |\n",
    "| ---------- | --------------- | ------------- |\n",
    "| Output     | Incremental     | Full response |\n",
    "| Latency    | Low             | High          |\n",
    "| UX         | Real-time       | Delayed       |\n",
    "| Complexity | Slightly higher | Simple        |\n",
    "\n",
    "---\n",
    "\n",
    "### What Can Stream Tokens\n",
    "\n",
    "| Component        | Token Streaming |\n",
    "| ---------------- | --------------- |\n",
    "| LLMs             | ✅               |\n",
    "| RunnableSequence | ✅ (via LLM)     |\n",
    "| RunnableParallel | ✅ (per branch)  |\n",
    "| Retriever        | ❌               |\n",
    "| Prompt templates | ❌               |\n",
    "| RunnableLambda   | ❌               |\n",
    "\n",
    "Token streaming only occurs at **token-producing components**.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "* Forgetting `streaming=True`\n",
    "* Printing `chunk` instead of `chunk.content`\n",
    "* Expecting non-LLM steps to stream\n",
    "* Blocking the event loop in async servers\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Token Streaming\n",
    "\n",
    "Use it when:\n",
    "\n",
    "* Building chat applications\n",
    "* Responses are long\n",
    "* UX responsiveness matters\n",
    "* Users expect live feedback\n",
    "\n",
    "Avoid it when:\n",
    "\n",
    "* Batch/offline processing\n",
    "* Post-processing is required before output\n",
    "* Logging-only pipelines\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "Token streaming turns:\n",
    "\n",
    "```\n",
    "invoke() → full text\n",
    "```\n",
    "\n",
    "into:\n",
    "\n",
    "```\n",
    "stream() → token → token → token\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Token streaming emits output **incrementally**\n",
    "* Enabled via `streaming=True`\n",
    "* Works through sequences and parallel graphs\n",
    "* Essential for production chat UX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
