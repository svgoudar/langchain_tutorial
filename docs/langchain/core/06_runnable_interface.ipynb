{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f433ad7",
   "metadata": {},
   "source": [
    "\n",
    "```{contents}\n",
    "```\n",
    "\n",
    "## Runnable Interface\n",
    "\n",
    "### What is the Runnable Interface\n",
    "\n",
    "The **Runnable interface** is the **core execution abstraction in LangChain**.\n",
    "Every executable component in LangChain (LLMs, prompts, chains, retrievers, parsers, agents) is implemented as a **Runnable**.\n",
    "\n",
    "> A Runnable is **anything that can be invoked, composed, streamed, batched, retried, or executed asynchronously** in a uniform way.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Runnable Exists\n",
    "\n",
    "Before Runnable:\n",
    "\n",
    "* Chains were rigid\n",
    "* Limited composition\n",
    "* Poor async & streaming support\n",
    "\n",
    "With Runnable:\n",
    "\n",
    "* Everything is composable\n",
    "* Declarative pipelines\n",
    "* Built-in async, streaming, retries\n",
    "* One mental model for all components\n",
    "\n",
    "---\n",
    "\n",
    "### Runnable Mental Model\n",
    "\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "Runnable\n",
    "  ↓\n",
    "Output\n",
    "```\n",
    "\n",
    "And multiple Runnables can be composed:\n",
    "\n",
    "```\n",
    "Runnable → Runnable → Runnable\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Core Runnable Capabilities\n",
    "\n",
    "Every Runnable supports:\n",
    "\n",
    "* `invoke()` – sync execution\n",
    "* `ainvoke()` – async execution\n",
    "* `stream()` – token streaming\n",
    "* `batch()` – batch processing\n",
    "* `with_retry()` – retries\n",
    "* `with_fallbacks()` – fallback logic\n",
    "* `with_config()` – tracing & metadata\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Runnable Example\n",
    "\n",
    "#### LLM as a Runnable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a881f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed for developing applications that leverage language models (LLMs) by facilitating their integration, chaining, and interaction with other data sources and tools.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "result = llm.invoke(\"Explain LangChain in one sentence\")\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d615cc3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "`ChatOpenAI` is a **Runnable**.\n",
    "\n",
    "---\n",
    "\n",
    "### Prompt as a Runnable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22178974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "messages = prompt.invoke({\"input\": \"What is RAG?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d6829",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "`ChatPromptTemplate` is also a **Runnable**.\n",
    "\n",
    "---\n",
    "\n",
    "### Runnable Composition (LCEL)\n",
    "\n",
    "#### The `|` Operator (LCEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69984fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce88367",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This creates a **RunnableSequence**.\n",
    "\n",
    "Execution flow:\n",
    "\n",
    "```\n",
    "dict → prompt → messages → llm → AIMessage\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Full Runnable Chain Demonstration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5cb170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG stands for Retrieval-Augmented Generation, which is a framework used in natural language processing, particularly in the context of language models and information retrieval. The main idea behind RAG is to combine the strengths of both retrieval-based methods and generation-based methods to improve the accuracy and relevance of generated responses.\n",
      "\n",
      "### Key Components of RAG:\n",
      "\n",
      "1. **Retrieval**:\n",
      "   - In the first step of the RAG process, relevant documents or pieces of information are retrieved from a large corpus or knowledge base. This is typically done using a retrieval model that identifies content that is semantically similar to the input query.\n",
      "   - The retrieval component can use various techniques, including traditional information retrieval methods (like TF-IDF, BM25) or more advanced neural models (like dense embeddings).\n",
      "\n",
      "2. **Augmentation**:\n",
      "   - The retrieved documents are then used to provide additional context and information to the generative model. This helps ensure that the generation process is grounded in actual knowledge, rather than relying solely on the model's pre-learned knowledge.\n",
      "\n",
      "3. **Generation**:\n",
      "   - The generative model takes both the user's query and the relevant, retrieved documents as input to produce a coherent and contextually appropriate response. This model could be a transformer-based architecture, such as GPT or BERT.\n",
      "\n",
      "### Advantages of RAG:\n",
      "\n",
      "- **Improved Relevance**: By incorporating real-time retrieval of relevant documents, RAG can often provide more accurate and up-to-date information than models that solely rely on pre-learned data.\n",
      "- **Broader Knowledge Base**: RAG can leverage vast external knowledge sources, allowing for responses that might not be strictly contained within the pre-trained model's training set.\n",
      "- **Contextual Awareness**: The retrieved information helps ground the output in specific facts or contexts, which can improve the quality and factuality of the responses.\n",
      "\n",
      "### Applications of RAG:\n",
      "\n",
      "RAG can be applied in various domains including:\n",
      "\n",
      "- **Question Answering**: Enhancing the accuracy of answers based on dynamic retrieval of relevant documents.\n",
      "- **Chatbots**: Building conversational agents that provide responses based on up-to-date knowledge.\n",
      "- **Content Generation**: Assisting in generating content that requires factual accuracy and context based on large datasets.\n",
      "\n",
      "In summary, RAG is a powerful approach that combines information retrieval with text generation, leading to improved quality and relevance in various applications of language technology.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({\"input\": \"Explain RAG\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4567c38",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Each component is a Runnable.\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableLambda (Custom Logic)\n",
    "\n",
    "#### Wrap any Python function as a Runnable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d41cf85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "uppercase = RunnableLambda(lambda x: x.upper())\n",
    "\n",
    "uppercase.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee4e41",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Useful for:\n",
    "\n",
    "* Pre-processing\n",
    "* Post-processing\n",
    "* Glue logic\n",
    "\n",
    "---\n",
    "\n",
    "### RunnablePassthrough\n",
    "\n",
    "#### Pass input through unchanged (or assign values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36de2796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'LangChain', 'length': 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        length=lambda x: len(x[\"input\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "chain.invoke({\"input\": \"LangChain\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c905e2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Used heavily in:\n",
    "\n",
    "* Agents\n",
    "* Prompt variable injection\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableParallel\n",
    "\n",
    "#### Execute multiple Runnables in parallel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1dc354a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='LangChain is a framework designed to facilitate the development of applications that integrate with large language models (LLMs). It provides a suite of tools, components, and utilities that allow developers to build sophisticated natural language processing workflows and applications with ease. Here are some key aspects of LangChain:\\n\\n1. **Modular Components**: LangChain is built around modular components that can be used independently or combined to create complex systems. These components include things like language models, prompt templates, chains, and agents.\\n\\n2. **Language Models**: LangChain supports multiple LLMs, providing a uniform interface for interacting with different models, whether they are hosted by companies like OpenAI, Hugging Face, or others.\\n\\n3. **Prompt Management**: The framework includes tools for creating and managing prompts, enhancing the way developers can structure their interactions with language models. This can help optimize responses and improve overall application performance.\\n\\n4. **Chains and Workflows**: LangChain allows developers to create \"chains\" of operations, where the output of one component feeds into the input of another. This is useful for tasks that require multiple steps of processing or decision-making.\\n\\n5. **Memory**: The framework supports memory management, allowing applications to maintain context over interactions. This can enhance user experience in applications like chatbots or personal assistants, where remembering past interactions can be crucial.\\n\\n6. **Integrations**: LangChain is designed for easy integration with other systems and APIs, allowing developers to combine LLM capabilities with various data sources and services.\\n\\n7. **Use Cases**: Common use cases for LangChain include chatbots, content generation tools, question-answering systems, data analysis, and more. It\\'s particularly well-suited for applications that require high-level natural language understanding or generation.\\n\\n8. **Community and Ecosystem**: LangChain is an open-source framework with a growing community of developers. This ensures continuous improvement, support, and a variety of shared resources and tutorials for getting started.\\n\\nOverall, LangChain aims to simplify the process of building applications that leverage the power of language models while providing flexibility and extensibility to suit various use cases.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 429, 'prompt_tokens': 19, 'total_tokens': 448, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_644f11dd4d', 'id': 'chatcmpl-Cp4pxwi6p9dIl7naHVedbn50xl4YJ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b3f0c-b869-7232-bfc4-b26e66b9b1de-0', usage_metadata={'input_tokens': 19, 'output_tokens': 429, 'total_tokens': 448, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " 'length': 17}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    answer=prompt | llm,\n",
    "    length=lambda x: len(x[\"input\"])\n",
    ")\n",
    "\n",
    "parallel.invoke({\"input\": \"Explain LangChain\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5e9d6d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"answer\": AIMessage(...),\n",
    "  \"length\": 18\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Streaming with Runnables\n",
    "\n",
    "```python\n",
    "for chunk in llm.stream(\"Explain LangChain\"):\n",
    "    print(chunk.content, end=\"\")\n",
    "```\n",
    "\n",
    "Works for:\n",
    "\n",
    "* Chat UIs\n",
    "* SSE\n",
    "* WebSockets\n",
    "\n",
    "---\n",
    "\n",
    "### Async Execution\n",
    "\n",
    "```python\n",
    "await llm.ainvoke(\"Explain RAG\")\n",
    "```\n",
    "\n",
    "Every Runnable supports async.\n",
    "\n",
    "---\n",
    "\n",
    "### Batch Execution\n",
    "\n",
    "```python\n",
    "llm.batch([\n",
    "    \"What is LangChain?\",\n",
    "    \"What is RAG?\"\n",
    "])\n",
    "```\n",
    "\n",
    "Used for:\n",
    "\n",
    "* Embedding generation\n",
    "* Bulk inference\n",
    "\n",
    "---\n",
    "\n",
    "### Retry & Fallback\n",
    "\n",
    "#### Retry\n",
    "\n",
    "```python\n",
    "safe_llm = llm.with_retry(max_attempts=3)\n",
    "safe_llm.invoke(\"Explain LangChain\")\n",
    "```\n",
    "\n",
    "#### Fallback\n",
    "\n",
    "```python\n",
    "fallback_llm = llm.with_fallbacks([backup_llm])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Runnable Configuration & Tracing\n",
    "\n",
    "```python\n",
    "chain.invoke(\n",
    "    {\"input\": \"Explain LangChain\"},\n",
    "    config={\"tags\": [\"demo\"], \"metadata\": {\"env\": \"dev\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "Used by:\n",
    "\n",
    "* LangSmith\n",
    "* Observability tools\n",
    "\n",
    "---\n",
    "\n",
    "### Runnables in Agents\n",
    "\n",
    "Internally, agents are **Runnable graphs**:\n",
    "\n",
    "```\n",
    "Input\n",
    " ↓\n",
    "Prompt Runnable\n",
    " ↓\n",
    "LLM Runnable\n",
    " ↓\n",
    "Tool Runnable\n",
    " ↓\n",
    "Parser Runnable\n",
    "```\n",
    "\n",
    "This is why agents:\n",
    "\n",
    "* Are composable\n",
    "* Are debuggable\n",
    "* Support streaming\n",
    "\n",
    "---\n",
    "\n",
    "### Runnables in RAG\n",
    "\n",
    "```python\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"input\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "```\n",
    "\n",
    "Retriever, prompt, LLM → all Runnables.\n",
    "\n",
    "---\n",
    "\n",
    "### What is NOT a Runnable\n",
    "\n",
    "* Plain strings\n",
    "* Raw Python objects\n",
    "* SDK responses (without wrapping)\n",
    "\n",
    "Everything must be **wrapped** or **adapted** into a Runnable.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Runnable Types (Cheat Sheet)\n",
    "\n",
    "| Runnable            | Purpose            |\n",
    "| ------------------- | ------------------ |\n",
    "| LLM / ChatModel     | Model invocation   |\n",
    "| PromptTemplate      | Prompt formatting  |\n",
    "| Retriever           | Document retrieval |\n",
    "| OutputParser        | Structured parsing |\n",
    "| RunnableLambda      | Custom logic       |\n",
    "| RunnableParallel    | Fan-out            |\n",
    "| RunnablePassthrough | Input wiring       |\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "* Mixing sync and async incorrectly\n",
    "* Forgetting that prompt output ≠ string\n",
    "* Manual chaining instead of LCEL\n",
    "* Overusing custom glue code\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “The Runnable interface is LangChain’s execution backbone. Every component—LLMs, prompts, retrievers, agents—is a Runnable, enabling uniform invocation, streaming, batching, retries, and composable pipelines using LCEL.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "If it **executes**, it should be a **Runnable**.\n",
    "If it **doesn’t compose**, you’re doing it wrong.\n",
    "\n",
    "---\n",
    "\n",
    "If you want next:\n",
    "\n",
    "* Runnable internals\n",
    "* Runnable vs Chain vs Agent\n",
    "* Debugging Runnable graphs\n",
    "* LangGraph vs Runnable\n",
    "* Performance tuning with Runnables\n",
    "\n",
    "State which one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6843b45b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
