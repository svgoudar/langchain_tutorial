{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbeff369",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "\n",
    "## LangChain Expression Language (LCEL)\n",
    "\n",
    "\n",
    "\n",
    "LCEL (LangChain Expression Language) is **LangChain’s declarative syntax** for composing **Runnable components** into execution pipelines.\n",
    "\n",
    "> LCEL lets you describe *what* should happen, not *how* to wire it.\n",
    "\n",
    "It replaces rigid “chains” with **composable, inspectable graphs**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why LCEL Exists\n",
    "\n",
    "Before LCEL:\n",
    "\n",
    "* Chains were fixed\n",
    "* Hard to branch or parallelize\n",
    "* Limited streaming & async support\n",
    "\n",
    "With LCEL:\n",
    "\n",
    "* Everything is a Runnable\n",
    "* Pipelines are composable\n",
    "* Built-in streaming, async, retries\n",
    "* Same code works for LLMs, RAG, agents\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Runnable as the Building Block\n",
    "\n",
    "Every LCEL expression is built from **Runnables**:\n",
    "\n",
    "* Prompt templates\n",
    "* Chat models\n",
    "* Retrievers\n",
    "* Output parsers\n",
    "* Custom logic\n",
    "\n",
    "LCEL does **not** introduce new execution logic.\n",
    "It **orchestrates existing Runnables**.\n",
    "\n",
    "---\n",
    "\n",
    "### The `|` Operator (Sequential Composition)\n",
    "\n",
    "The pipe operator creates a **RunnableSequence**.\n",
    "\n",
    "```python\n",
    "chain = prompt | llm\n",
    "```\n",
    "\n",
    "Execution flow:\n",
    "\n",
    "```\n",
    "Input → prompt → llm → Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Simple Prompt → LLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "358b3280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCEL can refer to different things depending on the context. Here are a few possibilities:\n",
      "\n",
      "1. **Low-Cost Enhanced Learning (LCEL)** - This might refer to educational initiatives or technologies aimed at providing affordable learning opportunities.\n",
      "\n",
      "2. **Liquid Crystal Energy Luminescence (LCEL)** - This could refer to specific applications of liquid crystal technology in energy harvesting or display technologies.\n",
      "\n",
      "3. **Language and Cultural Education in Language (LCEL)** - This may pertain to programs or studies focused on integrating cultural education with language learning.\n",
      "\n",
      "If you can provide more context or specify the field or topic you are interested in, I could give a more targeted explanation!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "result = chain.invoke({\"input\": \"What is LCEL?\"})\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958dcc38",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### LCEL with Output Parsing\n",
    "\n",
    "#### Prompt → LLM → Parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79890330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCEL can refer to different concepts or organizations depending on the context, but it's not a widely recognized acronym in common literature or frameworks. However, if you meant \"LCL,\" it typically refers to \"Less than Container Load\" in shipping and logistics, which is a shipment that doesn't occupy a full shipping container.\n",
      "\n",
      "If you were referring to a specific organization, concept, or technology with \"LCEL,\" please provide additional details or context so I can assist you more accurately.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({\"input\": \"Explain LCEL\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b10cf",
   "metadata": {},
   "source": [
    "Each stage is a Runnable.\n",
    "\n",
    "---\n",
    "\n",
    "### LCEL with Custom Logic\n",
    "\n",
    "#### RunnableLambda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a2d5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LCEL can refer to various concepts depending on the context, but there may not be a widely recognized term or acronym that specifically denotes \"LCEL\" without further information. Here are a few possibilities:\\n\\n1. **Local Cell**: In telecommunications, LCEL could refer to a specific type of local cell or small cell within a larger network infrastructure, used to improve coverage and capacity in a particular geographical area. \\n\\n2. **Learning and Community Engagement Lab**: In the context of education or community services, LCEL could represent a laboratory or initiative focused on blending learning with community engagement activities.\\n\\n3. **Legal, Compliance, and Ethics**: It could also represent a department or function within organizations that focus on upholding legal standards, ensuring compliance, and maintaining ethical practices.\\n\\nIf you have a specific context in mind (such as technology, education, or another field), please provide more details so I can offer a more precise explanation!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 188, 'prompt_tokens': 12, 'total_tokens': 200, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_644f11dd4d', 'id': 'chatcmpl-Cp51BvL7EoQlTidI3fFuSQQwHuD1J', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b3f17-590f-7310-b6ab-cc85d385e7f6-0', usage_metadata={'input_tokens': 12, 'output_tokens': 188, 'total_tokens': 200, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "uppercase = RunnableLambda(lambda x: x.upper())\n",
    "\n",
    "chain = uppercase | llm\n",
    "\n",
    "chain.invoke(\"Explain LCEL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ca578",
   "metadata": {},
   "source": [
    "Used for:\n",
    "\n",
    "* Pre-processing\n",
    "* Post-processing\n",
    "* Glue logic\n",
    "\n",
    "---\n",
    "\n",
    "### LCEL Parallel Execution\n",
    "\n",
    "#### RunnableParallel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaa11972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='LCEL could refer to various things depending on the context, but it is not a widely recognized acronym or term in common usage. It could stand for a specific organization, concept, or technology that may not be well-known or may have developed recently. \\n\\nTo provide a more accurate and helpful explanation, could you please specify the context in which you encountered \"LCEL\"? For example, are you referring to a specific field such as education, technology, science, or something else? Any additional details would be beneficial in providing a more targeted response.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 109, 'prompt_tokens': 19, 'total_tokens': 128, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_644f11dd4d', 'id': 'chatcmpl-Cp51WNU4IpYyvS3w7c56cw5pWI2S2', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b3f17-a804-7c40-bde1-12a295faf9ee-0', usage_metadata={'input_tokens': 19, 'output_tokens': 109, 'total_tokens': 128, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " 'length': 12}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    answer=prompt | llm,\n",
    "    length=lambda x: len(x[\"input\"])\n",
    ")\n",
    "\n",
    "parallel.invoke({\"input\": \"Explain LCEL\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ebd10",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"answer\": AIMessage(...),\n",
    "  \"length\": 12\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### LCEL with Input Mapping\n",
    "\n",
    "#### RunnablePassthrough\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0cb60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=lambda x: \"LangChain context\"\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c0a52",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Used heavily in:\n",
    "\n",
    "* Agents\n",
    "* RAG pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## LCEL in Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "### Retriever as a Runnable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"input\": RunnablePassthrough()\n",
    "    }\n",
    "    | ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Answer using the context only\"),\n",
    "        (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{input}\")\n",
    "    ])\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c292e56",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Retriever, prompt, and LLM are composed uniformly.\n",
    "\n",
    "---\n",
    "\n",
    "### LCEL Streaming\n",
    "\n",
    "#### Token Streaming\n",
    "\n",
    "```python\n",
    "for chunk in chain.stream({\"input\": \"Explain LCEL\"}):\n",
    "    print(chunk.content, end=\"\")\n",
    "```\n",
    "\n",
    "Works automatically if the model supports streaming.\n",
    "\n",
    "---\n",
    "\n",
    "### LCEL Async Execution\n",
    "\n",
    "```python\n",
    "await chain.ainvoke({\"input\": \"Explain LCEL\"})\n",
    "```\n",
    "\n",
    "All LCEL pipelines support async execution.\n",
    "\n",
    "---\n",
    "\n",
    "### LCEL Batch Execution\n",
    "\n",
    "```python\n",
    "chain.batch([\n",
    "    {\"input\": \"What is LCEL?\"},\n",
    "    {\"input\": \"What is LangChain?\"}\n",
    "])\n",
    "```\n",
    "\n",
    "Efficient for bulk inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Error Handling in LCEL\n",
    "\n",
    "#### Retry\n",
    "\n",
    "```python\n",
    "safe_chain = chain.with_retry(max_attempts=3)\n",
    "```\n",
    "\n",
    "#### Fallback\n",
    "\n",
    "```python\n",
    "fallback_chain = chain.with_fallbacks([backup_chain])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### LCEL vs Chains\n",
    "\n",
    "| Aspect      | Chains  | LCEL   |\n",
    "| ----------- | ------- | ------ |\n",
    "| Flexibility | Low     | High   |\n",
    "| Composable  | Limited | Full   |\n",
    "| Streaming   | Partial | Native |\n",
    "| Async       | Limited | Native |\n",
    "\n",
    "### LCEL vs Agents\n",
    "\n",
    "| Aspect       | LCEL     | Agents     |\n",
    "| ------------ | -------- | ---------- |\n",
    "| Control Flow | Static   | Dynamic    |\n",
    "| Tool Choice  | Explicit | LLM-driven |\n",
    "| Determinism  | High     | Lower      |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use LCEL\n",
    "\n",
    "* Deterministic pipelines\n",
    "* RAG systems\n",
    "* Data extraction\n",
    "* Pre/post-processing\n",
    "* High-performance inference\n",
    "\n",
    "\n",
    "### When Not to Use LCEL Alone\n",
    "\n",
    "* Multi-step reasoning\n",
    "* Tool decision-making\n",
    "* Human-in-the-loop logic\n",
    "\n",
    "Use **Agents or LangGraph** in those cases.\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “LCEL is LangChain’s declarative language for composing Runnables into execution pipelines. It enables streaming, async, retries, parallelism, and deterministic orchestration using a concise pipe syntax.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "If the flow is **known ahead of time**, use **LCEL**.\n",
    "If the flow must be **decided by the model**, use **Agents or LangGraph**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec1347",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
