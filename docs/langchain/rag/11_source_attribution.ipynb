{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d2063a",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Source Attribution\n",
    "\n",
    "\n",
    "**Source attribution** is the practice of **linking each generated answer back to the original documents or chunks** that supported it.\n",
    "\n",
    "> It answers: **“Where did this answer come from?”**\n",
    "\n",
    "In RAG systems, source attribution provides **traceability, trust, and debuggability**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Source Attribution Is Important\n",
    "\n",
    "Without attribution:\n",
    "\n",
    "* Users cannot verify answers\n",
    "* Hallucinations go unnoticed\n",
    "* Compliance and audits fail\n",
    "* Debugging retrieval errors is hard\n",
    "\n",
    "With attribution:\n",
    "\n",
    "* Answers are explainable\n",
    "* Trust increases\n",
    "* Errors are diagnosable\n",
    "* Compliance requirements are met\n",
    "\n",
    "---\n",
    "\n",
    "### Where Source Attribution Fits in RAG\n",
    "\n",
    "```\n",
    "Documents\n",
    "  ↓\n",
    "Chunks (with metadata)\n",
    "  ↓\n",
    "Vector Store\n",
    "  ↓\n",
    "Retriever\n",
    "  ↓\n",
    "LLM\n",
    "  ↓\n",
    "Answer + Sources\n",
    "```\n",
    "\n",
    "Attribution relies on **metadata propagation** from ingestion to output.\n",
    "\n",
    "---\n",
    "\n",
    "### What Is a “Source”\n",
    "\n",
    "A source can be:\n",
    "\n",
    "* File name (PDF, DOC, TXT)\n",
    "* URL\n",
    "* Database record ID\n",
    "* Ticket ID\n",
    "* Page number\n",
    "* Chunk ID\n",
    "\n",
    "Example metadata:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"source\": \"jira_ticket_123.csv\",\n",
    "  \"page\": 4,\n",
    "  \"chunk_id\": 2\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### How Source Attribution Works (Mechanism)\n",
    "\n",
    "### Step 1: Attach Metadata at Ingestion\n",
    "\n",
    "```python\n",
    "Document(\n",
    "    page_content=\"Ticket escalation process...\",\n",
    "    metadata={\n",
    "        \"source\": \"jira_guide.pdf\",\n",
    "        \"page\": 5\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Preserve Metadata Through Chunking\n",
    "\n",
    "Text splitters **copy metadata** to every chunk.\n",
    "\n",
    "Each chunk knows:\n",
    "\n",
    "* Where it came from\n",
    "* Its position in the source\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Retrieve Chunks With Metadata\n",
    "\n",
    "```python\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "```\n",
    "\n",
    "Each `doc` includes:\n",
    "\n",
    "* `page_content`\n",
    "* `metadata`\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Return Sources Alongside the Answer\n",
    "\n",
    "The LLM answer is accompanied by:\n",
    "\n",
    "* The chunks used\n",
    "* Their metadata\n",
    "\n",
    "---\n",
    "\n",
    "### Source Attribution in LangChain (Basic)\n",
    "\n",
    "### RetrievalQA with Sources\n",
    "\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke({\"query\": \"How does escalation work?\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Output Structure\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"result\": \"Escalation happens when...\",\n",
    "  \"source_documents\": [\n",
    "    Document(metadata={\"source\": \"jira_guide.pdf\", \"page\": 5}),\n",
    "    Document(metadata={\"source\": \"sla_policy.pdf\", \"page\": 2})\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Source Attribution in LCEL (Modern Pattern)\n",
    "\n",
    "Conceptually:\n",
    "\n",
    "```\n",
    "Retriever → Prompt → LLM\n",
    "          ↘ sources ↗\n",
    "```\n",
    "\n",
    "You explicitly return both:\n",
    "\n",
    "* Generated answer\n",
    "* Retrieved documents\n",
    "\n",
    "Used in APIs and UIs.\n",
    "\n",
    "---\n",
    "\n",
    "### Inline Source Attribution (Citations)\n",
    "\n",
    "### Pattern\n",
    "\n",
    "```\n",
    "Escalation occurs after SLA breach [jira_guide.pdf, p.5].\n",
    "```\n",
    "\n",
    "### How It’s Done\n",
    "\n",
    "* LLM is instructed to cite sources\n",
    "* Chunk IDs or metadata keys are provided\n",
    "* Post-processing maps citations to metadata\n",
    "\n",
    "---\n",
    "\n",
    "### Prompt Pattern for Attribution\n",
    "\n",
    "```text\n",
    "Answer the question using only the provided context.\n",
    "For each statement, cite the source using [source, page].\n",
    "```\n",
    "\n",
    "This is a **soft guardrail**.\n",
    "\n",
    "---\n",
    "\n",
    "### Hard Attribution (Safer)\n",
    "\n",
    "Instead of trusting the LLM:\n",
    "\n",
    "* Extract sources programmatically\n",
    "* Attach them outside the generated text\n",
    "\n",
    "Preferred in production.\n",
    "\n",
    "---\n",
    "\n",
    "### Source Attribution vs Citations\n",
    "\n",
    "| Concept        | Source Attribution | Citations   |\n",
    "| -------------- | ------------------ | ----------- |\n",
    "| Location       | Metadata / UI      | Inline text |\n",
    "| Reliability    | High               | Medium      |\n",
    "| LLM-controlled | ❌                  | ✅           |\n",
    "| Production use | ✅                  | ⚠️          |\n",
    "\n",
    "---\n",
    "\n",
    "### Source Attribution vs Hallucination Control\n",
    "\n",
    "Attribution does **not prevent hallucination by itself**, but:\n",
    "\n",
    "* Makes hallucinations visible\n",
    "* Enables confidence scoring\n",
    "* Enables fallback logic\n",
    "\n",
    "Often combined with:\n",
    "\n",
    "* “Answer only from context” prompts\n",
    "* Similarity thresholds\n",
    "* Reranking\n",
    "\n",
    "---\n",
    "\n",
    "### Production-Grade Source Attribution\n",
    "\n",
    "### 1. Metadata Standards\n",
    "\n",
    "Always include:\n",
    "\n",
    "* `source_id`\n",
    "* `document_name`\n",
    "* `chunk_id`\n",
    "* Optional: page, URL, timestamp\n",
    "\n",
    "---\n",
    "\n",
    "### 2. UI-Level Attribution\n",
    "\n",
    "Display:\n",
    "\n",
    "* Answer\n",
    "* Clickable sources\n",
    "* Highlighted snippets\n",
    "\n",
    "Never rely on plain text citations alone.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Attribution Granularity\n",
    "\n",
    "| Granularity    | Use Case          |\n",
    "| -------------- | ----------------- |\n",
    "| Document-level | High-level QA     |\n",
    "| Page-level     | PDFs              |\n",
    "| Chunk-level    | Precise answers   |\n",
    "| Sentence-level | Regulated domains |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Multi-Source Answers\n",
    "\n",
    "Answers may rely on **multiple sources**.\n",
    "Production systems:\n",
    "\n",
    "* Deduplicate sources\n",
    "* Rank by contribution\n",
    "* Show top-N sources\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "#### Trusting LLM-generated citations\n",
    "\n",
    "❌ Can hallucinate sources\n",
    "\n",
    "#### Losing metadata during ingestion\n",
    "\n",
    "❌ No attribution possible\n",
    "\n",
    "#### Overloading UI with all chunks\n",
    "\n",
    "❌ Confusing for users\n",
    "\n",
    "#### Mixing sources from different tenants\n",
    "\n",
    "❌ Security risk\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Attach metadata at ingestion\n",
    "* Preserve metadata through all stages\n",
    "* Return sources separately from answer text\n",
    "* Use attribution as a confidence signal\n",
    "* Log sources for observability\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “Source attribution is the process of linking LLM-generated answers back to the documents or chunks used during retrieval. In RAG systems, it relies on metadata propagation and is critical for trust, debugging, and compliance.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **No metadata → no attribution**\n",
    "* **Attribution ≠ citation**\n",
    "* **Production systems must show sources**\n",
    "* **If you can’t explain the source, don’t trust the answer**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
