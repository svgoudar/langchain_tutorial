{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "317eea36",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Context Window Management \n",
    "\n",
    "\n",
    "**Context window management** is the practice of **controlling how much information is sent to an LLM per request** so that:\n",
    "\n",
    "* The input fits within the model’s token limit\n",
    "* The most relevant information is preserved\n",
    "* Cost, latency, and hallucinations are minimized\n",
    "\n",
    "> An LLM can only “see” what fits inside its context window.\n",
    "\n",
    "---\n",
    "\n",
    "### What a Context Window Is\n",
    "\n",
    "A **context window** is the **maximum number of tokens** (input + output) an LLM can process in one call.\n",
    "\n",
    "```\n",
    "Total Tokens = System + User + Context + History + Output\n",
    "```\n",
    "\n",
    "If this limit is exceeded:\n",
    "\n",
    "* Requests fail, or\n",
    "* Older context is silently dropped (dangerous)\n",
    "\n",
    "---\n",
    "\n",
    "### Why Context Window Management Is Critical\n",
    "\n",
    "Without management:\n",
    "\n",
    "* Token overflow errors\n",
    "* Missing important context\n",
    "* Increased hallucinations\n",
    "* High latency and cost\n",
    "* Unstable multi-turn conversations\n",
    "\n",
    "Context window management is **mandatory in production systems**.\n",
    "\n",
    "---\n",
    "\n",
    "### Where Context Window Pressure Comes From\n",
    "\n",
    "### Main Contributors\n",
    "\n",
    "1. System instructions\n",
    "2. User query\n",
    "3. Retrieved documents (RAG)\n",
    "4. Chat history (memory)\n",
    "5. Tool outputs\n",
    "6. Expected model response\n",
    "\n",
    "All compete for the same token budget.\n",
    "\n",
    "---\n",
    "\n",
    "### Context Window in a RAG Pipeline\n",
    "\n",
    "```\n",
    "User Query\n",
    "   ↓\n",
    "Retriever (Top-K chunks)\n",
    "   ↓\n",
    "Context Assembly  ← (critical step)\n",
    "   ↓\n",
    "Prompt\n",
    "   ↓\n",
    "LLM\n",
    "```\n",
    "\n",
    "Context assembly decides **what goes in** and **what stays out**.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Context Window Management Techniques\n",
    "\n",
    "#### Chunking (Foundation)\n",
    "\n",
    "Documents are split into chunks before embedding.\n",
    "\n",
    "```python\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50\n",
    "```\n",
    "\n",
    "Prevents sending entire documents to the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "#### Top-K Control (Hard Limit)\n",
    "\n",
    "Limit the number of retrieved chunks.\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "```\n",
    "\n",
    "Tradeoff:\n",
    "\n",
    "* Small k → risk missing context\n",
    "* Large k → token explosion\n",
    "\n",
    "---\n",
    "\n",
    "#### Reranking (Precision Control)\n",
    "\n",
    "Retrieve many, send few.\n",
    "\n",
    "```\n",
    "Retrieve 20 → Rerank → Send top 3\n",
    "```\n",
    "\n",
    "Reduces noise and token usage.\n",
    "\n",
    "---\n",
    "\n",
    "#### Contextual Compression\n",
    "\n",
    "Reduce chunk size **after retrieval**.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Extract only relevant sentences\n",
    "* Summarize chunks\n",
    "* Remove boilerplate\n",
    "\n",
    "LangChain pattern:\n",
    "\n",
    "```\n",
    "Retriever → Compressor → LLM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Stuff vs MapReduce vs Refine\n",
    "\n",
    "| Strategy  | Context Usage |\n",
    "| --------- | ------------- |\n",
    "| Stuff     | High          |\n",
    "| MapReduce | Controlled    |\n",
    "| Refine    | Incremental   |\n",
    "\n",
    "For large inputs, avoid `stuff`.\n",
    "\n",
    "---\n",
    "\n",
    "### Managing Chat History (Memory Control)\n",
    "\n",
    "#### Problem\n",
    "\n",
    "Unbounded chat history quickly fills the context window.\n",
    "\n",
    "---\n",
    "\n",
    "### Solutions\n",
    "\n",
    "#### Windowed Memory\n",
    "\n",
    "Keep only last N turns.\n",
    "\n",
    "```python\n",
    "ConversationBufferWindowMemory(k=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary Memory\n",
    "\n",
    "Summarize old messages.\n",
    "\n",
    "```\n",
    "Old turns → Summary → Single memory entry\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Hybrid Memory\n",
    "\n",
    "Recent turns + long-term summary.\n",
    "\n",
    "Production standard.\n",
    "\n",
    "---\n",
    "\n",
    "### Token Budgeting (Production Practice)\n",
    "\n",
    "### Define a Budget\n",
    "\n",
    "Example (8k model):\n",
    "\n",
    "| Component         | Tokens |\n",
    "| ----------------- | ------ |\n",
    "| System            | 500    |\n",
    "| User              | 200    |\n",
    "| Retrieved context | 4,000  |\n",
    "| History           | 2,000  |\n",
    "| Response          | 1,300  |\n",
    "\n",
    "Never “fill to the brim”.\n",
    "\n",
    "---\n",
    "\n",
    "### Dynamic Context Selection\n",
    "\n",
    "#### Query-Aware Context\n",
    "\n",
    "Not all queries need the same context size.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Fact lookup → small context\n",
    "* Reasoning → larger context\n",
    "\n",
    "Production systems dynamically adjust:\n",
    "\n",
    "* k\n",
    "* chunk size\n",
    "* reranking depth\n",
    "\n",
    "---\n",
    "\n",
    "### Context Window vs Hallucination\n",
    "\n",
    "Too much context:\n",
    "\n",
    "* Model gets confused\n",
    "* Contradictions increase\n",
    "\n",
    "Too little context:\n",
    "\n",
    "* Model guesses\n",
    "* Hallucinations increase\n",
    "\n",
    "**Optimal context beats maximum context.**\n",
    "\n",
    "---\n",
    "\n",
    "### Context Window Management in LangChain (LCEL Pattern)\n",
    "\n",
    "Conceptual flow:\n",
    "\n",
    "```\n",
    "Retriever\n",
    "   ↓\n",
    "Reranker\n",
    "   ↓\n",
    "Compressor\n",
    "   ↓\n",
    "Prompt\n",
    "   ↓\n",
    "LLM\n",
    "```\n",
    "\n",
    "Each stage reduces context entropy.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Production Mistakes\n",
    "\n",
    "#### Sending all retrieved chunks\n",
    "\n",
    "❌ Token overflow\n",
    "\n",
    "#### No reranking\n",
    "\n",
    "❌ Noisy context\n",
    "\n",
    "#### Unlimited chat history\n",
    "\n",
    "❌ Context poisoning\n",
    "\n",
    "#### Ignoring output tokens\n",
    "\n",
    "❌ Runtime failures\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices (Production-Grade)\n",
    "\n",
    "* Chunk aggressively at ingestion\n",
    "* Use hybrid search + reranking\n",
    "* Enforce top-k limits\n",
    "* Compress context before LLM\n",
    "* Summarize long histories\n",
    "* Monitor token usage per request\n",
    "* Fail fast if context exceeds budget\n",
    "\n",
    "---\n",
    "\n",
    "### Context Window vs Long-Context Models\n",
    "\n",
    "Long-context models:\n",
    "\n",
    "* Reduce pressure\n",
    "* Do NOT eliminate the need for management\n",
    "\n",
    "Even with large windows:\n",
    "\n",
    "* Cost scales linearly\n",
    "* Noise still harms quality\n",
    "\n",
    "Context management is still required.\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “Context window management is the discipline of selecting, compressing, and prioritizing information sent to an LLM so that it fits within token limits while preserving relevance, accuracy, and performance. It is a core concern in production RAG and conversational systems.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **More context ≠ better answers**\n",
    "* **Relevant context > all context**\n",
    "* **Rerank, then compress**\n",
    "* **Budget tokens like memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea54ca5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
