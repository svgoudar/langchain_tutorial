{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07bd976c",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## RAG and Knowledge Grounding Metrics\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation) and knowledge grounding metrics** evaluate whether:\n",
    "\n",
    "1. The **right documents were retrieved**\n",
    "2. The **retrieved knowledge was relevant**\n",
    "3. The **LLM used only that knowledge**\n",
    "4. The answer **did not hallucinate**\n",
    "\n",
    "They answer the most important RAG question:\n",
    "\n",
    "> **Did the model say only what it could justify from retrieved knowledge?**\n",
    "\n",
    "These metrics are **mandatory** for enterprise, regulated, and high-trust systems.\n",
    "\n",
    "---\n",
    "\n",
    "### Why RAG Metrics Are Separate from Quality Metrics\n",
    "\n",
    "A response can be:\n",
    "\n",
    "* Relevant ❌\n",
    "* Fluent ❌\n",
    "* Confident ❌\n",
    "  and still be **hallucinated**.\n",
    "\n",
    "RAG metrics focus on **evidence alignment**, not language quality.\n",
    "\n",
    "---\n",
    "\n",
    "### Categories of RAG & Grounding Metrics\n",
    "\n",
    "```\n",
    "RAG Metrics\n",
    "│\n",
    "├── Retrieval Quality Metrics\n",
    "├── Context Quality Metrics\n",
    "├── Grounding / Faithfulness Metrics\n",
    "└── Hallucination Metrics\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Retrieval Quality Metrics\n",
    "\n",
    "#### Retrieval Precision@K\n",
    "\n",
    "**What it measures**\n",
    "How many retrieved documents are actually relevant.\n",
    "\n",
    "$$\n",
    "\\text{Precision@K} = \\frac{\\text{Relevant Docs Retrieved}}{K}\n",
    "$$\n",
    "\n",
    "**Demonstration**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485f5b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved_docs, relevant_docs, k):\n",
    "    retrieved = retrieved_docs[:k]\n",
    "    relevant = set(relevant_docs)\n",
    "    hits = sum(1 for d in retrieved if d in relevant)\n",
    "    return hits / k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9ebaf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Retrieval Recall@K\n",
    "\n",
    "**What it measures**\n",
    "Whether the system retrieved at least one required document.\n",
    "\n",
    "$$\n",
    "\\text{Recall@K} = \\frac{\\text{Relevant Docs Retrieved}}{\\text{Total Relevant Docs}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e276f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(retrieved_docs, relevant_docs, k):\n",
    "    retrieved = set(retrieved_docs[:k])\n",
    "    relevant = set(relevant_docs)\n",
    "    return len(retrieved & relevant) / len(relevant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86234b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Key rule**\n",
    "If recall = 0 → generation is guaranteed to fail.\n",
    "\n",
    "---\n",
    "\n",
    "#### Mean Reciprocal Rank (MRR)\n",
    "\n",
    "**What it measures**\n",
    "How early the first correct document appears.\n",
    "\n",
    "$$\n",
    "\\text{MRR} = \\frac{1}{\\text{rank of first relevant doc}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c141a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(retrieved_docs, relevant_docs):\n",
    "    for i, doc in enumerate(retrieved_docs, start=1):\n",
    "        if doc in relevant_docs:\n",
    "            return 1 / i\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed17b13",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Context Quality Metrics\n",
    "\n",
    "#### Context Relevance\n",
    "\n",
    "**What it measures**\n",
    "Are the retrieved documents relevant to the question?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e57129b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'The criterion asks if the submission is referring to a real quote from the text. However, there is no text provided for the submission to quote from. The submission is answering the question about what RAG in machine learning is, but it is not quoting from any text. Therefore, the submission does not meet the criterion.\\n\\nN',\n",
       " 'value': 'N',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.evaluation import load_evaluator\n",
    "\n",
    "# Example question and context for demonstration\n",
    "question = \"What is RAG in machine learning?\"\n",
    "context_text = \"\"\"\n",
    "RAG stands for Retrieval-Augmented Generation. It is a technique that combines \n",
    "document retrieval with language model generation. The system first retrieves \n",
    "relevant documents from a knowledge base, then uses those documents as context \n",
    "to generate accurate, grounded responses.\n",
    "\"\"\"\n",
    "\n",
    "context_relevance = load_evaluator(\"criteria\", criteria=\"relevance\")\n",
    "\n",
    "context_relevance.evaluate_strings(\n",
    "    input=question,\n",
    "    prediction=context_text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50771742",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Prevents **prompt pollution**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Context Coverage\n",
    "\n",
    "**What it measures**\n",
    "Does the context contain all necessary information?\n",
    "\n",
    "Often measured by:\n",
    "\n",
    "* Keyword coverage\n",
    "* LLM-as-judge completeness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c8a1af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage score: 1\n",
      "Reasoning: The criterion is completeness. The question asks for the meaning of RAG in machine learning. \n",
      "\n",
      "The submitted answer provides a definition of RAG, explaining that it stands for Retrieval-Augmented Generation. It also explains what this technique does, combining document retrieval with language model generation. The answer further explains how the system works, retrieving relevant documents from a knowledge base and using those documents as context to generate responses. \n",
      "\n",
      "The answer seems to cover all necessary information to answer the question comprehensively. It provides a definition, explains the technique, and describes how it works. \n",
      "\n",
      "Therefore, the submission meets the criterion of completeness. \n",
      "\n",
      "Y\n"
     ]
    }
   ],
   "source": [
    "# Define custom completeness criteria\n",
    "custom_criteria = {\n",
    "    \"completeness\": \"Does the context contain all necessary information to answer the question comprehensively?\"\n",
    "}\n",
    "\n",
    "coverage_eval = load_evaluator(\"criteria\", criteria=custom_criteria)\n",
    "\n",
    "# Example usage\n",
    "coverage_result = coverage_eval.evaluate_strings(\n",
    "    input=question,\n",
    "    prediction=context_text\n",
    ")\n",
    "print(f\"Coverage score: {coverage_result['score']}\")\n",
    "print(f\"Reasoning: {coverage_result['reasoning']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167b19ed",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Knowledge Grounding / Faithfulness Metrics\n",
    "\n",
    "#### Faithfulness (Groundedness)\n",
    "\n",
    "**What it measures**\n",
    "Is every claim in the answer supported by retrieved context?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c2fddfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness evaluation:\n",
      "Score: 1\n",
      "Reasoning: CORRECT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangouda\\AppData\\Local\\Temp\\ipykernel_43300\\2593192178.py:28: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
      "To use references, use the labeled_criteria instead.\n",
      "  result = faithfulness_eval.evaluate_strings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Faithfulness score: 0\n",
      "Reasoning: The criterion is faithfulness, which requires that every claim in the answer is supported by the provided context. The context in this case is the question \"What is RAG in machine learning?\"\n",
      "\n",
      "The submission claims that RAG (Retrieval-Augmented Generation) is a machine learning technique that combines information retrieval with text generation. It also claims that RAG retrieves relevant documents and uses them as context for generating responses.\n",
      "\n",
      "The context, which is the question, does not provide any information to verify these claims. Therefore, the submission does not meet the criterion of faithfulness.\n",
      "\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "# Define answer for demonstration\n",
    "answer = \"\"\"\n",
    "RAG (Retrieval-Augmented Generation) is a machine learning technique that combines \n",
    "information retrieval with text generation. It retrieves relevant documents and uses \n",
    "them as context for generating responses.\n",
    "\"\"\"\n",
    "\n",
    "# Option 1: Use context_qa evaluator (checks if answer is supported by context)\n",
    "context_qa_eval = load_evaluator(\"context_qa\")\n",
    "\n",
    "faithfulness_result = context_qa_eval.evaluate_strings(\n",
    "    input=question,\n",
    "    prediction=answer,\n",
    "    reference=context_text\n",
    ")\n",
    "\n",
    "print(f\"Faithfulness evaluation:\")\n",
    "print(f\"Score: {faithfulness_result.get('score', 'N/A')}\")\n",
    "print(f\"Reasoning: {faithfulness_result.get('reasoning', 'N/A')}\")\n",
    "\n",
    "# Option 2: Use custom criteria for faithfulness\n",
    "faithfulness_criteria = {\n",
    "    \"faithfulness\": \"Is every claim in the answer supported by the provided context? The answer should not include information that cannot be verified from the context.\"\n",
    "}\n",
    "\n",
    "faithfulness_eval = load_evaluator(\"criteria\", criteria=faithfulness_criteria)\n",
    "\n",
    "result = faithfulness_eval.evaluate_strings(\n",
    "    input=question,\n",
    "    prediction=answer,\n",
    "    reference=context_text\n",
    ")\n",
    "\n",
    "print(f\"\\nCustom Faithfulness score: {result['score']}\")\n",
    "print(f\"Reasoning: {result['reasoning']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31541a3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This is the **single most important RAG metric**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Attribution Accuracy\n",
    "\n",
    "**What it measures**\n",
    "Can the answer be traced to specific sources?\n",
    "\n",
    "```text\n",
    "Answer: RAG combines retrieval with generation. [Doc1]\n",
    "```\n",
    "\n",
    "Used in:\n",
    "\n",
    "* Legal\n",
    "* Healthcare\n",
    "* Enterprise search\n",
    "\n",
    "---\n",
    "\n",
    "### Hallucination Metrics\n",
    "\n",
    "#### Hallucination Rate\n",
    "\n",
    "**What it measures**\n",
    "Percentage of answers that include unsupported claims.\n",
    "\n",
    "$$\n",
    "\\text{Hallucination Rate} = \\frac{\\text{Unfaithful Answers}}{\\text{Total Answers}}\n",
    "$$\n",
    "\n",
    "```python\n",
    "hallucination_rate = unfaithful_answers / total_answers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Unsupported Claim Ratio\n",
    "\n",
    "Measures:\n",
    "\n",
    "* How much of the answer is unsupported\n",
    "* Partial hallucinations\n",
    "\n",
    "---\n",
    "\n",
    "### End-to-End RAG Evaluation Flow (Demo)\n",
    "\n",
    "```python\n",
    "docs = retriever.invoke(question)\n",
    "context = \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "answer = rag_chain.invoke(question).content\n",
    "\n",
    "faithfulness_score = faithfulness.evaluate_strings(\n",
    "    input=question,\n",
    "    prediction=answer,\n",
    "    reference=context\n",
    ")[\"score\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Acceptance Thresholds (Industry Practice)\n",
    "\n",
    "| Metric             | Typical Threshold |\n",
    "| ------------------ | ----------------- |\n",
    "| Faithfulness       | ≥ 0.8             |\n",
    "| Recall@K           | ≥ 0.9             |\n",
    "| Precision@K        | ≥ 0.6             |\n",
    "| Hallucination Rate | ≤ 5%              |\n",
    "\n",
    "---\n",
    "\n",
    "### Common RAG Failure Patterns\n",
    "\n",
    "| Failure                | Metric That Detects It |\n",
    "| ---------------------- | ---------------------- |\n",
    "| Wrong documents        | Recall@K               |\n",
    "| Too much noise         | Precision@K            |\n",
    "| Hallucinations         | Faithfulness           |\n",
    "| Incomplete answers     | Context Coverage       |\n",
    "| Over-confident answers | Attribution accuracy   |\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "RAG Quality =\n",
    "Did we fetch the right knowledge?\n",
    "Did we give enough knowledge?\n",
    "Did the model stick to that knowledge?\n",
    "```\n",
    "\n",
    "If any answer is **no**, the RAG system failed.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* RAG metrics evaluate **evidence usage**, not language quality\n",
    "* Faithfulness is the most critical metric\n",
    "* Retrieval failure guarantees generation failure\n",
    "* Hallucination detection is mandatory in production\n",
    "* These metrics protect trust, safety, and compliance\n",
    "\n",
    "---\n",
    "\n",
    "If you want next:\n",
    "\n",
    "* **Safety & alignment metrics**\n",
    "* **RAG regression testing**\n",
    "* **Production RAG metric dashboards**\n",
    "* **How to auto-regenerate answers on low faithfulness**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
