{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de4ed27",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Intrinsic (Model-Centric) Metrics\n",
    "\n",
    "*Used during training & research*\n",
    "\n",
    "| Metric                  | Purpose                            |\n",
    "| ----------------------- | ---------------------------------- |\n",
    "| Perplexity          | How well the model predicts tokens |\n",
    "| Cross-Entropy Loss      | Training optimization objective    |\n",
    "| Negative Log-Likelihood | Token probability accuracy         |\n",
    "| Bits-per-Token          | Compression efficiency             |\n",
    "| Entropy                 | Uncertainty of predictions         |\n",
    "\n",
    "These answer:\n",
    "\n",
    "> *Is this a good language model mathematically?*\n",
    "\n",
    "\n",
    "**Model intrinsic metrics** measure the **internal quality of the language model itself**, independent of any downstream task.\n",
    "\n",
    "They answer one question:\n",
    "\n",
    "> **How good is this model as a probabilistic language model?**\n",
    "\n",
    "They are mainly used during:\n",
    "\n",
    "* Pretraining\n",
    "* Fine-tuning\n",
    "* Research benchmarking\n",
    "\n",
    "Not for production product evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Model Intrinsic Metrics\n",
    "\n",
    "| Metric                  | What It Measures                           |\n",
    "| ----------------------- | ------------------------------------------ |\n",
    "| Perplexity              | How well the model predicts the next token |\n",
    "| Cross-Entropy Loss      | Training optimization objective            |\n",
    "| Negative Log-Likelihood | Log probability of tokens                  |\n",
    "| Bits-per-Token          | Compression efficiency                     |\n",
    "| Entropy                 | Uncertainty of predictions                 |\n",
    "\n",
    "---\n",
    "\n",
    "### Why Intrinsic Metrics Matter\n",
    "\n",
    "Intrinsic metrics:\n",
    "\n",
    "* Track training progress\n",
    "* Compare base models\n",
    "* Diagnose under/overfitting\n",
    "* Guide architecture changes\n",
    "\n",
    "They **do not** measure usefulness, safety, or business success.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Metric: Perplexity (Demonstration)\n",
    "\n",
    "#### Formula\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = e^{\\text{Cross-Entropy Loss}}\n",
    "$$\n",
    "\n",
    "Lower = better language modeling.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Demonstration (PyTorch-style)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Example predicted token probabilities\n",
    "logits = torch.tensor([[2.0, 0.5, 0.1]])  # model output\n",
    "target = torch.tensor([0])               # correct token index\n",
    "\n",
    "# Compute cross-entropy loss\n",
    "loss = F.cross_entropy(logits, target)\n",
    "\n",
    "# Compute perplexity\n",
    "perplexity = math.exp(loss.item())\n",
    "\n",
    "print(\"Cross-Entropy Loss:\", loss.item())\n",
    "print(\"Perplexity:\", perplexity)\n",
    "```\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "* Lower loss → lower perplexity → better model\n",
    "\n",
    "---\n",
    "\n",
    "### Negative Log-Likelihood (NLL)\n",
    "\n",
    "```python\n",
    "log_probs = F.log_softmax(logits, dim=1)\n",
    "nll = -log_probs[0, target]\n",
    "```\n",
    "\n",
    "Measures how surprised the model is by the correct token.\n",
    "\n",
    "---\n",
    "\n",
    "### Bits-per-Token\n",
    "\n",
    "```python\n",
    "bits_per_token = loss / math.log(2)\n",
    "```\n",
    "\n",
    "Lower bits → more efficient language representation.\n",
    "\n",
    "---\n",
    "\n",
    "### Entropy (Uncertainty)\n",
    "\n",
    "```python\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "entropy = -(probs * torch.log(probs)).sum()\n",
    "```\n",
    "\n",
    "High entropy = model unsure\n",
    "Low entropy = confident predictions\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Intrinsic Metrics\n",
    "\n",
    "| Phase                       | Used? |\n",
    "| --------------------------- | ----- |\n",
    "| Pretraining                 | ✅     |\n",
    "| Fine-tuning                 | ✅     |\n",
    "| Model architecture research | ✅     |\n",
    "| Prompt evaluation           | ❌     |\n",
    "| RAG systems                 | ❌     |\n",
    "| Chatbot quality             | ❌     |\n",
    "\n",
    "---\n",
    "\n",
    "### Why Intrinsic Metrics Fail for Real Systems\n",
    "\n",
    "A model with low perplexity can still:\n",
    "\n",
    "* Hallucinate\n",
    "* Be unsafe\n",
    "* Be irrelevant\n",
    "* Be useless for business tasks\n",
    "\n",
    "Hence intrinsic metrics ≠ product quality.\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "Intrinsic Metrics → Model Fitness\n",
    "Extrinsic Metrics → System Success\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Task & Quality Metrics\n",
    "\n",
    "*Used in real applications*\n",
    "\n",
    "| Metric       | Purpose                      |\n",
    "| ------------ | ---------------------------- |\n",
    "| Correctness  | Factual accuracy             |\n",
    "| Relevance    | Question–answer alignment    |\n",
    "| Completeness | Coverage of requirements     |\n",
    "| Fluency      | Grammar & naturalness        |\n",
    "| Coherence    | Logical structure            |\n",
    "| Consistency  | Stability across runs        |\n",
    "| Specificity  | Avoids generic answers       |\n",
    "| Conciseness  | Avoids unnecessary verbosity |\n",
    "\n",
    "---\n",
    "\n",
    "### RAG & Knowledge Grounding Metrics\n",
    "\n",
    "| Metric                      | Purpose                     |\n",
    "| --------------------------- | --------------------------- |\n",
    "| Retrieval Precision@K       | Relevant docs ratio         |\n",
    "| Retrieval Recall@K          | Was required info retrieved |\n",
    "| MRR                         | Ranking quality             |\n",
    "| Context Relevance           | Context usefulness          |\n",
    "| Faithfulness / Groundedness | Hallucination detection     |\n",
    "| Hallucination Rate          | % unsupported claims        |\n",
    "| Context Coverage            | Info completeness           |\n",
    "| Attribution Accuracy        | Traceability to sources     |\n",
    "\n",
    "---\n",
    "\n",
    "### Safety & Alignment Metrics\n",
    "\n",
    "| Metric               | Purpose                |\n",
    "| -------------------- | ---------------------- |\n",
    "| Toxicity Score       | Harmful language       |\n",
    "| Bias Score           | Fairness               |\n",
    "| Policy Compliance    | Rule adherence         |\n",
    "| Jailbreak Resistance | Attack robustness      |\n",
    "| Refusal Accuracy     | Proper denial behavior |\n",
    "| PII Leakage Rate     | Privacy risk           |\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "| Metric          | Purpose         |\n",
    "| --------------- | --------------- |\n",
    "| Latency         | Response time   |\n",
    "| p50 / p95 / p99 | Tail latency    |\n",
    "| TTFT            | Streaming speed |\n",
    "| Throughput      | Requests/sec    |\n",
    "| Error Rate      | Failure ratio   |\n",
    "| Availability    | Uptime          |\n",
    "\n",
    "---\n",
    "\n",
    "### Cost & Efficiency Metrics\n",
    "\n",
    "| Metric           | Purpose               |\n",
    "| ---------------- | --------------------- |\n",
    "| Token Usage      | Cost driver           |\n",
    "| Cost per Request | Dollar spend          |\n",
    "| Cost per User    | Monetization          |\n",
    "| Retry Cost       | Reliability indicator |\n",
    "| Fallback Cost    | Capacity indicator    |\n",
    "| Cache Hit Ratio  | Cost optimization     |\n",
    "\n",
    "---\n",
    "\n",
    "### Reliability & Stability Metrics\n",
    "\n",
    "| Metric                 | Purpose             |\n",
    "| ---------------------- | ------------------- |\n",
    "| Regression Delta       | Quality change      |\n",
    "| Consistency Drift      | Stability over time |\n",
    "| Model Degradation Rate | Long-term health    |\n",
    "| Retry Rate             | Infra health        |\n",
    "| Fallback Rate          | Capacity health     |\n",
    "| Timeout Rate           | SLA risk            |\n",
    "\n",
    "---\n",
    "\n",
    "### User Experience Metrics\n",
    "\n",
    "| Metric                  | Purpose                  |\n",
    "| ----------------------- | ------------------------ |\n",
    "| User Satisfaction Score | Human rating             |\n",
    "| Resolution Rate         | Task success             |\n",
    "| Escalation Rate         | Automation effectiveness |\n",
    "| Conversation Length     | Efficiency               |\n",
    "| Abandonment Rate        | UX health                |\n",
    "\n",
    "---\n",
    "\n",
    "### Final Mental Model\n",
    "\n",
    "```\n",
    "Training Phase → Intrinsic Metrics (Perplexity, Loss)\n",
    "Production Phase → All Other Metrics\n",
    "```\n",
    "\n",
    "Perplexity belongs **only** in **Intrinsic Metrics**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
