{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e3192f",
   "metadata": {},
   "source": [
    "\n",
    "## Task & Quality Metrics\n",
    "\n",
    "\n",
    "**Task & Quality metrics** measure **how good the model’s output is for the user’s task**.\n",
    "\n",
    "They answer:\n",
    "\n",
    "> **Did the model actually solve the problem correctly and well?**\n",
    "\n",
    "These metrics are:\n",
    "\n",
    "* Model-agnostic\n",
    "* Task-dependent\n",
    "* Used in **product evaluation, regression testing, and deployment gating**\n",
    "\n",
    "---\n",
    "\n",
    "### Core Task & Quality Metrics\n",
    "\n",
    "| Metric           | What It Measures                |\n",
    "| ---------------- | ------------------------------- |\n",
    "| **Correctness**  | Factual accuracy                |\n",
    "| **Relevance**    | Alignment with the question     |\n",
    "| **Completeness** | Coverage of all required points |\n",
    "| **Coherence**    | Logical flow                    |\n",
    "| **Fluency**      | Language quality                |\n",
    "| **Consistency**  | Stability across runs           |\n",
    "| **Specificity**  | Avoids generic answers          |\n",
    "| **Conciseness**  | Avoids unnecessary verbosity    |\n",
    "\n",
    "---\n",
    "\n",
    "### Why These Metrics Matter\n",
    "\n",
    "A response can be:\n",
    "\n",
    "* Fluent ❌\n",
    "* Confident ❌\n",
    "* Well-structured ❌\n",
    "* **Wrong** ❌\n",
    "\n",
    "Task & quality metrics ensure outputs are **useful, reliable, and trustworthy**.\n",
    "\n",
    "---\n",
    "\n",
    "### Demonstration Setup\n",
    "\n",
    "#### Example Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5acdec0",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "samples = [\n",
    "    {\n",
    "        \"question\": \"What is RAG?\",\n",
    "        \"ground_truth\": \"RAG combines document retrieval with language model generation.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is token streaming?\",\n",
    "        \"ground_truth\": \"Token streaming sends output tokens incrementally.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d46656",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Model Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "476105b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"Answer clearly: {question}\")\n",
    "chain = prompt | llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb552f0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Generate Predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0898d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for s in samples:\n",
    "    out = chain.invoke({\"question\": s[\"question\"]})\n",
    "    results.append({**s, \"prediction\": out.content})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12960ea6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluate Task & Quality Metrics\n",
    "\n",
    "#### Correctness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c032b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctness: 0\n",
      "Correctness: 1\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.evaluation import load_evaluator\n",
    "\n",
    "correctness_eval = load_evaluator(\"labeled_criteria\", criteria=\"correctness\")\n",
    "\n",
    "for r in results:\n",
    "    score = correctness_eval.evaluate_strings(\n",
    "        input=r[\"question\"],\n",
    "        prediction=r[\"prediction\"],\n",
    "        reference=r[\"ground_truth\"]\n",
    "    )\n",
    "    print(\"Correctness:\", score[\"score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3945305",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Relevance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f575eb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance: 0\n",
      "Relevance: 0\n"
     ]
    }
   ],
   "source": [
    "relevance_eval = load_evaluator(\"criteria\", criteria=\"relevance\")\n",
    "\n",
    "for r in results:\n",
    "    score = relevance_eval.evaluate_strings(\n",
    "        input=r[\"question\"],\n",
    "        prediction=r[\"prediction\"]\n",
    "    )\n",
    "    print(\"Relevance:\", score[\"score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3486d0a1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Completeness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2503988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness: 0\n",
      "Completeness: 1\n"
     ]
    }
   ],
   "source": [
    "completeness_eval = load_evaluator(\n",
    "    \"labeled_criteria\", \n",
    "    criteria={\n",
    "        \"completeness\": \"Does the submission cover all key points from the reference answer?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    score = completeness_eval.evaluate_strings(\n",
    "        input=r[\"question\"],\n",
    "        prediction=r[\"prediction\"],\n",
    "        reference=r[\"ground_truth\"]\n",
    "    )\n",
    "    print(\"Completeness:\", score[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc6306",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Coherence & Fluency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2c91695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence: 1\n",
      "Fluency: 1\n",
      "Coherence: 1\n",
      "Fluency: 1\n"
     ]
    }
   ],
   "source": [
    "coherence_eval = load_evaluator(\"criteria\", criteria=\"coherence\")\n",
    "fluency_eval   = load_evaluator(\"criteria\", criteria={\n",
    "    \"fluency\": \"Is the submission well-written, grammatically correct, and easy to read?\"\n",
    "})\n",
    "\n",
    "for r in results:\n",
    "    print(\"Coherence:\", coherence_eval.evaluate_strings(\n",
    "        input=r[\"question\"], prediction=r[\"prediction\"])[\"score\"])\n",
    "    \n",
    "    print(\"Fluency:\", fluency_eval.evaluate_strings(\n",
    "        input=r[\"question\"], prediction=r[\"prediction\"])[\"score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969945f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Consistency (Multi-run)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71c6bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency variation: 3\n"
     ]
    }
   ],
   "source": [
    "answers = [chain.invoke({\"question\": \"What is RAG?\"}).content for _ in range(3)]\n",
    "print(\"Consistency variation:\", len(set(answers)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5b10d",
   "metadata": {},
   "source": [
    "\n",
    "Lower variation → higher consistency.\n",
    "\n",
    "---\n",
    "\n",
    "#### Specificity & Conciseness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15713c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity_eval = load_evaluator(\"criteria\", criteria={\n",
    "\t\"specificity\": \"Does the submission provide specific details rather than generic or vague statements?\"\n",
    "})\n",
    "conciseness_eval = load_evaluator(\"criteria\", criteria=\"conciseness\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7de8b1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Aggregating Quality Score\n",
    "\n",
    "```python\n",
    "final_quality = (\n",
    "    0.3 * correctness +\n",
    "    0.25 * relevance +\n",
    "    0.2 * completeness +\n",
    "    0.15 * coherence +\n",
    "    0.1 * fluency\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### How Companies Use This\n",
    "\n",
    "* Pre-release validation\n",
    "* Regression detection\n",
    "* Prompt tuning\n",
    "* Model comparison\n",
    "* Automated gating in CI/CD\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "Task & Quality Metrics =\n",
    "Does it work + Is it useful + Is it readable + Is it reliable\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* These metrics evaluate **actual output quality**\n",
    "* They are independent of the model internals\n",
    "* They form the backbone of LLM testing & deployment\n",
    "* Must be automated & tracked continuously\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
