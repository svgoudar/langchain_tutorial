{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c1be544",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Relevance\n",
    "\n",
    "**Relevance** measures **how well an LLM’s answer addresses the user’s question**.\n",
    "It checks **alignment**, not truth or grounding.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "> **Does the answer actually answer what was asked?**\n",
    "\n",
    "Relevance is a core evaluation dimension in LangChain–based evaluation pipelines and applies to **LLMs, RAG systems, and agents**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Relevance Matters\n",
    "\n",
    "An answer can be:\n",
    "\n",
    "* Factually correct ❌\n",
    "* Well grounded ❌\n",
    "* Completely off-topic ❌\n",
    "\n",
    "Relevance ensures:\n",
    "\n",
    "* User intent is satisfied\n",
    "* No unnecessary or unrelated content\n",
    "* High UX quality\n",
    "\n",
    "In production, **low relevance feels like failure**, even if the answer is correct.\n",
    "\n",
    "---\n",
    "\n",
    "### Relevance vs Faithfulness vs Correctness\n",
    "\n",
    "| Metric       | Checks                       |\n",
    "| ------------ | ---------------------------- |\n",
    "| Relevance    | Does it answer the question? |\n",
    "| Faithfulness | Is it supported by context?  |\n",
    "| Correctness  | Is it factually right?       |\n",
    "\n",
    "An answer can be:\n",
    "\n",
    "* Relevant but unfaithful\n",
    "* Faithful but irrelevant\n",
    "* Correct but irrelevant\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Flow\n",
    "\n",
    "![Image](https://images.ctfassets.net/otwaplf7zuwf/2aXqN1u0QPT1ST23Na7r8Y/03a9cb3f9206ed66362adef4ebcd3631/image.png)\n",
    "\n",
    "![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AYXv5YPOI0U9tWP7AG-gNJw.png)\n",
    "\n",
    "![Image](https://www.deepchecks.com/wp-content/uploads/2025/04/img-rag-evaluation.jpg)\n",
    "\n",
    "```\n",
    "User Question\n",
    "   ↓\n",
    "LLM Answer\n",
    "   ↓\n",
    "Relevance Check\n",
    "   → On-topic ✅\n",
    "   → Off-topic ❌\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Example Question\n",
    "\n",
    "#### Question\n",
    "\n",
    "```text\n",
    "What is Retrieval Augmented Generation (RAG)?\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Relevant vs Irrelevant Answers\n",
    "\n",
    "#### Relevant Answer ✅\n",
    "\n",
    "```text\n",
    "RAG combines document retrieval with language model generation to answer questions.\n",
    "```\n",
    "\n",
    "✔ Directly answers *what RAG is*.\n",
    "\n",
    "---\n",
    "\n",
    "### Irrelevant Answer ❌\n",
    "\n",
    "```text\n",
    "Large language models are trained on massive datasets using transformers.\n",
    "```\n",
    "\n",
    "❌ Factually correct\n",
    "❌ Completely unrelated to the question\n",
    "\n",
    "---\n",
    "\n",
    "### Relevance Evaluation Using LangChain\n",
    "\n",
    "#### Load a Relevance Evaluator\n",
    "\n",
    "```python\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "relevance_eval = load_evaluator(\n",
    "    \"criteria\",\n",
    "    criteria=\"relevance\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluate a Relevant Answer\n",
    "\n",
    "```python\n",
    "result = relevance_eval.evaluate_strings(\n",
    "    input=\"What is RAG?\",\n",
    "    prediction=\"RAG combines document retrieval with language model generation.\"\n",
    ")\n",
    "\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Example Output**\n",
    "\n",
    "```python\n",
    "{\"score\": 1.0, \"reasoning\": \"The answer directly addresses the question.\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluate an Irrelevant Answer\n",
    "\n",
    "```python\n",
    "result = relevance_eval.evaluate_strings(\n",
    "    input=\"What is RAG?\",\n",
    "    prediction=\"Transformers are neural network architectures for NLP.\"\n",
    ")\n",
    "\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Example Output**\n",
    "\n",
    "```python\n",
    "{\"score\": 0.0, \"reasoning\": \"The answer does not address the question.\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Partial Relevance (Common Case)\n",
    "\n",
    "#### Partially Relevant Answer ⚠️\n",
    "\n",
    "```text\n",
    "RAG uses retrieval, and transformers are commonly used in NLP models.\n",
    "```\n",
    "\n",
    "* First clause: relevant\n",
    "* Second clause: unnecessary\n",
    "\n",
    "Typical score:\n",
    "\n",
    "```\n",
    "0.4 – 0.6\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Relevance in RAG Pipelines\n",
    "\n",
    "#### Where Relevance Is Applied\n",
    "\n",
    "In RAG, relevance is checked at **two levels**:\n",
    "\n",
    "| Level               | What Is Checked                          |\n",
    "| ------------------- | ---------------------------------------- |\n",
    "| Answer relevance    | Does the answer match the question?      |\n",
    "| Retrieval relevance | Are retrieved docs related to the query? |\n",
    "\n",
    "Answer relevance ≠ Retrieval relevance.\n",
    "\n",
    "---\n",
    "\n",
    "### Relevance Check on RAG Output\n",
    "\n",
    "```python\n",
    "relevance_eval.evaluate_strings(\n",
    "    input=question,\n",
    "    prediction=rag_answer\n",
    ")\n",
    "```\n",
    "\n",
    "This ensures:\n",
    "\n",
    "* The model didn’t drift\n",
    "* The answer stayed focused\n",
    "\n",
    "---\n",
    "\n",
    "### Common Relevance Failure Patterns\n",
    "\n",
    "| Pattern               | Example                           |\n",
    "| --------------------- | --------------------------------- |\n",
    "| Topic drift           | Answer shifts mid-way             |\n",
    "| Over-generalization   | Talks broadly, not specifically   |\n",
    "| Template leakage      | Generic explanation               |\n",
    "| Keyword matching only | Mentions terms but doesn’t answer |\n",
    "\n",
    "---\n",
    "\n",
    "### Improving Relevance\n",
    "\n",
    "* Use intent-focused prompts\n",
    "  *“Answer **only** what is asked.”*\n",
    "* Penalize verbosity\n",
    "* Use relevance thresholds\n",
    "* Combine with faithfulness\n",
    "* Reject low-relevance outputs\n",
    "\n",
    "---\n",
    "\n",
    "### Relevance Thresholds (Typical)\n",
    "\n",
    "| Score     | Action              |\n",
    "| --------- | ------------------- |\n",
    "| ≥ 0.8     | Accept              |\n",
    "| 0.5 – 0.8 | Review / regenerate |\n",
    "| < 0.5     | Reject              |\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "Relevance answers:\n",
    "\n",
    "> **“If I read only the question and the answer, do they match?”**\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Relevance measures **question–answer alignment**\n",
    "* Independent of correctness and faithfulness\n",
    "* Essential for good UX\n",
    "* Evaluated using LLM-as-a-judge\n",
    "* Mandatory metric for RAG and agents"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
