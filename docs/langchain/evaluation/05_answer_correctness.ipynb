{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388eb987",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Answer Correctness\n",
    "\n",
    "\n",
    "**Answer correctness** measures whether an LLM’s output is **factually correct with respect to an expected ground truth**, regardless of *where the information came from*.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "> **Is the answer right?**\n",
    "\n",
    "Answer correctness is a core evaluation metric in LangChain–based evaluation pipelines and applies to:\n",
    "\n",
    "* Plain LLM Q&A\n",
    "* RAG outputs (after faithfulness)\n",
    "* Agent decisions\n",
    "* Prompt/model regression testing\n",
    "\n",
    "---\n",
    "\n",
    "### Why Answer Correctness Matters\n",
    "\n",
    "An answer can be:\n",
    "\n",
    "* Relevant ❌\n",
    "* Faithful ❌\n",
    "* Fluent ❌\n",
    "  and still be **wrong**.\n",
    "\n",
    "Correctness ensures:\n",
    "\n",
    "* Factual accuracy\n",
    "* Trust in system outputs\n",
    "* Safe deployment in enterprise use cases\n",
    "\n",
    "In many systems, **correctness is the final gate** before shipping.\n",
    "\n",
    "---\n",
    "\n",
    "### Correctness vs Relevance vs Faithfulness\n",
    "\n",
    "| Metric       | Checks                              |\n",
    "| ------------ | ----------------------------------- |\n",
    "| Correctness  | Is the answer factually right?      |\n",
    "| Relevance    | Does it answer the question?        |\n",
    "| Faithfulness | Is it grounded in provided context? |\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Correct but unfaithful → Uses outside knowledge\n",
    "* Faithful but incorrect → Context itself is wrong\n",
    "* Relevant but incorrect → Answers the question wrongly\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Flow\n",
    "\n",
    "![Image](https://images.ctfassets.net/otwaplf7zuwf/2tNy3bcdnxBV6ced1QEjcW/149cebc79f9215159e79d1ac9836bc5f/image.png)\n",
    "\n",
    "![Image](https://images.ctfassets.net/otwaplf7zuwf/77WwI4e8hpjjIzazrSEdTp/95391e9b0e5d9c39a1d57690ebdcdea9/image.png)\n",
    "\n",
    "![Image](https://chatgen.ai/wp-content/uploads/2023/12/ezgif-2-dec5b52644-1024x566.jpeg)\n",
    "\n",
    "```\n",
    "Question + Expected Answer\n",
    "        ↓\n",
    "   LLM Answer\n",
    "        ↓\n",
    " Correctness Check\n",
    "   → Correct ✅\n",
    "   → Incorrect ❌\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example Question and Ground Truth\n",
    "\n",
    "#### Question\n",
    "\n",
    "```text\n",
    "What is Retrieval Augmented Generation (RAG)?\n",
    "```\n",
    "\n",
    "#### Expected (Ground Truth) Answer\n",
    "\n",
    "```text\n",
    "RAG combines document retrieval with language model generation.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Correct vs Incorrect Answers\n",
    "\n",
    "#### Correct Answer ✅\n",
    "\n",
    "```text\n",
    "RAG combines document retrieval with language model generation.\n",
    "```\n",
    "\n",
    "✔ Matches the expected meaning\n",
    "✔ Factually accurate\n",
    "\n",
    "---\n",
    "\n",
    "### Incorrect Answer ❌\n",
    "\n",
    "```text\n",
    "RAG fine-tunes large language models during inference.\n",
    "```\n",
    "\n",
    "❌ Factually wrong\n",
    "❌ Contradicts known definition\n",
    "\n",
    "---\n",
    "\n",
    "### Answer Correctness Evaluation Using LangChain\n",
    "\n",
    "#### Load a Correctness Evaluator\n",
    "\n",
    "```python\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "correctness_eval = load_evaluator(\n",
    "    \"labeled_criteria\",\n",
    "    criteria=\"correctness\"\n",
    ")\n",
    "```\n",
    "\n",
    "This uses an **LLM-as-a-judge** to score correctness.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluate a Correct Answer\n",
    "\n",
    "```python\n",
    "result = correctness_eval.evaluate_strings(\n",
    "    input=\"What is RAG?\",\n",
    "    prediction=\"RAG combines document retrieval with language model generation.\",\n",
    "    reference=\"RAG combines document retrieval with language model generation.\"\n",
    ")\n",
    "\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Example Output**\n",
    "\n",
    "```python\n",
    "{\"score\": 1.0, \"reasoning\": \"The answer is factually correct.\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluate an Incorrect Answer\n",
    "\n",
    "```python\n",
    "result = correctness_eval.evaluate_strings(\n",
    "    input=\"What is RAG?\",\n",
    "    prediction=\"RAG fine-tunes models during inference.\",\n",
    "    reference=\"RAG combines document retrieval with language model generation.\"\n",
    ")\n",
    "\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Example Output**\n",
    "\n",
    "```python\n",
    "{\"score\": 0.0, \"reasoning\": \"The answer contains factual errors.\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Partial Correctness (Very Common)\n",
    "\n",
    "#### Partially Correct Answer ⚠️\n",
    "\n",
    "```text\n",
    "RAG retrieves documents and uses them with an LLM.\n",
    "```\n",
    "\n",
    "✔ Core idea present\n",
    "❌ Missing full definition\n",
    "\n",
    "Typical score:\n",
    "\n",
    "```\n",
    "0.6 – 0.8\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Correctness in RAG Pipelines\n",
    "\n",
    "In RAG systems, correctness is evaluated **after**:\n",
    "\n",
    "1. Retrieval quality\n",
    "2. Faithfulness (groundedness)\n",
    "\n",
    "```python\n",
    "correctness_eval.evaluate_strings(\n",
    "    input=question,\n",
    "    prediction=rag_answer,\n",
    "    reference=expected_answer\n",
    ")\n",
    "```\n",
    "\n",
    "Correctness ensures:\n",
    "\n",
    "* Even grounded answers are **factually right**\n",
    "* Outdated or misleading sources are detected\n",
    "\n",
    "---\n",
    "\n",
    "### Common Correctness Failure Patterns\n",
    "\n",
    "| Pattern             | Example                 |\n",
    "| ------------------- | ----------------------- |\n",
    "| Outdated facts      | Old version numbers     |\n",
    "| Confident errors    | Fluent but wrong        |\n",
    "| Over-generalization | Missing key constraints |\n",
    "| Misinterpretation   | Wrong definition        |\n",
    "\n",
    "---\n",
    "\n",
    "### Improving Answer Correctness\n",
    "\n",
    "* Maintain high-quality ground truth datasets\n",
    "* Use multiple evaluators (ensemble judging)\n",
    "* Combine with relevance + faithfulness\n",
    "* Reject low-score outputs\n",
    "* Add human review for edge cases\n",
    "\n",
    "---\n",
    "\n",
    "### Typical Correctness Thresholds\n",
    "\n",
    "| Score     | Action |\n",
    "| --------- | ------ |\n",
    "| ≥ 0.8     | Accept |\n",
    "| 0.6 – 0.8 | Review |\n",
    "| < 0.6     | Reject |\n",
    "\n",
    "Thresholds depend on **risk level** of the application.\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "Answer correctness answers one question:\n",
    "\n",
    "> **“If a domain expert reads this answer, would they say it’s right?”**\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Answer correctness measures **factual accuracy**\n",
    "* Independent of relevance and faithfulness\n",
    "* Evaluated using LLM-as-a-judge + ground truth\n",
    "* Critical for regression testing and production readiness\n",
    "* Final quality gate in most LLM systems\n",
    "\n",
    "If you want next:\n",
    "\n",
    "* **Combining correctness + faithfulness**\n",
    "* **Human vs automated correctness evaluation**\n",
    "* **CI pipelines for correctness regression testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43c9a8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
