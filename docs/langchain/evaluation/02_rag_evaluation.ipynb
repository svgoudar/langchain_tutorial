{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f044b645",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## RAG Evaluation \n",
    "\n",
    "**RAG (Retrieval-Augmented Generation) evaluation** measures whether a RAG system:\n",
    "\n",
    "1. **Retrieves the right documents**\n",
    "2. **Uses retrieved context correctly**\n",
    "3. **Avoids hallucinations**\n",
    "4. **Produces accurate, relevant answers**\n",
    "\n",
    "RAG evaluation is **not just LLM evaluation**.\n",
    "It evaluates **retrieval + generation together and separately**.\n",
    "\n",
    "Supported natively in LangChain and visualized via LangSmith.\n",
    "\n",
    "---\n",
    "\n",
    "### Why RAG Evaluation Is Different from LLM Evaluation\n",
    "\n",
    "| Aspect              | LLM Eval       | RAG Eval         |\n",
    "| ------------------- | -------------- | ---------------- |\n",
    "| Focus               | Answer quality | Answer + sources |\n",
    "| Hallucination check | Weak           | Strong           |\n",
    "| Retrieval quality   | ❌              | ✅                |\n",
    "| Context grounding   | ❌              | ✅                |\n",
    "| Explainability      | Low            | High             |\n",
    "\n",
    "A RAG answer can be **well-written but wrong** if retrieval fails.\n",
    "\n",
    "---\n",
    "\n",
    "### RAG Evaluation Dimensions\n",
    "\n",
    "RAG evaluation is usually split into **four layers**:\n",
    "\n",
    "| Layer             | What It Measures                  |\n",
    "| ----------------- | --------------------------------- |\n",
    "| Retrieval Quality | Are the right docs fetched?       |\n",
    "| Context Quality   | Is context sufficient & relevant? |\n",
    "| Faithfulness      | Is answer grounded in context?    |\n",
    "| Answer Quality    | Is the answer correct & helpful?  |\n",
    "\n",
    "---\n",
    "\n",
    "### RAG Evaluation Architecture\n",
    "\n",
    "![Image](https://miro.medium.com/v2/resize%3Afit%3A1200/1%2Ak3qP8mLd2NBB5Z9S9k13-A.png)\n",
    "\n",
    "![Image](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/687e38823874b267af004f9c_18_RAG_metrics-min.png)\n",
    "\n",
    "![Image](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/06/06/ML-16629-rag-architecture-1.jpg)\n",
    "\n",
    "```\n",
    "Question\n",
    "  ↓\n",
    "Retriever ──► Retrieved Docs\n",
    "  ↓\n",
    "Prompt + Context\n",
    "  ↓\n",
    "LLM Answer\n",
    "  ↓\n",
    "Evaluators (Retrieval, Faithfulness, QA)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Define a Simple RAG Pipeline\n",
    "\n",
    "#### RAG Chain Under Test\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the question using ONLY the context below.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"question\": lambda x: x,\n",
    "        \"context\": retriever\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Prepare an Evaluation Dataset\n",
    "\n",
    "#### RAG Evaluation Dataset\n",
    "\n",
    "```python\n",
    "rag_eval_data = [\n",
    "    {\n",
    "        \"question\": \"What is RAG?\",\n",
    "        \"expected_answer\": \"RAG combines document retrieval with text generation.\",\n",
    "        \"expected_sources\": [\"retrieval augmented generation\"]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Run RAG and Capture Outputs\n",
    "\n",
    "#### Generate RAG Outputs\n",
    "\n",
    "```python\n",
    "results = []\n",
    "\n",
    "for row in rag_eval_data:\n",
    "    response = rag_chain.invoke(row[\"question\"])\n",
    "    docs = retriever.invoke(row[\"question\"])\n",
    "\n",
    "    results.append({\n",
    "        \"question\": row[\"question\"],\n",
    "        \"answer\": response.content,\n",
    "        \"documents\": docs,\n",
    "        \"expected\": row[\"expected_answer\"]\n",
    "    })\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Retrieval Quality Evaluation\n",
    "\n",
    "#### Check If Correct Docs Were Retrieved\n",
    "\n",
    "```python\n",
    "def retrieval_recall(docs, expected_keywords):\n",
    "    text = \" \".join(d.page_content.lower() for d in docs)\n",
    "    return any(k in text for k in expected_keywords)\n",
    "\n",
    "for r in results:\n",
    "    score = retrieval_recall(\n",
    "        r[\"documents\"],\n",
    "        [\"retrieval\", \"generation\"]\n",
    "    )\n",
    "    print(\"Retrieval recall:\", score)\n",
    "```\n",
    "\n",
    "✔ Ensures **retrieval did not fail**\n",
    "❌ If this fails, generation quality is irrelevant\n",
    "\n",
    "---\n",
    "\n",
    "### Faithfulness (Groundedness) Evaluation\n",
    "\n",
    "#### Faithfulness Evaluator\n",
    "\n",
    "```python\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "faithfulness_eval = load_evaluator(\"faithfulness\")\n",
    "\n",
    "for r in results:\n",
    "    score = faithfulness_eval.evaluate_strings(\n",
    "        input=r[\"question\"],\n",
    "        prediction=r[\"answer\"],\n",
    "        reference=\"\\n\".join(d.page_content for d in r[\"documents\"])\n",
    "    )\n",
    "    print(\"Faithfulness score:\", score[\"score\"])\n",
    "```\n",
    "\n",
    "✔ Detects hallucinations\n",
    "✔ Confirms answer is **supported by retrieved docs**\n",
    "\n",
    "---\n",
    "\n",
    "### Answer Correctness Evaluation\n",
    "\n",
    "#### Correctness Scoring\n",
    "\n",
    "```python\n",
    "correctness_eval = load_evaluator(\n",
    "    \"labeled_criteria\",\n",
    "    criteria=\"correctness\"\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    score = correctness_eval.evaluate_strings(\n",
    "        input=r[\"question\"],\n",
    "        prediction=r[\"answer\"],\n",
    "        reference=r[\"expected\"]\n",
    "    )\n",
    "    print(\"Correctness:\", score[\"score\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Combined RAG Evaluation Score\n",
    "\n",
    "#### Aggregate Metrics\n",
    "\n",
    "```python\n",
    "final_score = (\n",
    "    0.4 * retrieval_score +\n",
    "    0.4 * faithfulness_score +\n",
    "    0.2 * correctness_score\n",
    ")\n",
    "```\n",
    "\n",
    "This reflects **industry practice**:\n",
    "\n",
    "* Retrieval & faithfulness matter more than fluency\n",
    "\n",
    "---\n",
    "\n",
    "### RAG Evaluation via Tracing (Production)\n",
    "\n",
    "Enable tracing:\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=true\n",
    "```\n",
    "\n",
    "Each RAG run captures:\n",
    "\n",
    "* Retrieved chunks\n",
    "* Prompt size\n",
    "* Faithfulness failures\n",
    "* Latency per step\n",
    "* Token cost\n",
    "\n",
    "Reviewed in LangSmith UI.\n",
    "\n",
    "---\n",
    "\n",
    "### Common RAG Failure Patterns (What Evaluation Finds)\n",
    "\n",
    "| Failure                   | Detected By     |\n",
    "| ------------------------- | --------------- |\n",
    "| Wrong documents retrieved | Retrieval eval  |\n",
    "| Too much context          | Latency + cost  |\n",
    "| Hallucinated facts        | Faithfulness    |\n",
    "| Partial answers           | Correctness     |\n",
    "| Outdated docs             | Retrieval audit |\n",
    "\n",
    "---\n",
    "\n",
    "### Best-Practice RAG Evaluation Strategy\n",
    "\n",
    "| Stage      | What to Evaluate   |\n",
    "| ---------- | ------------------ |\n",
    "| Chunking   | Recall / coverage  |\n",
    "| Retriever  | Precision & recall |\n",
    "| Prompt     | Faithfulness       |\n",
    "| Model      | Answer quality     |\n",
    "| Production | Drift + regression |\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "RAG evaluation answers **three questions**:\n",
    "\n",
    "```\n",
    "Did we fetch the right info?\n",
    "Did the model use it?\n",
    "Did it answer correctly?\n",
    "```\n",
    "\n",
    "If any answer is **no**, RAG failed.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* RAG evaluation ≠ LLM evaluation\n",
    "* Faithfulness is the most critical metric\n",
    "* Retrieval failure guarantees generation failure\n",
    "* Automated + dataset-based evaluation is mandatory\n",
    "* Required for production-grade RAG systems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ccedd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
