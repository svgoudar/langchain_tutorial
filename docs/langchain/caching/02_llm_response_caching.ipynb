{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f39bad",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## LLM Response Caching \n",
    "\n",
    "**LLM response caching** is the practice of storing the **final output produced by a language model** for a given prompt and reusing it when the same (or equivalent) prompt appears again.\n",
    "\n",
    "It prevents repeated model calls for identical work.\n",
    "\n",
    "**Resulting effects**\n",
    "\n",
    "* Lower latency\n",
    "* Lower API / compute cost\n",
    "* Higher request capacity\n",
    "* Increased reliability during traffic spikes\n",
    "\n",
    "---\n",
    "\n",
    "### Position in the Request Pipeline\n",
    "\n",
    "```\n",
    "Client Request\n",
    "      ↓\n",
    "Cache Lookup ── hit → Return Cached Response\n",
    "      ↓ miss\n",
    "Prompt Builder → LLM → Response → Store in Cache → Return\n",
    "```\n",
    "\n",
    "**Demonstration**\n",
    "\n",
    "Only when the cache misses does the system execute the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Cache Strategy: Exact Match\n",
    "\n",
    "#### Concept\n",
    "\n",
    "The **same normalized prompt** should always return the **same stored output**.\n",
    "\n",
    "#### Demonstration (Python)\n",
    "\n",
    "```python\n",
    "response_cache = {}\n",
    "\n",
    "def get_llm_response(prompt: str):\n",
    "    key = prompt.strip().lower()\n",
    "\n",
    "    if key in response_cache:\n",
    "        print(\"Cache Hit\")\n",
    "        return response_cache[key]\n",
    "\n",
    "    print(\"Cache Miss — Calling LLM\")\n",
    "    response = llm.invoke(prompt).content\n",
    "    response_cache[key] = response\n",
    "    return response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Semantic Response Caching (Similarity-Based)\n",
    "\n",
    "When prompts are not identical but **meaningfully similar**, embeddings allow reuse.\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "semantic_cache = []\n",
    "\n",
    "def get_semantic_response(prompt):\n",
    "    q_vec = model.encode(prompt)\n",
    "\n",
    "    for cached_prompt, vec, response in semantic_cache:\n",
    "        similarity = np.dot(q_vec, vec)\n",
    "        if similarity > 0.9:\n",
    "            print(\"Semantic Cache Hit\")\n",
    "            return response\n",
    "\n",
    "    print(\"Semantic Cache Miss — Calling LLM\")\n",
    "    response = llm.invoke(prompt).content\n",
    "    semantic_cache.append((prompt, q_vec, response))\n",
    "    return response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Cache Invalidation (Critical)\n",
    "\n",
    "Caching is only safe if **invalidated correctly**.\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "def invalidate_response_cache(key):\n",
    "    response_cache.pop(key, None)\n",
    "```\n",
    "\n",
    "**Invalidate when**\n",
    "\n",
    "* Prompt templates change\n",
    "* Model version changes\n",
    "* Knowledge base changes\n",
    "* Business rules change\n",
    "\n",
    "---\n",
    "\n",
    "### TTL-Based Production Cache\n",
    "\n",
    "Prevents stale content.\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "from cachetools import TTLCache\n",
    "\n",
    "response_cache = TTLCache(maxsize=10000, ttl=3600)\n",
    "```\n",
    "\n",
    "Responses expire automatically after one hour.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. What Should Be Cached\n",
    "\n",
    "| Layer        | Cache Content     |\n",
    "| ------------ | ----------------- |\n",
    "| Final output | LLM response text |\n",
    "| Retrieval    | RAG documents     |\n",
    "| Prompts      | Compiled prompt   |\n",
    "| Embeddings   | Query embeddings  |\n",
    "\n",
    "---\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "* Normalize prompt before hashing\n",
    "* Include **model name + version** in cache key\n",
    "* Apply TTL and eviction policy\n",
    "* Use Redis or Memcached for distributed systems\n",
    "* Track cache hit ratio\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "LLM Response Caching = Short-term memory for your AI system\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Response caching is one of the highest impact optimizations\n",
    "* Eliminates redundant LLM calls\n",
    "* Reduces cost and latency dramatically\n",
    "* Essential for scalable LLM architectures"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
