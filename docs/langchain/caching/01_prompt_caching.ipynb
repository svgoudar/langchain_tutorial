{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ec34e6",
   "metadata": {},
   "source": [
    "\n",
    "## Prompt Caching \n",
    "\n",
    "\n",
    "**Prompt caching** is a technique where **responses for previously seen prompts are stored** so that if the same (or very similar) prompt appears again, the system can return the cached result instead of calling the LLM.\n",
    "\n",
    "This improves:\n",
    "\n",
    "* **Latency**\n",
    "* **Cost efficiency**\n",
    "* **System throughput**\n",
    "\n",
    "---\n",
    "\n",
    "### Where Prompt Caching Fits\n",
    "\n",
    "```\n",
    "User Query\n",
    "   ↓\n",
    "Cache Lookup ── hit → return cached response\n",
    "   ↓ miss\n",
    "Prompt → LLM → Response → Store in Cache\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why Prompt Caching Matters\n",
    "\n",
    "| Benefit           | Impact                 |\n",
    "| ----------------- | ---------------------- |\n",
    "| Lower latency     | Faster responses       |\n",
    "| Reduced cost      | Fewer LLM calls        |\n",
    "| Higher throughput | Handles more users     |\n",
    "| Stability         | Protects during spikes |\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Prompt Caching\n",
    "\n",
    "| Type              | Description                                           |\n",
    "| ----------------- | ----------------------------------------------------- |\n",
    "| Exact Match       | Same prompt returns same answer                       |\n",
    "| Semantic Cache    | Similar prompts return similar answer                 |\n",
    "| Partial Cache     | Cache parts of the pipeline (e.g., retrieved context) |\n",
    "| Multi-layer Cache | Cache at retrieval, prompt, and response              |\n",
    "\n",
    "---\n",
    "\n",
    "### Demonstration\n",
    "\n",
    "---\n",
    "\n",
    "#### A. Simple Exact-Match Cache\n",
    "\n",
    "```python\n",
    "cache = {}\n",
    "\n",
    "def get_answer(prompt):\n",
    "    if prompt in cache:\n",
    "        return cache[prompt]\n",
    "\n",
    "    response = llm.invoke(prompt).content\n",
    "    cache[prompt] = response\n",
    "    return response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### B. Semantic Prompt Cache (Embedding-Based)\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "cache = []\n",
    "\n",
    "def semantic_cache(prompt):\n",
    "    emb = model.encode(prompt)\n",
    "    for p, e, ans in cache:\n",
    "        if np.dot(emb, e) > 0.9:\n",
    "            return ans\n",
    "    return None\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Cache Invalidation (Important)\n",
    "\n",
    "```python\n",
    "def invalidate_cache(key):\n",
    "    cache.pop(key, None)\n",
    "```\n",
    "\n",
    "Invalidate on:\n",
    "\n",
    "* Prompt changes\n",
    "* Model upgrades\n",
    "* Knowledge base updates\n",
    "\n",
    "---\n",
    "\n",
    "#### D. TTL-Based Cache\n",
    "\n",
    "```python\n",
    "from cachetools import TTLCache\n",
    "\n",
    "cache = TTLCache(maxsize=1000, ttl=3600)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### What Should Be Cached?\n",
    "\n",
    "| Layer            | Cache Candidate     |\n",
    "| ---------------- | ------------------- |\n",
    "| User queries     | Full responses      |\n",
    "| RAG retrieval    | Retrieved documents |\n",
    "| Prompt templates | Compiled prompts    |\n",
    "| Embeddings       | Query embeddings    |\n",
    "\n",
    "---\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "* Hash normalized prompts\n",
    "* Include model + prompt version in cache key\n",
    "* Use Redis for distributed cache\n",
    "* Apply TTL and eviction policies\n",
    "* Log cache hit ratio\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "Prompt Caching = Memory for your LLM system\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Prompt caching is one of the highest ROI optimizations\n",
    "* Reduces both cost and latency dramatically\n",
    "* Must be version-aware and invalidation-safe\n",
    "* Essential for scalable LLM applications\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
