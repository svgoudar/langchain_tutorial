{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f05518ff",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Embeddings Cache \n",
    "\n",
    "\n",
    "An **embeddings cache** stores the **vector representation** of previously processed text so the system does **not recompute embeddings** for the same content.\n",
    "\n",
    "This avoids repeated model calls and makes retrieval, search, and similarity operations much faster.\n",
    "\n",
    "---\n",
    "\n",
    "### Where It Fits in the Pipeline\n",
    "\n",
    "```\n",
    "Text Input\n",
    "   ↓\n",
    "Embedding Cache Lookup ── hit → Return Vector\n",
    "   ↓ miss\n",
    "Embedding Model → Vector → Store in Cache → Return\n",
    "```\n",
    "\n",
    "**Demonstration**\n",
    "\n",
    "Only cache misses require calling the embedding model.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Embeddings Caching Matters\n",
    "\n",
    "| Benefit            | Impact                           |\n",
    "| ------------------ | -------------------------------- |\n",
    "| Lower latency      | Faster retrieval & search        |\n",
    "| Lower cost         | Fewer embedding model calls      |\n",
    "| Higher throughput  | More concurrent users            |\n",
    "| Stable performance | Protection during traffic spikes |\n",
    "\n",
    "---\n",
    "\n",
    "### Simple Exact-Match Embedding Cache\n",
    "\n",
    "#### Demonstration (Python)\n",
    "\n",
    "```python\n",
    "embedding_cache = {}\n",
    "\n",
    "def get_embedding(text: str):\n",
    "    key = text.strip().lower()\n",
    "\n",
    "    if key in embedding_cache:\n",
    "        print(\"Embedding Cache Hit\")\n",
    "        return embedding_cache[key]\n",
    "\n",
    "    print(\"Embedding Cache Miss — Computing Embedding\")\n",
    "    vector = embedding_model.encode(text)\n",
    "    embedding_cache[key] = vector\n",
    "    return vector\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Persistent Cache Using Disk\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "import pickle, os, hashlib\n",
    "\n",
    "CACHE_FILE = \"embeddings.cache\"\n",
    "\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    with open(CACHE_FILE, \"rb\") as f:\n",
    "        embedding_cache = pickle.load(f)\n",
    "else:\n",
    "    embedding_cache = {}\n",
    "\n",
    "def cache_key(text):\n",
    "    return hashlib.sha256(text.encode()).hexdigest()\n",
    "\n",
    "def get_embedding(text):\n",
    "    key = cache_key(text)\n",
    "\n",
    "    if key in embedding_cache:\n",
    "        print(\"Disk Cache Hit\")\n",
    "        return embedding_cache[key]\n",
    "\n",
    "    vector = embedding_model.encode(text)\n",
    "    embedding_cache[key] = vector\n",
    "\n",
    "    with open(CACHE_FILE, \"wb\") as f:\n",
    "        pickle.dump(embedding_cache, f)\n",
    "\n",
    "    return vector\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### TTL-Based Embeddings Cache\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "from cachetools import TTLCache\n",
    "\n",
    "embedding_cache = TTLCache(maxsize=50000, ttl=86400)  # 24 hours\n",
    "\n",
    "def get_embedding(text):\n",
    "    if text in embedding_cache:\n",
    "        return embedding_cache[text]\n",
    "\n",
    "    vector = embedding_model.encode(text)\n",
    "    embedding_cache[text] = vector\n",
    "    return vector\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Distributed Cache (Production Pattern)\n",
    "\n",
    "```\n",
    "Application → Redis → Embedding Model\n",
    "```\n",
    "\n",
    "#### Demonstration (Conceptual)\n",
    "\n",
    "```python\n",
    "def get_embedding(text):\n",
    "    key = hash(text)\n",
    "\n",
    "    vector = redis.get(key)\n",
    "    if vector:\n",
    "        return deserialize(vector)\n",
    "\n",
    "    vector = embedding_model.encode(text)\n",
    "    redis.set(key, serialize(vector), ex=86400)\n",
    "    return vector\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Invalidation Strategy\n",
    "\n",
    "Invalidate when:\n",
    "\n",
    "* Embedding model changes\n",
    "* Tokenization rules change\n",
    "* Text normalization rules change\n",
    "\n",
    "---\n",
    "\n",
    "### What Should Be Cached\n",
    "\n",
    "| Layer      | Cached Data         |\n",
    "| ---------- | ------------------- |\n",
    "| Text input | Embedding vector    |\n",
    "| Chunks     | Chunk embeddings    |\n",
    "| Queries    | Query embeddings    |\n",
    "| Documents  | Document embeddings |\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "Embeddings Cache = Memory of all meaning your system has already computed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Embeddings are expensive — caching them is mandatory at scale\n",
    "* Combine exact-match cache + TTL + persistent storage\n",
    "* Include model version in the cache key\n",
    "* One of the highest ROI optimizations in any RAG system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51aadda",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
