{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dbd20e6",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Retry and Fallback\n",
    "\n",
    "In LLM pipelines, **retry** and **fallback** are **resilience mechanisms** used to handle failures such as:\n",
    "\n",
    "* Transient network errors\n",
    "* Rate limits\n",
    "* Timeouts\n",
    "* Model unavailability\n",
    "\n",
    "Both are first-class concepts in LangChain runnables.\n",
    "\n",
    "```\n",
    "Request\n",
    "  ↓\n",
    "Primary Runnable\n",
    "  ├─ success → return result\n",
    "  └─ failure\n",
    "        ├─ Retry (same runnable)\n",
    "        └─ Fallback (alternative runnable)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Retry vs Fallback (Conceptual Difference)\n",
    "\n",
    "| Mechanism    | Purpose                    | Strategy                  |\n",
    "| ------------ | -------------------------- | ------------------------- |\n",
    "| **Retry**    | Handle transient failures  | Try again (same runnable) |\n",
    "| **Fallback** | Handle persistent failures | Switch to backup runnable |\n",
    "\n",
    "They are often **used together**.\n",
    "\n",
    "---\n",
    "\n",
    "### Retry\n",
    "\n",
    "\n",
    "**Retry** automatically re-executes a runnable when it fails, based on:\n",
    "\n",
    "* Max attempts\n",
    "* Backoff strategy\n",
    "* Exception type\n",
    "\n",
    "Retry is ideal for **temporary issues**.\n",
    "\n",
    "---\n",
    "\n",
    "### Retry Demonstration (Runnable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25bdd40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry in LLMs (Large Language Models) refers to the process of re-running a failed inference or generation task multiple times in order to improve the model's output quality and accuracy. \n",
      "\n",
      "When an LLM encounters an error or produces a low-quality output during inference, the retry mechanism allows the model to try again with slightly different input data or parameters. This can help the model overcome obstacles such as sampling bias, noise in the data, or misinterpretation of context.\n",
      "\n",
      "Retry in LLMs can be implemented in various ways, such as by adjusting the randomness in sampling, changing the temperature hyperparameter, or fine-tuning the model based on feedback from previous attempts. By incorporating retry mechanisms, LLMs can improve their performance and generate more accurate and reliable outputs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI().with_retry(\n",
    "    stop_after_attempt=3,   # max retries\n",
    "    wait_exponential_jitter=True\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"Explain retry in LLMs\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9102e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**What happens**\n",
    "\n",
    "* If the first call fails → retry\n",
    "* Up to 3 attempts\n",
    "* Exponential backoff with jitter\n",
    "\n",
    "---\n",
    "\n",
    "### Retry in a RunnableSequence\n",
    "\n",
    "```python\n",
    "chain = (\n",
    "    prompt\n",
    "    | ChatOpenAI().with_retry(stop_after_attempt=2)\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is retry logic?\")\n",
    "```\n",
    "\n",
    "Retry applies **only to the runnable it is attached to**.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Retry Use Cases\n",
    "\n",
    "* API rate limits\n",
    "* Temporary network drops\n",
    "* Model overload\n",
    "* Intermittent infra issues\n",
    "\n",
    "---\n",
    "\n",
    "### Fallback\n",
    "\n",
    "\n",
    "**Fallback** defines **alternative runnables** to execute if the primary runnable fails.\n",
    "\n",
    "Fallback is ideal for:\n",
    "\n",
    "* Model outages\n",
    "* Cost-aware degradation\n",
    "* SLA guarantees\n",
    "\n",
    "---\n",
    "\n",
    "### Fallback Demonstration (Two Models)\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "primary_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "backup_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "llm_with_fallback = primary_llm.with_fallbacks([backup_llm])\n",
    "\n",
    "response = llm_with_fallback.invoke(\"Explain fallback strategy\")\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "**Execution logic**\n",
    "\n",
    "1. Try `gpt-4`\n",
    "2. If it fails → switch to `gpt-3.5`\n",
    "3. Return first successful response\n",
    "\n",
    "---\n",
    "\n",
    "### Multiple Fallbacks (Priority Order)\n",
    "\n",
    "```python\n",
    "llm = ChatOpenAI(model=\"gpt-4\").with_fallbacks([\n",
    "    ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
    "    ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "])\n",
    "```\n",
    "\n",
    "Fallbacks are tried **in order**.\n",
    "\n",
    "---\n",
    "\n",
    "### Retry + Fallback\n",
    "\n",
    "#### Combined Demonstration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6127689d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Retry\" and \"fallback\" are terms used in computing to describe methods for handling issues when an operation does not succeed on the first attempt.\\n\\nRetry is a strategy where a system attempts the same operation again if it fails the first time. This can be useful in cases where the failure was due to a temporary issue (like a network glitch). For instance, when you send a request to a server and the server does not respond, the client will often retry the same request instead of immediately indicating a failure.\\n\\nFallback, on the other hand, is a strategy where a system will try a different approach if the primary method fails. This can be useful in cases where the primary method is known to have reliability issues or in cases where the failure was due to a more permanent issue with the primary method. For example, when a main server fails, the system may have a fallback server to use instead.\\n\\nBoth are considered as part of error handling and resilience design in software and network engineering. It enables the system to continue functioning even when things go wrong, improving overall reliability and robustness.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 217, 'prompt_tokens': 12, 'total_tokens': 229, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-Cpzckh6nnMNTdOix7UxWTLzqlgAI5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4c0f-8b90-73b0-b0f9-bdc278dcc420-0', usage_metadata={'input_tokens': 12, 'output_tokens': 217, 'total_tokens': 229, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primary = ChatOpenAI(model=\"gpt-4\").with_retry(\n",
    "    stop_after_attempt=2\n",
    ")\n",
    "\n",
    "secondary = ChatOpenAI(model=\"gpt-3.5-turbo\").with_retry(\n",
    "    stop_after_attempt=2\n",
    ")\n",
    "\n",
    "llm = primary.with_fallbacks([secondary])\n",
    "\n",
    "llm.invoke(\"Explain retry and fallback\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7db32",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Execution order**\n",
    "\n",
    "1. GPT-4 attempt #1\n",
    "2. GPT-4 retry #2\n",
    "3. If still failing → GPT-3.5 attempt #1\n",
    "4. GPT-3.5 retry #2\n",
    "\n",
    "---\n",
    "\n",
    "### Retry + Fallback in RAG Pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3126f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Create a sample retriever and prompt for demonstration\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"LangChain supports retry and fallback mechanisms for resilience.\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question based on context:\\nContext: {context}\\nQuestion: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b7a1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain ensures reliability by supporting retry and fallback mechanisms for resilience.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 68, 'total_tokens': 81, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cpzgcz7q30LfaHuuei4EW1Q983cGq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4c13-3494-7893-8f06-12404a6ae923-0', usage_metadata={'input_tokens': 68, 'output_tokens': 13, 'total_tokens': 81, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever\n",
    "    }\n",
    "    | prompt\n",
    "    | ChatOpenAI().with_retry()\n",
    "        .with_fallbacks([ChatOpenAI(model=\"gpt-3.5-turbo\")])\n",
    ")\n",
    "chain.invoke(\"How does LangChain ensure reliability?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce3077f",
   "metadata": {},
   "source": [
    "\n",
    "This ensures:\n",
    "\n",
    "* Retrieval still happens\n",
    "* Generation is resilient\n",
    "\n",
    "---\n",
    "\n",
    "### Failure Scoping (Important)\n",
    "\n",
    "#### Where Retry/Fallback Applies\n",
    "\n",
    "| Level    | Behavior                                |\n",
    "| -------- | --------------------------------------- |\n",
    "| Runnable | Only that step retries                  |\n",
    "| Sequence | Downstream not executed on failure      |\n",
    "| Parallel | Any branch failure fails whole runnable |\n",
    "| Batch    | Per-input isolation                     |\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "#### When to Use Retry\n",
    "\n",
    "Use retry when:\n",
    "\n",
    "* Failures are transient\n",
    "* Same request is safe to repeat\n",
    "* You want transparent recovery\n",
    "\n",
    "Avoid retry when:\n",
    "\n",
    "* Failures are deterministic\n",
    "* Calls are expensive or stateful\n",
    "\n",
    "---\n",
    "\n",
    "#### When to Use Fallback\n",
    "\n",
    "Use fallback when:\n",
    "\n",
    "* You have alternative models/tools\n",
    "* SLA is critical\n",
    "* Graceful degradation is acceptable\n",
    "\n",
    "---\n",
    "\n",
    "#### Common Mistakes\n",
    "\n",
    "* Retrying indefinitely\n",
    "* Retrying non-idempotent operations\n",
    "* Not logging fallback usage\n",
    "* Using fallback without retry\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "Retry = **“try again”**\n",
    "Fallback = **“try something else”**\n",
    "\n",
    "Together they provide **fault tolerance**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Retry handles **temporary failures**\n",
    "* Fallback handles **permanent failures**\n",
    "* Both are composable at runnable level\n",
    "* Essential for production-grade LLM systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
