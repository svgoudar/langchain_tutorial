{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7522648",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## LlamaIndex Integration\n",
    "\n",
    "\n",
    "**LlamaIndex** is a data framework for building **retrieval-augmented generation (RAG)** systems.\n",
    "It connects your **external data** (documents, databases, files, APIs) with LLMs.\n",
    "\n",
    "It provides:\n",
    "\n",
    "* Data ingestion pipelines\n",
    "* Index structures\n",
    "* Query engines\n",
    "* Retrieval orchestration\n",
    "\n",
    "---\n",
    "\n",
    "### Where LlamaIndex Fits\n",
    "\n",
    "```\n",
    "Client → API / UI → LlamaIndex → Vector Store → LLM → Response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Basic LlamaIndex Workflow\n",
    "\n",
    "| Stage      | Purpose               |\n",
    "| ---------- | --------------------- |\n",
    "| Ingestion  | Load & chunk data     |\n",
    "| Indexing   | Convert to embeddings |\n",
    "| Storage    | Persist vectors       |\n",
    "| Query      | Retrieve context      |\n",
    "| Generation | LLM response          |\n",
    "\n",
    "---\n",
    "\n",
    "### Document Ingestion & Indexing\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "documents = SimpleDirectoryReader(\"docs\").load_data()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "index.storage_context.persist()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Querying the Index\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Explain neural networks\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Using Custom LLM & Embeddings\n",
    "\n",
    "```python\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "embed = OpenAIEmbedding()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, llm=llm, embed_model=embed)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Adding LlamaIndex to FastAPI\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/ask\")\n",
    "def ask(q: str):\n",
    "    response = query_engine.query(q)\n",
    "    return {\"answer\": str(response)}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Updating the Index\n",
    "\n",
    "```python\n",
    "new_docs = SimpleDirectoryReader(\"new_docs\").load_data()\n",
    "index.insert(new_docs)\n",
    "index.storage_context.persist()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced: Hybrid Search\n",
    "\n",
    "```python\n",
    "index = VectorStoreIndex.from_documents(documents, use_async=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "LlamaIndex = Data brain for your LLM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Bridges external knowledge with LLMs\n",
    "* Core engine for RAG systems\n",
    "* Handles ingestion, indexing, retrieval, and querying\n",
    "* Essential for knowledge-grounded AI"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
