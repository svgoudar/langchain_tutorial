{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98145585",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Output Validation \n",
    "\n",
    "\n",
    "**Output validation** is the process of **verifying, filtering, and correcting** the response generated by a system (especially an LLM) **before it is returned to the user**.\n",
    "\n",
    "It ensures the output is:\n",
    "\n",
    "* Safe\n",
    "* Correctly formatted\n",
    "* Policy-compliant\n",
    "* Free of forbidden or dangerous content\n",
    "\n",
    "---\n",
    "\n",
    "### Where It Fits in the Pipeline\n",
    "\n",
    "```\n",
    "User Request\n",
    "   ↓\n",
    "LLM Generates Output\n",
    "   ↓\n",
    "Output Validator ── valid → Return to User\n",
    "   ↓ invalid\n",
    "Fix / Block / Regenerate / Log\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why Output Validation Matters\n",
    "\n",
    "| Risk              | Consequence           |\n",
    "| ----------------- | --------------------- |\n",
    "| Hallucinations    | Wrong answers         |\n",
    "| Policy violations | Legal / safety issues |\n",
    "| Bad formatting    | System failure        |\n",
    "| Data leakage      | Security breach       |\n",
    "\n",
    "---\n",
    "\n",
    "### Schema & Format Validation\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "class LLMResponse(BaseModel):\n",
    "    answer: str\n",
    "    confidence: float\n",
    "\n",
    "def validate_format(raw_output):\n",
    "    try:\n",
    "        return LLMResponse.parse_raw(raw_output)\n",
    "    except ValidationError:\n",
    "        return None\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Content Safety Validation\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "BLOCKED_TERMS = [\"password\", \"secret\", \"credit card\"]\n",
    "\n",
    "def contains_blocked_content(text):\n",
    "    for term in BLOCKED_TERMS:\n",
    "        if term in text.lower():\n",
    "            return True\n",
    "    return False\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Length & Constraint Validation\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "MAX_TOKENS = 500\n",
    "\n",
    "def validate_constraints(text):\n",
    "    if len(text.split()) > MAX_TOKENS:\n",
    "        return False\n",
    "    return True\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### End-to-End Validation Pipeline\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "def validate_output(raw_output):\n",
    "    if not validate_constraints(raw_output):\n",
    "        return None\n",
    "\n",
    "    if contains_blocked_content(raw_output):\n",
    "        return None\n",
    "\n",
    "    return raw_output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Enforcement & Recovery Strategy\n",
    "\n",
    "```python\n",
    "def safe_generate(prompt):\n",
    "    response = llm.invoke(prompt).content\n",
    "\n",
    "    valid = validate_output(response)\n",
    "    if not valid:\n",
    "        return \"Response blocked: output failed validation.\"\n",
    "\n",
    "    return valid\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "Output Validation = Final quality gate before the user sees anything\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Never trust model output blindly\n",
    "* Validate format, content, and constraints\n",
    "* Combine with input sanitization and injection detection\n",
    "* Mandatory for production AI systems\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
