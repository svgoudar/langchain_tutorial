{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5287448a",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## ConversationBufferMemory\n",
    "\n",
    "\n",
    "**ConversationBufferMemory** is a **chat history memory mechanism** that stores the **entire conversation (user + assistant messages) sequentially** and injects it back into the prompt on every turn.\n",
    "\n",
    "It enables an LLM to remain **context-aware across multiple interactions**.\n",
    "\n",
    "```\n",
    "User ↔ Assistant\n",
    "   ↓\n",
    "ConversationBufferMemory\n",
    "   ↓\n",
    "Prompt (with full chat history)\n",
    "```\n",
    "\n",
    "Provided by LangChain, it is the **simplest and most transparent memory type**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Conversation Memory Is Needed\n",
    "\n",
    "Without memory:\n",
    "\n",
    "* Each request is stateless\n",
    "* The model forgets previous context\n",
    "\n",
    "With ConversationBufferMemory:\n",
    "\n",
    "* Follow-up questions work\n",
    "* References like “that issue” are understood\n",
    "* Multi-turn reasoning becomes possible\n",
    "\n",
    "---\n",
    "\n",
    "### How ConversationBufferMemory Works Internally\n",
    "\n",
    "1. User sends a message\n",
    "2. Assistant responds\n",
    "3. Both messages are appended to memory\n",
    "4. Entire memory is added to the next prompt\n",
    "\n",
    "```\n",
    "Human: Hi\n",
    "AI: Hello\n",
    "\n",
    "Human: What is my issue?\n",
    "AI: (Uses prior context)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture View\n",
    "\n",
    "![Image](https://media.geeksforgeeks.org/wp-content/uploads/20250926150233170952/architecture_of_conversation_buffer_window_memory.webp?utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://framerusercontent.com/images/BbOR2gelIAjnemj6JKXEOI77nns.jpg?height=655\\&width=1400\\&utm_source=chatgpt.com)\n",
    "\n",
    "![Image](https://i0.wp.com/genexdbs.com/wp-content/uploads/2025/05/ChatGPT-Image-May-12-2025-11_14_44-PM.png?resize=1024%2C683\\&ssl=1\\&utm_source=chatgpt.com)\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Demonstration (LangChain)\n",
    "\n",
    "#### Initialize Memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e618b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sangouda\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangouda\\AppData\\Local\\Temp\\ipykernel_21048\\3546126749.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f98e13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Attach Memory to a Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9fc96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangouda\\AppData\\Local\\Temp\\ipykernel_21048\\1426321495.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "from langchain_classic.chains import ConversationChain\n",
    "\n",
    "llm = OpenAI()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb5d3d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Run a Conversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d94d485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' You reported that your VPN is not working.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"My VPN is not working\")\n",
    "conversation.predict(input=\"What did I just report?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3494251e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: My VPN is not working\\nAI:  Oh no, I'm sorry to hear that. Can you provide me with more information about the issue? What type of VPN are you using? Is it a personal VPN or a corporate VPN? Also, have you tried restarting your device or contacting your IT department for assistance?\\nHuman: What did I just report?\\nAI:  You reported that your VPN is not working.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae05cf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Output**\n",
    "\n",
    "```\n",
    "You reported that your VPN is not working.\n",
    "```\n",
    "\n",
    "The model knows this because the **entire chat history was replayed**.\n",
    "\n",
    "---\n",
    "\n",
    "### What Is Stored in Memory\n",
    "\n",
    "```python\n",
    "print(memory.buffer)\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Human: My VPN is not working\n",
    "AI: I'm sorry to hear that.\n",
    "\n",
    "Human: What did I just report?\n",
    "AI: You reported that your VPN is not working.\n",
    "```\n",
    "\n",
    "This is a **plain text buffer**, not embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### Prompt Injection Mechanism\n",
    "\n",
    "Internally, the prompt looks like:\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "Human: My VPN is not working\n",
    "AI: I'm sorry to hear that.\n",
    "\n",
    "Human: What did I just report?\n",
    "AI:\n",
    "```\n",
    "\n",
    "This is why:\n",
    "\n",
    "* Context is preserved\n",
    "* Token usage increases with longer chats\n",
    "\n",
    "---\n",
    "\n",
    "### Strengths of ConversationBufferMemory\n",
    "\n",
    "* Very simple\n",
    "* Fully transparent\n",
    "* Ideal for short conversations\n",
    "* Excellent for demos and debugging\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations\n",
    "\n",
    "| Limitation       | Reason                          |\n",
    "| ---------------- | ------------------------------- |\n",
    "| Token growth     | Entire history is replayed      |\n",
    "| Cost increase    | More tokens per request         |\n",
    "| No summarization | Raw conversation only           |\n",
    "| Not scalable     | Long chats break context window |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use ConversationBufferMemory\n",
    "\n",
    "Use it when:\n",
    "\n",
    "* Conversations are short\n",
    "* Accuracy > cost\n",
    "* You need full context\n",
    "* You are prototyping or teaching concepts\n",
    "\n",
    "Avoid it when:\n",
    "\n",
    "* Sessions are long\n",
    "* You need memory compression\n",
    "* You run production chatbots at scale\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison with Other Memory Types\n",
    "\n",
    "| Memory Type               | Stores     | Scales | Use Case        |\n",
    "| ------------------------- | ---------- | ------ | --------------- |\n",
    "| ConversationBufferMemory  | Full chat  | ❌      | Simple chat     |\n",
    "| ConversationSummaryMemory | Summary    | ✅      | Long chats      |\n",
    "| VectorStoreMemory         | Embeddings | ✅      | Semantic recall |\n",
    "| Redis / DB Memory         | Structured | ✅      | Production      |\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Use Case\n",
    "\n",
    "**IT Support Chatbot**\n",
    "\n",
    "* User reports issue\n",
    "* Follows up with clarifications\n",
    "* Assistant remembers issue type\n",
    "* No database lookup required\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* ConversationBufferMemory stores **raw chat history**\n",
    "* It enables **multi-turn contextual reasoning**\n",
    "* It is simple but **not production-scalable**\n",
    "* Best used as a **baseline memory mechanism**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
