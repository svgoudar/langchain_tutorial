{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beadd697",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Latency Tracking\n",
    "\n",
    "**Latency tracking** measures **how long each part of an LLM workflow takes to execute**—from request start to final response.\n",
    "It helps identify **slow components** such as retrieval, prompt building, LLM calls, retries, or tools.\n",
    "\n",
    "In LLM systems, latency answers:\n",
    "\n",
    "* *Why is the response slow?*\n",
    "* *Which step is the bottleneck?*\n",
    "* *Is latency caused by retrieval, model, or retries?*\n",
    "\n",
    "Latency tracking is commonly implemented using:\n",
    "\n",
    "* Timers (manual)\n",
    "* Callback handlers\n",
    "* Tracing systems\n",
    "\n",
    "---\n",
    "\n",
    "### Why Latency Tracking Is Critical\n",
    "\n",
    "Without latency tracking:\n",
    "\n",
    "* Users complain about slowness\n",
    "* You guess where the problem is\n",
    "* Scaling decisions are blind\n",
    "\n",
    "With latency tracking:\n",
    "\n",
    "* Pinpoint slow steps\n",
    "* Optimize prompts, chunking, models\n",
    "* Enforce SLAs\n",
    "* Compare models and configurations\n",
    "\n",
    "---\n",
    "\n",
    "### What Contributes to Latency\n",
    "\n",
    "| Component       | Typical Cause                 |\n",
    "| --------------- | ----------------------------- |\n",
    "| Retriever       | Vector DB I/O                 |\n",
    "| Prompt building | Large context                 |\n",
    "| LLM call        | Model size / load             |\n",
    "| Retries         | Rate limits / timeouts        |\n",
    "| Streaming       | Same total latency, better UX |\n",
    "| Tools           | External APIs                 |\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture View\n",
    "\n",
    "![Image](https://portkey.ai/blog/content/images/size/w1200/2025/11/end-to-endllm-observability.png)\n",
    "\n",
    "![Image](https://bentoml.com/llm/assets/images/llm-inference-ttft-latency-3419154284149af2052def0403380a30.png)\n",
    "\n",
    "![Image](https://miro.medium.com/v2/resize%3Afit%3A2000/1%2AKfvrQNLtHCnFVD0js7tBMg.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Manual Latency Tracking\n",
    "\n",
    "#### Simple Timing with `time`\n",
    "\n",
    "```python\n",
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "start = time.time()\n",
    "response = llm.invoke(\"Explain latency tracking\")\n",
    "end = time.time()\n",
    "\n",
    "print(\"Latency (seconds):\", end - start)\n",
    "```\n",
    "\n",
    "This gives **end-to-end latency only**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-Level Latency Tracking\n",
    "\n",
    "```python\n",
    "import time\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def timed_step(name, fn):\n",
    "    def wrapper(x):\n",
    "        start = time.time()\n",
    "        result = fn(x)\n",
    "        duration = time.time() - start\n",
    "        print(f\"{name} latency: {duration:.3f}s\")\n",
    "        return result\n",
    "    return RunnableLambda(wrapper)\n",
    "\n",
    "chain = (\n",
    "    timed_step(\"Normalize\", lambda x: x.strip())\n",
    "    | timed_step(\"Uppercase\", lambda x: x.upper())\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "chain.invoke(\" latency demo \")\n",
    "```\n",
    "\n",
    "You now see **per-step latency**.\n",
    "\n",
    "---\n",
    "\n",
    "### Latency Tracking with Callback Handlers (Recommended)\n",
    "\n",
    "#### Custom Latency Callback\n",
    "\n",
    "```python\n",
    "import time\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "class LatencyCallback(BaseCallbackHandler):\n",
    "    def on_chain_start(self, serialized, inputs, **kwargs):\n",
    "        self.chain_start = time.time()\n",
    "\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        self.llm_start = time.time()\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        llm_latency = time.time() - self.llm_start\n",
    "        print(f\"LLM latency: {llm_latency:.3f}s\")\n",
    "\n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        total_latency = time.time() - self.chain_start\n",
    "        print(f\"Total chain latency: {total_latency:.3f}s\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Attach Latency Callback\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain {topic} briefly\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(callbacks=[LatencyCallback()])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"topic\": \"latency tracking\"})\n",
    "```\n",
    "\n",
    "**Output (example)**\n",
    "\n",
    "```\n",
    "LLM latency: 0.612s\n",
    "Total chain latency: 0.640s\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Latency Tracking in RAG Pipelines\n",
    "\n",
    "```python\n",
    "class RAGLatencyCallback(BaseCallbackHandler):\n",
    "    def on_retriever_start(self, serialized, query, **kwargs):\n",
    "        self.retriever_start = time.time()\n",
    "\n",
    "    def on_retriever_end(self, documents, **kwargs):\n",
    "        print(\"Retriever latency:\", time.time() - self.retriever_start)\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        print(\"LLM latency:\", time.time() - self.llm_start)\n",
    "```\n",
    "\n",
    "This shows:\n",
    "\n",
    "* Retrieval time\n",
    "* Generation time separately\n",
    "\n",
    "---\n",
    "\n",
    "### Latency Tracking with Async Execution\n",
    "\n",
    "```python\n",
    "async def run():\n",
    "    start = time.time()\n",
    "    await llm.ainvoke(\"Explain async latency tracking\")\n",
    "    print(\"Async latency:\", time.time() - start)\n",
    "```\n",
    "\n",
    "Async latency tracking is essential for:\n",
    "\n",
    "* FastAPI\n",
    "* High concurrency systems\n",
    "\n",
    "---\n",
    "\n",
    "### Latency Tracking via Tracing (Automatic)\n",
    "\n",
    "When tracing is enabled:\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=true\n",
    "```\n",
    "\n",
    "Each trace automatically records:\n",
    "\n",
    "* Step-level latency\n",
    "* LLM call duration\n",
    "* Tool and retriever timing\n",
    "* Retry delays\n",
    "\n",
    "Viewed visually in tracing UI.\n",
    "\n",
    "---\n",
    "\n",
    "### Latency Tracking vs Cost Tracking\n",
    "\n",
    "| Aspect       | Latency | Cost   |\n",
    "| ------------ | ------- | ------ |\n",
    "| Measures     | Time    | Money  |\n",
    "| Unit         | ms / s  | $      |\n",
    "| User impact  | UX      | Budget |\n",
    "| Optimization | Speed   | Spend  |\n",
    "\n",
    "They should **always be tracked together**.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Latency Problems\n",
    "\n",
    "* Large context windows\n",
    "* Too many retrieved chunks\n",
    "* Slow vector DB\n",
    "* Unbounded retries\n",
    "* Heavy callback logic\n",
    "\n",
    "---\n",
    "\n",
    "### Latency Optimization Hooks\n",
    "\n",
    "Typical actions based on latency:\n",
    "\n",
    "* Reduce `k` in retrieval\n",
    "* Switch to smaller model\n",
    "* Enable streaming\n",
    "* Cache results\n",
    "* Parallelize independent steps\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "Latency tracking is a **stopwatch attached to every step**.\n",
    "\n",
    "```\n",
    "Pipeline runs → timers record → slow step identified → optimized\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Latency tracking measures **time per step**\n",
    "* Can be manual, callback-based, or tracing-based\n",
    "* Essential for performance optimization\n",
    "* Required for SLAs and production readiness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96960e47",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
