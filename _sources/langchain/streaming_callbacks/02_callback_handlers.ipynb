{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9edde50c",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Callback Handlers\n",
    "\n",
    "**Callback handlers** are hooks that let you **observe, log, stream, or react to internal events** occurring during runnable execution—such as chain start/end, LLM token generation, tool calls, and errors.\n",
    "\n",
    "They are a core observability and control mechanism in LangChain.\n",
    "\n",
    "```\n",
    "Runnable Execution\n",
    "   ├── on_chain_start\n",
    "   ├── on_llm_start\n",
    "   ├── on_llm_new_token\n",
    "   ├── on_tool_start\n",
    "   ├── on_chain_end / on_error\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why Callback Handlers Are Needed\n",
    "\n",
    "Callback handlers enable:\n",
    "\n",
    "* Token streaming to UI\n",
    "* Debugging and tracing\n",
    "* Logging prompts and responses\n",
    "* Metrics and monitoring\n",
    "* Guardrails and policy checks\n",
    "\n",
    "Without callbacks, runnable execution is a **black box**.\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture View\n",
    "\n",
    "![Image](https://miro.medium.com/1%2AL-qolcgDJOGwxdZ9QaM1vA.jpeg)\n",
    "\n",
    "![Image](https://masterofcode.com/wp-content/uploads/2023/11/LOFTs-Architecture.png)\n",
    "\n",
    "![Image](https://framerusercontent.com/images/8WL6CwrJPxTeDBwqoQ4CNBJpMMc.jpg?height=640\\&width=1600)\n",
    "\n",
    "---\n",
    "\n",
    "### Callback Lifecycle (High Level)\n",
    "\n",
    "1. Runnable starts\n",
    "2. Callback handler is notified\n",
    "3. Sub-steps emit events\n",
    "4. Handler processes events\n",
    "5. Runnable finishes or errors\n",
    "\n",
    "Callbacks **do not change outputs** by default—they **observe**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### Built-in Callback Handler (StdOut)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ff794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback handlers are functions that are passed as arguments to other functions and are invoked by those functions at a later time. They are commonly used in event-driven programming to handle asynchronous tasks and respond to events. Callback handlers allow for modular and flexible code as they enable functions to be executed in response to specific events or conditions."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Callback handlers are functions that are passed as arguments to other functions and are invoked by those functions at a later time. They are commonly used in event-driven programming to handle asynchronous tasks and respond to events. Callback handlers allow for modular and flexible code as they enable functions to be executed in response to specific events or conditions.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--019b4c29-589f-7bd1-a341-c4da5e645fec', usage_metadata={'input_tokens': 12, 'output_tokens': 63, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "llm.invoke(\"Explain callback handlers briefly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f5c54",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**What happens**\n",
    "\n",
    "* Tokens are printed to stdout as they arrive\n",
    "* No custom code required\n",
    "\n",
    "---\n",
    "\n",
    "### Custom Callback Handler (Most Important)\n",
    "\n",
    "#### Define a Custom Handler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd50eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "class MyCallbackHandler(BaseCallbackHandler):\n",
    "    def on_chain_start(self, serialized, inputs, **kwargs):\n",
    "        print(\"Chain started:\", inputs)\n",
    "\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        print(\"\\nChain ended:\", outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49757353",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Attach It to a Runnable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c21bfde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback handlers are functions that are passed as arguments to other functions and are called when a certain event or condition occurs."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Callback handlers are functions that are passed as arguments to other functions and are called when a certain event or condition occurs.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--019b4c2b-55ca-7293-bca1-04335331b79c', usage_metadata={'input_tokens': 14, 'output_tokens': 23, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain {topic} in one sentence\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    streaming=True,\n",
    "    callbacks=[MyCallbackHandler()]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"topic\": \"callback handlers\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe456d9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### What Events You Can Hook Into\n",
    "\n",
    "Common callback methods:\n",
    "\n",
    "```text\n",
    "on_chain_start / on_chain_end\n",
    "on_llm_start / on_llm_end\n",
    "on_llm_new_token\n",
    "on_tool_start / on_tool_end\n",
    "on_retriever_start / on_retriever_end\n",
    "on_error\n",
    "```\n",
    "\n",
    "You implement **only what you need**.\n",
    "\n",
    "---\n",
    "\n",
    "### Callback Handlers with RunnableSequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6636d5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain started: callbacks in sequence\n",
      "Chain started: callbacks in sequence\n",
      "\n",
      "Chain ended: CALLBACKS IN SEQUENCE\n",
      "11.. The The first first callback callback function function is is executed executed.\n",
      ".\n",
      "22.. Once Once the the first first callback callback function function completes completes its its task task,, the the next next callback callback function function in in the the sequence sequence is is invoked invoked.\n",
      ".\n",
      "33.. This This process process continues continues until until all all the the callback callback functions functions in in the the sequence sequence have have been been executed executed..\n",
      "Chain ended: content='1. The first callback function is executed.\\n2. Once the first callback function completes its task, the next callback function in the sequence is invoked.\\n3. This process continues until all the callback functions in the sequence have been executed.' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default', 'model_provider': 'openai'} id='lc_run--019b4c2b-d458-77d3-8aea-ddfc766e3f75' usage_metadata={'input_tokens': 13, 'output_tokens': 47, 'total_tokens': 60, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. The first callback function is executed.\\n2. Once the first callback function completes its task, the next callback function in the sequence is invoked.\\n3. This process continues until all the callback functions in the sequence have been executed.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--019b4c2b-d458-77d3-8aea-ddfc766e3f75', usage_metadata={'input_tokens': 13, 'output_tokens': 47, 'total_tokens': 60, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = (\n",
    "    RunnableLambda(lambda x: x.upper())\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\n",
    "    \"callbacks in sequence\",\n",
    "    config={\"callbacks\": [MyCallbackHandler()]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4224dac",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Callbacks receive events from **all steps** in the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### Per-Invocation vs Global Callbacks\n",
    "\n",
    "#### Per-invocation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85262d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain started: Explain callbacks\n",
      "Chain started: Explain callbacks\n",
      "\n",
      "Chain ended: EXPLAIN CALLBACKS\n",
      "CallbacksCallbacks are are functions functions that that are are passed passed as as arguments arguments to to another another function function and and are are intended intended to to be be executed executed once once the the main main function function has has completed completed its its task task.. They They are are a a way way to to handle handle asynchronous asynchronous operations operations in in JavaScript JavaScript,, such such as as fetching fetching data data from from an an API API or or executing executing a a time time-consuming-consuming computation computation.\n",
      "\n",
      ".\n",
      "\n",
      "WhenWhen a a function function accepts accepts a a callback callback as as an an argument argument,, it it can can call call that that callback callback once once it it has has completed completed its its task task.. This This allows allows the the original original function function to to continue continue running running other other tasks tasks while while waiting waiting for for the the callback callback to to be be executed executed.\n",
      "\n",
      ".\n",
      "\n",
      "CallbacksCallbacks are are commonly commonly used used in in event event handlers handlers,, AJAX AJAX requests requests,, and and other other asynchronous asynchronous operations operations in in JavaScript JavaScript.. They They are are a a key key concept concept in in modern modern web web development development,, as as they they allow allow for for non non-blocking-blocking code code execution execution and and help help improve improve the the performance performance of of web web applications applications..\n",
      "Chain ended: content='Callbacks are functions that are passed as arguments to another function and are intended to be executed once the main function has completed its task. They are a way to handle asynchronous operations in JavaScript, such as fetching data from an API or executing a time-consuming computation.\\n\\nWhen a function accepts a callback as an argument, it can call that callback once it has completed its task. This allows the original function to continue running other tasks while waiting for the callback to be executed.\\n\\nCallbacks are commonly used in event handlers, AJAX requests, and other asynchronous operations in JavaScript. They are a key concept in modern web development, as they allow for non-blocking code execution and help improve the performance of web applications.' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default', 'model_provider': 'openai'} id='lc_run--019b4c2c-261a-71b1-8342-a75d6c90c9e4' usage_metadata={'input_tokens': 11, 'output_tokens': 137, 'total_tokens': 148, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Callbacks are functions that are passed as arguments to another function and are intended to be executed once the main function has completed its task. They are a way to handle asynchronous operations in JavaScript, such as fetching data from an API or executing a time-consuming computation.\\n\\nWhen a function accepts a callback as an argument, it can call that callback once it has completed its task. This allows the original function to continue running other tasks while waiting for the callback to be executed.\\n\\nCallbacks are commonly used in event handlers, AJAX requests, and other asynchronous operations in JavaScript. They are a key concept in modern web development, as they allow for non-blocking code execution and help improve the performance of web applications.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--019b4c2c-261a-71b1-8342-a75d6c90c9e4', usage_metadata={'input_tokens': 11, 'output_tokens': 137, 'total_tokens': 148, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    \"Explain callbacks\",\n",
    "    config={\"callbacks\": [MyCallbackHandler()]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21046f04",
   "metadata": {},
   "source": [
    "\n",
    "#### Global (applies everywhere)\n",
    "\n",
    "```python\n",
    "llm = ChatOpenAI(callbacks=[MyCallbackHandler()])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Callbacks + Streaming (Real-Time UI)\n",
    "\n",
    "Callback handlers are the **foundation** of:\n",
    "\n",
    "* Token streaming\n",
    "* SSE / WebSocket updates\n",
    "* Progress indicators\n",
    "\n",
    "Example usage:\n",
    "\n",
    "```python\n",
    "def on_llm_new_token(self, token, **kwargs):\n",
    "    websocket.send_text(token)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Callbacks in Async Execution\n",
    "\n",
    "```python\n",
    "async for chunk in llm.astream(\n",
    "    \"Explain async callbacks\",\n",
    "    config={\"callbacks\": [MyCallbackHandler()]}\n",
    "):\n",
    "    pass\n",
    "```\n",
    "\n",
    "Async and sync callbacks work the same conceptually.\n",
    "\n",
    "---\n",
    "\n",
    "### Callback Handlers vs Middleware\n",
    "\n",
    "| Aspect      | Callback Handler | Middleware    |\n",
    "| ----------- | ---------------- | ------------- |\n",
    "| Purpose     | Observe events   | Modify flow   |\n",
    "| Granularity | Step-level       | Request-level |\n",
    "| Streaming   | Yes              | No            |\n",
    "| LCEL-native | Yes              | No            |\n",
    "\n",
    "---\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "* Streaming tokens to UI\n",
    "* Logging prompts/responses\n",
    "* Cost and token accounting\n",
    "* Tracing (LangSmith)\n",
    "* Policy checks and audits\n",
    "* Debugging chains and agents\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "* Doing heavy blocking work in callbacks\n",
    "* Logging sensitive data\n",
    "* Forgetting callbacks don’t alter output\n",
    "* Attaching callbacks at the wrong scope\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "Callback handlers are **event listeners** for runnable execution.\n",
    "\n",
    "```\n",
    "Runnable runs → events fire → handlers react\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Callback handlers provide **observability**\n",
    "* They hook into every stage of execution\n",
    "* Essential for streaming, logging, and monitoring\n",
    "* Core building block for production LLM systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
