{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42e235f",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Performance Metrics \n",
    "\n",
    "\n",
    "**Performance metrics** measure **how fast, stable, and scalable** your LLM system is.\n",
    "\n",
    "They answer:\n",
    "\n",
    "> **Can this system handle real users reliably at scale?**\n",
    "\n",
    "Even the best model fails if performance is poor.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Performance Metrics\n",
    "\n",
    "| Metric                         | What It Measures         | Why It Matters       |\n",
    "| ------------------------------ | ------------------------ | -------------------- |\n",
    "| **Latency**                    | Total response time      | UX & SLA             |\n",
    "| **p50 / p95 / p99 Latency**    | Tail latency             | Bottleneck detection |\n",
    "| **TTFT (Time To First Token)** | Streaming responsiveness | Perceived speed      |\n",
    "| **Throughput**                 | Requests per second      | Scalability          |\n",
    "| **Error Rate**                 | Failed responses %       | Stability            |\n",
    "| **Availability**               | Uptime                   | Production readiness |\n",
    "\n",
    "---\n",
    "\n",
    "### Demonstration Setup\n",
    "\n",
    "```python\n",
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Latency Measurement\n",
    "\n",
    "```python\n",
    "start = time.time()\n",
    "llm.invoke(\"Explain performance metrics\")\n",
    "latency = time.time() - start\n",
    "print(\"Latency:\", latency)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### p50 / p95 / p99 Latency\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "latencies = []\n",
    "for _ in range(100):\n",
    "    start = time.time()\n",
    "    llm.invoke(\"Ping\")\n",
    "    latencies.append(time.time() - start)\n",
    "\n",
    "print(\"p50:\", np.percentile(latencies, 50))\n",
    "print(\"p95:\", np.percentile(latencies, 95))\n",
    "print(\"p99:\", np.percentile(latencies, 99))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### TTFT (Streaming Speed)\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_stream = ChatOpenAI(streaming=True)\n",
    "\n",
    "start = time.time()\n",
    "for chunk in llm_stream.stream(\"Explain streaming\"):\n",
    "    ttft = time.time() - start\n",
    "    print(\"TTFT:\", ttft)\n",
    "    break\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Throughput (Requests Per Second)\n",
    "\n",
    "```python\n",
    "start = time.time()\n",
    "for _ in range(50):\n",
    "    llm.invoke(\"Ping\")\n",
    "end = time.time()\n",
    "\n",
    "throughput = 50 / (end - start)\n",
    "print(\"Throughput:\", throughput)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Error Rate\n",
    "\n",
    "```python\n",
    "success = 0\n",
    "fail = 0\n",
    "\n",
    "for _ in range(50):\n",
    "    try:\n",
    "        llm.invoke(\"Ping\")\n",
    "        success += 1\n",
    "    except:\n",
    "        fail += 1\n",
    "\n",
    "error_rate = fail / (success + fail)\n",
    "print(\"Error rate:\", error_rate)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Availability (Uptime)\n",
    "\n",
    "```python\n",
    "uptime = successful_requests / total_requests\n",
    "```\n",
    "\n",
    "Tracked over time in production.\n",
    "\n",
    "---\n",
    "\n",
    "### Why These Metrics Matter\n",
    "\n",
    "| Problem            | Detected By   |\n",
    "| ------------------ | ------------- |\n",
    "| Slow UI            | Latency, TTFT |\n",
    "| System overload    | Throughput    |\n",
    "| Hidden bottlenecks | p95/p99       |\n",
    "| Crashes            | Error rate    |\n",
    "| Outages            | Availability  |\n",
    "\n",
    "---\n",
    "\n",
    "### Production Thresholds (Typical)\n",
    "\n",
    "| Metric       | Target    |\n",
    "| ------------ | --------- |\n",
    "| p95 latency  | < 1.5 sec |\n",
    "| TTFT         | < 300 ms  |\n",
    "| Error rate   | < 1%      |\n",
    "| Availability | > 99.9%   |\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "Performance = Speed + Scale + Stability\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Performance metrics are mandatory for production\n",
    "* Tail latency matters more than average latency\n",
    "* Streaming improves perceived performance\n",
    "* Must be continuously monitored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afece98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
