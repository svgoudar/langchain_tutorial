{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df65f936",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Web Scraping\n",
    "\n",
    "\n",
    "**Web scraping** is the process of **programmatically extracting data from websites** by fetching HTML pages and parsing the required content.\n",
    "\n",
    "In LLM / RAG systems, web scraping is used to:\n",
    "\n",
    "* Collect external knowledge\n",
    "* Build knowledge bases\n",
    "* Keep documents up to date\n",
    "* Ingest web pages for search and QA\n",
    "\n",
    "---\n",
    "\n",
    "### Where Web Scraping Fits in LLM Pipelines\n",
    "\n",
    "```\n",
    "Web Page\n",
    "  ↓\n",
    "HTTP Request\n",
    "  ↓\n",
    "HTML Content\n",
    "  ↓\n",
    "Parser / Scraper\n",
    "  ↓\n",
    "Clean Text\n",
    "  ↓\n",
    "Chunking → Embeddings → Vector DB → RAG\n",
    "```\n",
    "\n",
    "Scraping is **before** embeddings and retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Web Scraping\n",
    "\n",
    "| Type               | Description                   |\n",
    "| ------------------ | ----------------------------- |\n",
    "| Static scraping    | HTML is directly available    |\n",
    "| Dynamic scraping   | Content loaded via JavaScript |\n",
    "| API-based scraping | Hidden APIs behind web apps   |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###  Basic Web Scraping with `requests` + `BeautifulSoup`\n",
    "\n",
    "#### Install Dependencies\n",
    "\n",
    "```bash\n",
    "pip install requests beautifulsoup4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Fetch a Web Page\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "html = response.text\n",
    "print(html[:500])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Parse HTML Content\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "text = soup.get_text()\n",
    "print(text[:300])\n",
    "```\n",
    "\n",
    "This removes HTML tags and extracts readable text.\n",
    "\n",
    "---\n",
    "\n",
    "### Targeted Scraping (Specific Elements)\n",
    "\n",
    "#### Extract Headings and Paragraphs\n",
    "\n",
    "```python\n",
    "headings = [h.text for h in soup.find_all(\"h1\")]\n",
    "paragraphs = [p.text for p in soup.find_all(\"p\")]\n",
    "\n",
    "print(\"Headings:\", headings)\n",
    "print(\"Paragraphs:\", paragraphs[:3])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Extract Links (Metadata)\n",
    "\n",
    "```python\n",
    "links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n",
    "print(links[:5])\n",
    "```\n",
    "\n",
    "Useful for:\n",
    "\n",
    "* Crawling\n",
    "* Source tracking\n",
    "* Metadata enrichment\n",
    "\n",
    "---\n",
    "\n",
    "### Web Scraping for RAG (Clean + Structured)\n",
    "\n",
    "#### Convert Scraped Content into Documents\n",
    "\n",
    "```python\n",
    "from langchain.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=p.text,\n",
    "        metadata={\"source\": url}\n",
    "    )\n",
    "    for p in soup.find_all(\"p\")\n",
    "]\n",
    "```\n",
    "\n",
    "These `Document` objects are now ready for:\n",
    "\n",
    "* Chunking\n",
    "* Embeddings\n",
    "* Vector storage\n",
    "\n",
    "---\n",
    "\n",
    "### Using LangChain Web Loaders (Recommended)\n",
    "\n",
    "#### WebBaseLoader (Simple & Clean)\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://example.com\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents[0].page_content)\n",
    "```\n",
    "\n",
    "This handles:\n",
    "\n",
    "* Fetching\n",
    "* Cleaning\n",
    "* Metadata\n",
    "\n",
    "---\n",
    "\n",
    "### Scraping Multiple Pages\n",
    "\n",
    "#### Bulk Web Scraping\n",
    "\n",
    "```python\n",
    "urls = [\n",
    "    \"https://example.com/page1\",\n",
    "    \"https://example.com/page2\"\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(urls)\n",
    "documents = loader.load()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Handling Dynamic Websites (JavaScript)\n",
    "\n",
    "Static scraping fails when content loads via JS.\n",
    "\n",
    "**Solution options**:\n",
    "\n",
    "* Playwright / Selenium\n",
    "* Use hidden APIs\n",
    "* Headless browsers\n",
    "\n",
    "Example (conceptual):\n",
    "\n",
    "```python\n",
    "from playwright.sync_api import sync_playwright\n",
    "```\n",
    "\n",
    "Used only when required due to cost.\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices (Important)\n",
    "\n",
    "| Practice                  | Reason            |\n",
    "| ------------------------- | ----------------- |\n",
    "| Respect robots.txt        | Legal & ethical   |\n",
    "| Rate limiting             | Avoid blocking    |\n",
    "| Clean boilerplate         | Better embeddings |\n",
    "| Store source URLs         | Traceability      |\n",
    "| Avoid scraping auth pages | Security risk     |\n",
    "\n",
    "---\n",
    "\n",
    "### Common Scraping Pitfalls\n",
    "\n",
    "* Scraping JS-heavy pages incorrectly\n",
    "* Including navigation/footer noise\n",
    "* Ignoring website terms\n",
    "* Over-fetching pages\n",
    "* Not normalizing text\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "Web Scraping =\n",
    "Fetch → Parse → Clean → Structure → Ingest\n",
    "```\n",
    "\n",
    "If cleaning is bad, **RAG quality drops**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Web scraping is the foundation of web-based RAG\n",
    "* Use `requests + BeautifulSoup` for static pages\n",
    "* Use LangChain loaders for faster integration\n",
    "* Convert scraped data into structured documents\n",
    "* Always clean and preserve metadata\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
