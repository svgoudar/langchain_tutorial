{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd31ba37",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Load Balancing \n",
    "\n",
    "### What Is Load Balancing\n",
    "\n",
    "**Load balancing** is the process of **distributing incoming traffic across multiple servers** so that no single machine becomes overloaded.\n",
    "\n",
    "It ensures:\n",
    "\n",
    "* High availability\n",
    "* Scalability\n",
    "* Fault tolerance\n",
    "* Low latency\n",
    "\n",
    "---\n",
    "\n",
    "### Where Load Balancing Fits\n",
    "\n",
    "```\n",
    "Clients → Load Balancer → Server Pool → Services / LLM / DB\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why Load Balancing Is Critical for AI Systems\n",
    "\n",
    "* LLM requests are compute-heavy\n",
    "* Traffic patterns are bursty\n",
    "* Failures are expensive\n",
    "* User experience depends on latency\n",
    "\n",
    "---\n",
    "\n",
    "### Load Balancing Algorithms\n",
    "\n",
    "| Algorithm         | Description                        |\n",
    "| ----------------- | ---------------------------------- |\n",
    "| Round Robin       | Requests evenly distributed        |\n",
    "| Least Connections | Server with fewest active requests |\n",
    "| IP Hash           | Same client goes to same server    |\n",
    "| Weighted          | Some servers get more traffic      |\n",
    "\n",
    "---\n",
    "\n",
    "### Simple Python Load Balancer (Conceptual)\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "import itertools\n",
    "\n",
    "servers = [\"server1\", \"server2\", \"server3\"]\n",
    "cycle = itertools.cycle(servers)\n",
    "\n",
    "def route_request():\n",
    "    return next(cycle)\n",
    "\n",
    "for _ in range(10):\n",
    "    print(route_request())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Reverse Proxy Load Balancing (Nginx Example)\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```nginx\n",
    "http {\n",
    "    upstream llm_backend {\n",
    "        least_conn;\n",
    "        server 127.0.0.1:8001;\n",
    "        server 127.0.0.1:8002;\n",
    "        server 127.0.0.1:8003;\n",
    "    }\n",
    "\n",
    "    server {\n",
    "        listen 80;\n",
    "\n",
    "        location / {\n",
    "            proxy_pass http://llm_backend;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### FastAPI + Multiple Workers\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --workers 4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Cloud Load Balancer Architecture\n",
    "\n",
    "```\n",
    "Users\n",
    "  ↓\n",
    "Cloud Load Balancer\n",
    "  ↓\n",
    "Kubernetes Service\n",
    "  ↓\n",
    "Pods (FastAPI + LangChain)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Health Checks\n",
    "\n",
    "```nginx\n",
    "server 127.0.0.1:8001 max_fails=3 fail_timeout=30s;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### LLM-Specific Load Balancing Strategy\n",
    "\n",
    "| Layer     | Strategy     |\n",
    "| --------- | ------------ |\n",
    "| API       | Round robin  |\n",
    "| LLM       | Rate limited |\n",
    "| Workers   | Auto-scaled  |\n",
    "| Vector DB | Sharded      |\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "Load Balancer = Traffic police for your servers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Mandatory for scalable AI systems\n",
    "* Prevents downtime and overload\n",
    "* Enables horizontal scaling\n",
    "* Essential for production LLM infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded92ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
