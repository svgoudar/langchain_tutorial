{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b4a322",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Server-Sent Events (SSE) \n",
    "\n",
    "\n",
    "**Server-Sent Events (SSE)** is a web communication mechanism where the **server continuously streams data** to the client over a single long-lived HTTP connection.\n",
    "\n",
    "It is ideal for **real-time updates** such as:\n",
    "\n",
    "* LLM streaming responses\n",
    "* Notifications\n",
    "* Logs\n",
    "* Progress updates\n",
    "* Monitoring dashboards\n",
    "\n",
    "Unlike WebSockets, SSE is **one-directional**: server → client.\n",
    "\n",
    "---\n",
    "\n",
    "### Where SSE Fits in the Architecture\n",
    "\n",
    "```\n",
    "Client Browser\n",
    "     ↓  (HTTP request)\n",
    "Server\n",
    "     ↓  (open connection)\n",
    "Continuous Event Stream → Client\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why SSE for LLM Systems\n",
    "\n",
    "SSE allows the user to **see tokens as the LLM generates them**, improving UX and reducing perceived latency.\n",
    "\n",
    "---\n",
    "\n",
    "### SSE Protocol Format\n",
    "\n",
    "Each event is sent as text:\n",
    "\n",
    "```\n",
    "data: message content\\n\\n\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### FastAPI SSE Server Example\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "import asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "async def event_generator():\n",
    "    for i in range(5):\n",
    "        await asyncio.sleep(1)\n",
    "        yield f\"data: message {i}\\n\\n\"\n",
    "\n",
    "@app.get(\"/events\")\n",
    "async def stream():\n",
    "    return StreamingResponse(event_generator(), media_type=\"text/event-stream\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### JavaScript Client Example\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```html\n",
    "<script>\n",
    "const source = new EventSource(\"http://localhost:8000/events\");\n",
    "\n",
    "source.onmessage = function(event) {\n",
    "    console.log(\"Received:\", event.data);\n",
    "};\n",
    "\n",
    "source.onerror = function() {\n",
    "    console.log(\"Connection closed\");\n",
    "};\n",
    "</script>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### SSE with Streaming LLM Output\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "async def llm_stream(prompt):\n",
    "    async for chunk in llm.astream(prompt):\n",
    "        yield f\"data: {chunk.content}\\n\\n\"\n",
    "```\n",
    "\n",
    "```python\n",
    "@app.get(\"/chat\")\n",
    "async def chat(prompt: str):\n",
    "    return StreamingResponse(llm_stream(prompt), media_type=\"text/event-stream\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### SSE vs WebSockets\n",
    "\n",
    "| Feature    | SSE             | WebSockets     |\n",
    "| ---------- | --------------- | -------------- |\n",
    "| Direction  | Server → Client | Bi-directional |\n",
    "| Protocol   | HTTP            | WS             |\n",
    "| Complexity | Simple          | Complex        |\n",
    "| Scaling    | Easier          | Harder         |\n",
    "\n",
    "---\n",
    "\n",
    "### Reliability Features\n",
    "\n",
    "SSE automatically supports:\n",
    "\n",
    "* Auto-reconnect\n",
    "* Event IDs\n",
    "* Connection recovery\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "SSE = Live news feed from your server\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Perfect for real-time LLM streaming\n",
    "* Very simple to implement\n",
    "* More reliable than WebSockets for streaming text\n",
    "* Production-friendly for AI applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c54e1db",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
