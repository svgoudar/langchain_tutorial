{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207e42e7",
   "metadata": {},
   "source": [
    "## FastAPI Integration with LangChain\n",
    "\n",
    "\n",
    "**FastAPI** is a high-performance Python web framework used to expose **LLM pipelines as production APIs**.\n",
    "When combined with **LangChain**, it becomes the interface layer that:\n",
    "\n",
    "* Receives user requests\n",
    "* Runs LangChain chains / agents / RAG pipelines\n",
    "* Returns structured responses\n",
    "\n",
    "**Architecture Role**\n",
    "\n",
    "```\n",
    "Client → FastAPI → LangChain Pipeline → LLM / Vector DB / Tools → FastAPI → Client\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Project Structure\n",
    "\n",
    "```\n",
    "app/\n",
    " ├─ main.py\n",
    " ├─ chains.py\n",
    " ├─ models.py\n",
    " └─ services.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Create a LangChain Pipeline\n",
    "\n",
    "#### Build Chain (chains.py)\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer clearly: {question}\"\n",
    ")\n",
    "\n",
    "qa_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Define Request / Response Models\n",
    "\n",
    "#### Data Models (models.py)\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    question: str\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    answer: str\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Build FastAPI Service Layer\n",
    "\n",
    "#### Service Wrapper (services.py)\n",
    "\n",
    "```python\n",
    "from app.chains import qa_chain\n",
    "\n",
    "def run_chain(question: str) -> str:\n",
    "    result = qa_chain.invoke({\"question\": question})\n",
    "    return result[\"text\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Create FastAPI Application\n",
    "\n",
    "#### API Server (main.py)\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from app.models import QueryRequest, QueryResponse\n",
    "from app.services import run_chain\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/ask\", response_model=QueryResponse)\n",
    "def ask_question(request: QueryRequest):\n",
    "    answer = run_chain(request.question)\n",
    "    return {\"answer\": answer}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Run the Server\n",
    "\n",
    "```bash\n",
    "uvicorn app.main:app --reload\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Call the API\n",
    "\n",
    "```bash\n",
    "curl -X POST http://127.0.0.1:8000/ask \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"question\": \"Explain black holes\"}'\n",
    "```\n",
    "\n",
    "**Response**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"answer\": \"A black hole is...\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Adding RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "#### RAG Chain Example\n",
    "\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "retriever = FAISS.load_local(\"db\").as_retriever()\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "```\n",
    "\n",
    "#### Use in FastAPI\n",
    "\n",
    "```python\n",
    "def run_chain(question):\n",
    "    return rag_chain.invoke({\"query\": question})[\"result\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Async Support (High Throughput)\n",
    "\n",
    "```python\n",
    "@app.post(\"/ask\")\n",
    "async def ask_question(request: QueryRequest):\n",
    "    answer = await qa_chain.ainvoke({\"question\": request.question})\n",
    "    return {\"answer\": answer[\"text\"]}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Production Enhancements\n",
    "\n",
    "| Feature           | Purpose             |\n",
    "| ----------------- | ------------------- |\n",
    "| Rate limiting     | Protect API         |\n",
    "| Caching           | Reduce LLM cost     |\n",
    "| Input validation  | Security            |\n",
    "| Output validation | Safety              |\n",
    "| Tracing           | Debugging           |\n",
    "| Streaming         | Real-time responses |\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "FastAPI = Network Interface\n",
    "LangChain = Intelligence Engine\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* FastAPI exposes LangChain pipelines as scalable web services\n",
    "* Clear separation of concerns: API → Service → Chain\n",
    "* Supports sync, async, streaming, and RAG pipelines\n",
    "* Industry standard architecture for LLM applications\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
