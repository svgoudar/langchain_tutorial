{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce5d5ae",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## RunnableParallel\n",
    "\n",
    "### What Is RunnableParallel\n",
    "\n",
    "**RunnableParallel** is a LangChain runnable that allows you to **execute multiple runnables concurrently on the same input** and collect all their outputs into a single structured result.\n",
    "\n",
    "It is a **fan-out / fan-in primitive** in LCEL (LangChain Expression Language).\n",
    "\n",
    "Provided by LangChain.\n",
    "\n",
    "```\n",
    "Single Input\n",
    "   ├── Runnable A\n",
    "   ├── Runnable B\n",
    "   ├── Runnable C\n",
    "   ↓\n",
    "Combined Output (dict)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why RunnableParallel Is Needed\n",
    "\n",
    "In real pipelines, you often want to:\n",
    "\n",
    "* Run retrieval and preprocessing together\n",
    "* Extract multiple features from the same input\n",
    "* Call multiple tools/models in parallel\n",
    "* Reduce latency\n",
    "\n",
    "RunnableParallel solves:\n",
    "\n",
    "* Sequential bottlenecks\n",
    "* Manual threading/async code\n",
    "* Complex wiring logic\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Flow\n",
    "\n",
    "![Image](https://dz2cdn1.dzone.com/storage/temp/18047285-screenshot-2024-11-18-at-121208pm.png)\n",
    "\n",
    "![Image](https://www.pinecone.io/_next/image/?q=75\\&url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F63f8a8482c9ec06a8d7d1041514f87c06dd108a9-3442x942.png\\&w=3840)\n",
    "\n",
    "![Image](https://www.gocd.org/assets/images/blog/build-propagation/fan-in-fan-out-774eac6e.jpeg)\n",
    "\n",
    "```\n",
    "Input Query\n",
    "   ├─► Retriever\n",
    "   ├─► Metadata Extractor\n",
    "   └─► Normalizer\n",
    "        ↓\n",
    "   { context, metadata, normalized_query }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Demonstrations\n",
    "\n",
    "---\n",
    "\n",
    "### Basic RunnableParallel (Minimal)\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    upper=RunnableLambda(lambda x: x.upper()),\n",
    "    length=RunnableLambda(lambda x: len(x))\n",
    ")\n",
    "\n",
    "parallel.invoke(\"LangChain\")\n",
    "```\n",
    "\n",
    "**Output**\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"upper\": \"LANGCHAIN\",\n",
    "  \"length\": 9\n",
    "}\n",
    "```\n",
    "\n",
    "All runnables:\n",
    "\n",
    "* Receive the **same input**\n",
    "* Run **in parallel**\n",
    "* Return a dictionary\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableParallel Using LCEL Dictionary Syntax\n",
    "\n",
    "RunnableParallel is commonly written using a **dict literal**:\n",
    "\n",
    "```python\n",
    "parallel = {\n",
    "    \"original\": RunnableLambda(lambda x: x),\n",
    "    \"lower\": RunnableLambda(lambda x: x.lower()),\n",
    "    \"words\": RunnableLambda(lambda x: x.split())\n",
    "}\n",
    "\n",
    "parallel.invoke(\"Runnable Parallel Demo\")\n",
    "```\n",
    "\n",
    "**Output**\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"original\": \"Runnable Parallel Demo\",\n",
    "  \"lower\": \"runnable parallel demo\",\n",
    "  \"words\": [\"Runnable\", \"Parallel\", \"Demo\"]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableParallel in a RAG Pipeline (Very Common)\n",
    "\n",
    "### Problem\n",
    "\n",
    "You need:\n",
    "\n",
    "* Original question\n",
    "* Retrieved context\n",
    "* Possibly other derived data\n",
    "\n",
    "---\n",
    "\n",
    "### Solution\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "```\n",
    "\n",
    "This `{ ... }` block is a **RunnableParallel**.\n",
    "\n",
    "What happens:\n",
    "\n",
    "* Question → passed through unchanged\n",
    "* Question → sent to retriever\n",
    "* Both results are merged\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableParallel with Multiple Models\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_fast = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm_accurate = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    fast=llm_fast,\n",
    "    accurate=llm_accurate\n",
    ")\n",
    "\n",
    "parallel.invoke(\"Explain RAG briefly\")\n",
    "```\n",
    "\n",
    "**Use case**\n",
    "\n",
    "* Compare responses\n",
    "* Voting / re-ranking\n",
    "* Cost vs accuracy trade-off\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableParallel + RunnableLambda (Feature Extraction)\n",
    "\n",
    "```python\n",
    "parallel = RunnableParallel(\n",
    "    char_count=RunnableLambda(lambda x: len(x)),\n",
    "    word_count=RunnableLambda(lambda x: len(x.split())),\n",
    "    has_question_mark=RunnableLambda(lambda x: \"?\" in x)\n",
    ")\n",
    "\n",
    "parallel.invoke(\"What is RunnableParallel?\")\n",
    "```\n",
    "\n",
    "**Output**\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"char_count\": 25,\n",
    "  \"word_count\": 3,\n",
    "  \"has_question_mark\": True\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Execution Semantics\n",
    "\n",
    "* All branches receive **identical input**\n",
    "* Execution is **concurrent** (async where supported)\n",
    "* Failure in one branch fails the whole runnable\n",
    "* Output is always a **dictionary**\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableParallel vs Sequential Chains\n",
    "\n",
    "| Aspect    | RunnableParallel | Sequential     |\n",
    "| --------- | ---------------- | -------------- |\n",
    "| Execution | Concurrent       | One-by-one     |\n",
    "| Latency   | Low              | Higher         |\n",
    "| Output    | Dict             | Single value   |\n",
    "| Use case  | Fan-out          | Transformation |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use RunnableParallel\n",
    "\n",
    "Use it when:\n",
    "\n",
    "* Same input feeds multiple steps\n",
    "* You need speed\n",
    "* You want structured outputs\n",
    "* Building RAG / agents / evaluators\n",
    "\n",
    "Avoid it when:\n",
    "\n",
    "* Steps depend on each other\n",
    "* Order matters\n",
    "* Output of one step feeds the next\n",
    "\n",
    "---\n",
    "\n",
    "### Common Real-World Use Cases\n",
    "\n",
    "* RAG: question + context\n",
    "* Agent state construction\n",
    "* Guardrails + normalization\n",
    "* Multi-model evaluation\n",
    "* Feature extraction pipelines\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "* Expecting branches to see each other’s output\n",
    "* Using it when sequential dependency exists\n",
    "* Forgetting outputs are keyed\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "RunnableParallel is the **“map” stage** of an LLM pipeline.\n",
    "\n",
    "```\n",
    "One input → many computations → one structured state\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* RunnableParallel enables **fan-out / fan-in**\n",
    "* Core primitive of LCEL\n",
    "* Written using `{}` syntax most of the time\n",
    "* Essential for RAG and agent pipelines\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
