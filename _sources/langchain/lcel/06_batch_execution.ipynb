{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734f0719",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Batch Execution\n",
    "\n",
    "**Batch execution** runs the **same runnable pipeline on multiple inputs at once** instead of invoking it repeatedly in a loop.\n",
    "It is natively supported by runnables in LangChain via `.batch()` / `.abatch()`.\n",
    "\n",
    "```\n",
    "[input1, input2, input3]\n",
    "          ↓\n",
    "      Runnable\n",
    "          ↓\n",
    "[output1, output2, output3]\n",
    "```\n",
    "\n",
    "Each input is processed **independently**, but execution is optimized.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Batch Execution Is Needed\n",
    "\n",
    "Batch execution:\n",
    "\n",
    "* Reduces overhead (fewer model calls / RPCs)\n",
    "* Improves throughput\n",
    "* Enables parallelism\n",
    "* Is cost-efficient for embeddings, LLM calls, and retrieval\n",
    "\n",
    "Instead of:\n",
    "\n",
    "```python\n",
    "for x in inputs:\n",
    "    chain.invoke(x)\n",
    "```\n",
    "\n",
    "You do:\n",
    "\n",
    "```python\n",
    "chain.batch(inputs)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### How Batch Execution Works Internally\n",
    "\n",
    "1. Inputs are collected into a list\n",
    "2. Runnable executes them concurrently (where possible)\n",
    "3. Results are returned **in the same order**\n",
    "\n",
    "Important:\n",
    "\n",
    "* **No shared state between inputs**\n",
    "* One failure affects only that input (by default)\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Batch Execution (RunnableLambda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30319e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RAG', 'AGENTS', 'MEMORY']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "to_upper = RunnableLambda(lambda x: x.upper())\n",
    "\n",
    "to_upper.batch([\"rag\", \"agents\", \"memory\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db481b0f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Output**\n",
    "\n",
    "```python\n",
    "[\"RAG\", \"AGENTS\", \"MEMORY\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Batch Execution with RunnableSequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b86fce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['langchain', 'runnables', 'batch mode']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = (\n",
    "    RunnableLambda(lambda x: x.strip())\n",
    "    | RunnableLambda(lambda x: x.lower())\n",
    ")\n",
    "\n",
    "chain.batch([\n",
    "    \"  LangChain  \",\n",
    "    \"  RUNNABLES \",\n",
    "    \"  BATCH MODE \"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e55a39",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Output**\n",
    "\n",
    "```python\n",
    "[\"langchain\", \"runnables\", \"batch mode\"]\n",
    "```\n",
    "\n",
    "Each input flows through the **entire sequence independently**.\n",
    "\n",
    "---\n",
    "\n",
    "### Batch Execution with LLMs (Very Common)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aafa7256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='RAG stands for Red, Amber, Green and is a method used to prioritize tasks or projects based on their urgency and importance. \\n\\n- Red: Indicates tasks or projects that are critical and require immediate attention. These are high priority items that need to be addressed as soon as possible. \\n- Amber: Indicates tasks or projects that are important but not as urgent as red items. These may require attention in the near future, but can be delayed momentarily without major consequences. \\n- Green: Indicates tasks or projects that are not urgent and can be addressed when time allows. These are low priority items that can be completed once more critical tasks have been taken care of. \\n\\nBy using the RAG system, individuals or teams can easily identify which tasks or projects need immediate action and which can be deferred or completed at a later time. This helps to streamline workflow, prioritize work effectively and ensure that critical tasks are not overlooked.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 184, 'prompt_tokens': 11, 'total_tokens': 195, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CpzGsINVFEwUjQuyQw8BK56ZVHqM9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4bfa-db90-7170-a19a-d09d8954ebd3-0', usage_metadata={'input_tokens': 11, 'output_tokens': 184, 'total_tokens': 195, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='Agents are individuals or entities that act on behalf of another party to carry out tasks, make decisions, or represent interests. They can be human beings, such as sales representatives, lawyers, or property managers, or non-human entities, such as software programs or automated systems. Agents typically have the authority to make decisions or take actions within a defined scope on behalf of the principal, who is the party that the agent is representing. Agents are bound by certain legal obligations to act in the best interests of their principal and follow their instructions. The relationship between an agent and a principal is usually governed by a contract or agreement outlining the terms of the arrangement.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 129, 'prompt_tokens': 10, 'total_tokens': 139, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CpzGtXKEyLdq6lCoblzX2cml3QoBA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4bfa-db9f-7312-9329-c3999ce5b883-0', usage_metadata={'input_tokens': 10, 'output_tokens': 129, 'total_tokens': 139, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='Memory is the cognitive process of storing and retrieving information. It involves encoding, storing, and retrieving information in the brain. There are different types of memory, including short-term memory, long-term memory, and working memory.\\n\\nWhen information is first encountered, it is encoded into short-term memory, which has a limited capacity and duration. If the information is deemed important or rehearsed repeatedly, it may be transferred into long-term memory for more permanent storage.\\n\\nMemory retrieval is the process of recalling information stored in the brain. This can be done through recognition (identifying information as familiar) or recall (retrieving information from memory without any cues).\\n\\nMemory is a crucial aspect of cognitive functioning and plays a key role in learning, problem-solving, decision-making, and overall functioning in daily life. It can be influenced by various factors such as attention, motivation, emotional state, and prior experiences.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 178, 'prompt_tokens': 10, 'total_tokens': 188, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CpzGsyNkAPA6cDosv6LoUxBFR6nx8', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4bfa-dbaa-7170-bc4d-396f36000d13-0', usage_metadata={'input_tokens': 10, 'output_tokens': 178, 'total_tokens': 188, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "llm.batch([\n",
    "    \"Explain RAG\",\n",
    "    \"Explain agents\",\n",
    "    \"Explain memory\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60071d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Benefits:\n",
    "\n",
    "* One optimized batch call\n",
    "* Lower latency than sequential `.invoke()`\n",
    "\n",
    "---\n",
    "\n",
    "### Batch Execution in RAG Pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dd32418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for RAG batch example\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_classic.schema import Document\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"RAG (Retrieval Augmented Generation) combines retrieval with LLMs to answer questions using external knowledge.\"),\n",
    "    Document(page_content=\"Vector memory stores embeddings of documents for semantic search and retrieval.\"),\n",
    "    Document(page_content=\"LCEL (LangChain Expression Language) is a declarative way to compose chains using the pipe operator.\")\n",
    "]\n",
    "\n",
    "# Create retriever\n",
    "vectorstore = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using the context:\\n\\nContext: {context}\\n\\nQuestion: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6198a833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='RAG (Retrieval Augmented Generation) combines retrieval with LLMs to answer questions using external knowledge.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 180, 'total_tokens': 203, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CpzJ118uxNeAILL4rLUKK2QGemwPH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4bfc-e4e8-71e0-bffb-33f53f60d698-0', usage_metadata={'input_tokens': 180, 'output_tokens': 23, 'total_tokens': 203, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='Vector memory stores embeddings of documents for semantic search and retrieval.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 180, 'total_tokens': 192, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CpzJ1zhhZKnOmeaJmiia97NaxEBi7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4bfc-e4ee-7430-be4d-1ff976d774a5-0', usage_metadata={'input_tokens': 180, 'output_tokens': 12, 'total_tokens': 192, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='LCEL (LangChain Expression Language) is a declarative way to compose chains using the pipe operator.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 180, 'total_tokens': 201, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CpzJ1VO91UxHNvutFu41ImkywHPGm', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4bfc-e4f1-7fc1-9fe6-3f085f4097a0-0', usage_metadata={'input_tokens': 180, 'output_tokens': 21, 'total_tokens': 201, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"question\": RunnableLambda(lambda x: x),\n",
    "        \"context\": retriever\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "questions = [\n",
    "    \"What is RAG?\",\n",
    "    \"What is vector memory?\",\n",
    "    \"What is LCEL?\"\n",
    "]\n",
    "\n",
    "responses = chain.batch(questions)\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85d463",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Each question:\n",
    "\n",
    "* Runs retrieval\n",
    "* Builds prompt\n",
    "* Calls LLM\n",
    "* Returns its own answer\n",
    "\n",
    "---\n",
    "\n",
    "### Batch Execution with RunnableParallel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d92afe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'length': 24, 'words': 3}, {'length': 18, 'words': 2}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    length=RunnableLambda(lambda x: len(x)),\n",
    "    words=RunnableLambda(lambda x: len(x.split()))\n",
    ")\n",
    "\n",
    "parallel.batch([\n",
    "    \"Runnable batch execution\",\n",
    "    \"Parallel runnables\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc28817",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Output**\n",
    "\n",
    "```python\n",
    "[\n",
    "  {\"length\": 24, \"words\": 3},\n",
    "  {\"length\": 18, \"words\": 2}\n",
    "]\n",
    "```\n",
    "\n",
    "Each input produces a **structured result**.\n",
    "\n",
    "---\n",
    "\n",
    "### Async Batch Execution (`abatch`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b294fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Batching is a technique used in various machine learning tasks to process multiple inputs simultaneously in order to improve efficiency and speed. In the context of the provided documents, batching could refer to processing multiple embeddings of documents in vector memory, composing multiple chains using LCEL, or answering multiple questions using external knowledge and LLMs in a Retrieval Augmented Generation system. This approach allows for parallel processing of data and enables faster computation compared to processing inputs one by one.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 177, 'total_tokens': 269, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CpzKK8oR9O2GtBGbyGa55H60kThkv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4bfe-203e-7c02-95fd-b0ef09727aeb-0', usage_metadata={'input_tokens': 177, 'output_tokens': 92, 'total_tokens': 269, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='Streaming refers to the process of continuously transferring or transmitting data, typically audio or video content, over a network. In the context of the provided documents, streaming could also refer to the continuous flow of data or information in a particular format or structure, such as the vector memory storing embeddings of documents for semantic search and retrieval.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 177, 'total_tokens': 241, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CpzKK6qdzhOiU2fshV65g83bMbtbO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4bfe-203e-7c02-95fd-b0f2d873f555-0', usage_metadata={'input_tokens': 177, 'output_tokens': 64, 'total_tokens': 241, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = await chain.abatch([\n",
    "    \"Explain batching\",\n",
    "    \"Explain streaming\"\n",
    "])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1fe335",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Use when:\n",
    "\n",
    "* Running in async frameworks\n",
    "* Handling high concurrency\n",
    "\n",
    "---\n",
    "\n",
    "### Batch vs Streaming vs Parallel\n",
    "\n",
    "| Concept   | Purpose         |\n",
    "| --------- | --------------- |\n",
    "| Batch     | Many inputs     |\n",
    "| Streaming | Partial outputs |\n",
    "| Parallel  | Many branches   |\n",
    "\n",
    "They solve **different problems** and can be combined.\n",
    "\n",
    "---\n",
    "\n",
    "### Error Handling in Batch Execution\n",
    "\n",
    "By default:\n",
    "\n",
    "* One failing input does **not** stop others\n",
    "\n",
    "Optional:\n",
    "\n",
    "```python\n",
    "chain.batch(inputs, return_exceptions=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Batch Execution\n",
    "\n",
    "Use it when:\n",
    "\n",
    "* Processing multiple user queries\n",
    "* Generating embeddings\n",
    "* Running offline jobs\n",
    "* Evaluations / benchmarks\n",
    "* Indexing pipelines\n",
    "\n",
    "Avoid it when:\n",
    "\n",
    "* Each input depends on previous output\n",
    "* You need conversational state\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "Batch execution is **horizontal scaling** of a runnable:\n",
    "\n",
    "```\n",
    "Same pipeline × many inputs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Batch execution runs **one runnable on many inputs**\n",
    "* Uses `.batch()` / `.abatch()`\n",
    "* Faster and cheaper than loops\n",
    "* Preserves input–output ordering\n",
    "* Essential for production workloads\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
