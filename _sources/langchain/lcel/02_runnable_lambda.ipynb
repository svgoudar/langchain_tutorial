{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a542b2",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## RunnableLambda \n",
    "\n",
    "**RunnableLambda** is a LangChain runnable that wraps a **Python callable (function or lambda)** so it can participate **natively in LCEL (LangChain Expression Language) pipelines**.\n",
    "\n",
    "It allows you to apply **custom logic or lightweight transformations** inside a runnable chain.\n",
    "\n",
    "Provided by LangChain.\n",
    "\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "RunnableLambda (custom logic)\n",
    "  ↓\n",
    "Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why RunnableLambda Exists\n",
    "\n",
    "In real pipelines, you often need:\n",
    "\n",
    "* Small transformations\n",
    "* Field extraction\n",
    "* Formatting\n",
    "* Conditional logic\n",
    "* Metadata enrichment\n",
    "\n",
    "RunnableLambda solves:\n",
    "\n",
    "* “I need custom Python logic **inside** my chain”\n",
    "* Without breaking LCEL composability\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Difference\n",
    "\n",
    "| Component           | Purpose                            |\n",
    "| ------------------- | ---------------------------------- |\n",
    "| RunnablePassthrough | Forward input unchanged            |\n",
    "| **RunnableLambda**  | Transform input using custom logic |\n",
    "| LLM / Retriever     | Heavy model or retrieval step      |\n",
    "\n",
    "RunnableLambda = **glue logic**.\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture View\n",
    "\n",
    "![Image](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/06/image-26-1.png)\n",
    "\n",
    "![Image](https://miro.medium.com/v2/resize%3Afit%3A1200/1%2AOj7p9RbbH3BaeX1eZV3ebg.png)\n",
    "\n",
    "![Image](https://miro.medium.com/1%2AX9hmWQYKaZevGO1SlGchGA.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Demonstrations\n",
    "\n",
    "---\n",
    "\n",
    "### Basic RunnableLambda (Minimal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e50be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LANGCHAIN'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "uppercase = RunnableLambda(lambda x: x.upper())\n",
    "\n",
    "uppercase.invoke(\"langchain\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ce01f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Output**\n",
    "\n",
    "```\n",
    "LANGCHAIN\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "* Input is transformed\n",
    "* Output replaces input downstream\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableLambda with Named Function (Preferred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4e05af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_count(text: str) -> int:\n",
    "    return len(text.split())\n",
    "\n",
    "count_runnable = RunnableLambda(word_count)\n",
    "\n",
    "count_runnable.invoke(\"Runnable lambda is powerful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b5b2d5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Output**\n",
    "\n",
    "```\n",
    "4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableLambda in a Chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3d8e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain allows for the creation and customization of AI pipelines that can be combined and integrated with each other.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 23, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CpyqZeJ9P8R3jx3zj1cZjJHLpfNeX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4be1-f766-7e00-8731-2f5aebb3fe81-0', usage_metadata={'input_tokens': 23, 'output_tokens': 21, 'total_tokens': 44, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize this text in one sentence:\\n{text}\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnableLambda(lambda x: x.strip())\n",
    "    | RunnableLambda(lambda x: {\"text\": x})\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\"LangChain enables composable AI pipelines\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df464706",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "What happens:\n",
    "\n",
    "1. Trim whitespace\n",
    "2. Wrap input into a dict\n",
    "3. Prompt formatting\n",
    "4. LLM call\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableLambda for Field Extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5456b321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is RAG?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_question = RunnableLambda(lambda x: x[\"question\"])\n",
    "\n",
    "extract_question.invoke(\n",
    "    {\"question\": \"What is RAG?\", \"user\": \"admin\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183fe62",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Output**\n",
    "\n",
    "```\n",
    "What is RAG?\n",
    "```\n",
    "\n",
    "Common in:\n",
    "\n",
    "* Tool outputs\n",
    "* Agent state\n",
    "* Structured pipelines\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableLambda in RAG (Very Common)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "075c9630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create sample documents\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"Retrieval Augmented Generation (RAG) is a technique that combines retrieval with generation.\",\n",
    "        metadata={\"source\": \"doc1\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"RAG retrieves relevant documents from a knowledge base and uses them as context for the LLM.\",\n",
    "        metadata={\"source\": \"doc2\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain provides tools to build RAG pipelines efficiently.\",\n",
    "        metadata={\"source\": \"doc3\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Setup prompt template for RAG\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the question using the context.\n",
    "\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d55387",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"question\": RunnableLambda(lambda x: x),\n",
    "        \"context\": retriever\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2bc70",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here:\n",
    "\n",
    "* RunnableLambda forwards/transforms input\n",
    "* Retriever generates context\n",
    "* Both are merged\n",
    "\n",
    "(This is interchangeable with RunnablePassthrough when transformation is needed.)\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableLambda with Conditional Logic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97e69749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain: RAG'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_query(q: str) -> str:\n",
    "    if len(q) < 5:\n",
    "        return f\"Explain: {q}\"\n",
    "    return q\n",
    "\n",
    "normalize = RunnableLambda(normalize_query)\n",
    "\n",
    "normalize.invoke(\"RAG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3735f5f1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Output**\n",
    "\n",
    "```\n",
    "Explain: RAG\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableLambda for Metadata Enrichment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60671ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangouda\\AppData\\Local\\Temp\\ipykernel_40836\\264983453.py:6: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is LangChain?', 'timestamp': '2025-12-23T15:47:43.199302'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "add_metadata = RunnableLambda(\n",
    "    lambda x: {\n",
    "        \"query\": x,\n",
    "        \"timestamp\": datetime.utcnow().isoformat()\n",
    "    }\n",
    ")\n",
    "\n",
    "add_metadata.invoke(\"What is LangChain?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678533fa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableLambda vs Plain Lambda\n",
    "\n",
    "| Aspect              | RunnableLambda | Plain lambda |   |\n",
    "| ------------------- | -------------- | ------------ | - |\n",
    "| LCEL compatible     | ✅              | ❌            |   |\n",
    "| Chainable with `    | `              | ✅            | ❌ |\n",
    "| Async support       | ✅              | ❌            |   |\n",
    "| Tracing / callbacks | ✅              | ❌            |   |\n",
    "| Readability         | High           | Medium       |   |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use RunnableLambda\n",
    "\n",
    "Use it when:\n",
    "\n",
    "* Logic is simple and local\n",
    "* You need transformation inside a chain\n",
    "* You want LCEL composability\n",
    "* You need tracing / observability\n",
    "\n",
    "Avoid it when:\n",
    "\n",
    "* Logic is complex → use a service or tool\n",
    "* Heavy computation → use async workers\n",
    "\n",
    "---\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "* Input normalization\n",
    "* Output parsing\n",
    "* Key mapping\n",
    "* Guardrails\n",
    "* Feature flags\n",
    "* Routing decisions\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* RunnableLambda wraps **custom Python logic**\n",
    "* It is the **transformation primitive** of LCEL\n",
    "* Essential for real-world RAG and agent pipelines\n",
    "* Best paired with RunnablePassthrough and retrievers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
