{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e1fcec",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## RunnableSequence\n",
    "\n",
    "\n",
    "**RunnableSequence** represents a **linear, ordered pipeline** of runnables where the **output of one step becomes the input of the next**.\n",
    "In LangChain’s LCEL (LangChain Expression Language), a sequence is most commonly written using the pipe operator `|`.\n",
    "\n",
    "Provided by LangChain.\n",
    "\n",
    "```\n",
    "Input\n",
    "  → Step 1\n",
    "  → Step 2\n",
    "  → Step 3\n",
    "  → Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why RunnableSequence Is Needed\n",
    "\n",
    "Most LLM workflows are **sequential by nature**:\n",
    "\n",
    "* Normalize input\n",
    "* Build prompt\n",
    "* Call LLM\n",
    "* Parse output\n",
    "\n",
    "RunnableSequence provides:\n",
    "\n",
    "* Deterministic execution order\n",
    "* Clear data flow\n",
    "* Composability\n",
    "* Readable pipelines\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Flow\n",
    "\n",
    "```\n",
    "User Input\n",
    "   ↓\n",
    "Runnable A (transform)\n",
    "   ↓\n",
    "Runnable B (format)\n",
    "   ↓\n",
    "Runnable C (LLM)\n",
    "   ↓\n",
    "Final Output\n",
    "```\n",
    "\n",
    "Each step **depends on the previous one**.\n",
    "\n",
    "---\n",
    "\n",
    "### Minimal RunnableSequence Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81359976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RUNNABLE SEQUENCE'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "sequence = (\n",
    "    RunnableLambda(lambda x: x.strip())\n",
    "    | RunnableLambda(lambda x: x.upper())\n",
    ")\n",
    "\n",
    "sequence.invoke(\"  runnable sequence  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba5a8f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Output**\n",
    "\n",
    "```\n",
    "RUNNABLE SEQUENCE\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "* Step 1 trims whitespace\n",
    "* Step 2 uppercases text\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableSequence with Prompt + LLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4e02454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A RunnableSequence is a sequence of runnable tasks that can be executed concurrently in a multi-threaded environment. Each task is independent of the others and can be run in parallel, allowing for efficient utilization of system resources and increased performance. This concept is commonly used in programming to execute multiple tasks simultaneously.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 16, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cpyyd4DHddr150379ok2QK9tUX6sp', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4be9-9875-7502-9221-6093c2a25660-0', usage_metadata={'input_tokens': 16, 'output_tokens': 59, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain the following concept briefly:\\n{concept}\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnableLambda(lambda x: {\"concept\": x})\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "chain.invoke(\"RunnableSequence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d074901",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "Execution order:\n",
    "\n",
    "1. Wrap input into dict\n",
    "2. Render prompt\n",
    "3. Call LLM\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableSequence in RAG (Sequential Dependency)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5ff3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple retriever for demonstration\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_classic.schema import Document\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain is a framework for developing applications powered by language models.\"),\n",
    "    Document(page_content=\"RunnableSequence executes steps in order, where each step depends on the previous one.\"),\n",
    "    Document(page_content=\"Vector stores enable semantic search over documents using embeddings.\")\n",
    "]\n",
    "\n",
    "# Create vector store and retriever\n",
    "vectorstore = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eeb9090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain provides a platform for developing applications that utilize language models, while RunnableSequence allows for the sequential execution of steps in an application where each step builds upon the outcome of the previous one. Vector stores, on the other hand, enable semantic search capabilities over documents through the use of embeddings.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 51, 'total_tokens': 109, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cpz0QwgQ3HAZJgoIrDfdDE9BxVZNY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4beb-493c-7eb2-b418-16d2b0a60ea9-0', usage_metadata={'input_tokens': 51, 'output_tokens': 58, 'total_tokens': 109, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    retriever                 # input → documents\n",
    "    | RunnableLambda(lambda d: \"\\n\".join(x.page_content for x in d))\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"Answer using the context:\\n{context}\"\n",
    "    )\n",
    "    | llm\n",
    ")\n",
    "chain.invoke(\"What is LangChain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6bdada",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Why this must be a sequence:\n",
    "\n",
    "* Retriever output is **required** before prompt\n",
    "* Prompt output is **required** before LLM\n",
    "\n",
    "---\n",
    "\n",
    "### Explicit RunnableSequence (Alternative Form)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaf97e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "sequence = RunnableSequence(\n",
    "    RunnableLambda(lambda x: x + 1),\n",
    "    RunnableLambda(lambda x: x * 2),\n",
    "    RunnableLambda(lambda x: x - 3),\n",
    ")\n",
    "\n",
    "sequence.invoke(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50895429",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Output**\n",
    "\n",
    "```\n",
    "9\n",
    "```\n",
    "\n",
    "Calculation:\n",
    "\n",
    "```\n",
    "(5 + 1) * 2 - 3 = 9\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableSequence vs RunnableParallel\n",
    "\n",
    "| Aspect     | RunnableSequence | RunnableParallel |\n",
    "| ---------- | ---------------- | ---------------- |\n",
    "| Execution  | Ordered          | Concurrent       |\n",
    "| Dependency | Yes              | No               |\n",
    "| Output     | Single value     | Dict             |\n",
    "| Use case   | Pipelines        | Fan-out          |\n",
    "\n",
    "---\n",
    "\n",
    "### RunnableSequence vs RunnableLambda\n",
    "\n",
    "| Component        | Role                  |\n",
    "| ---------------- | --------------------- |\n",
    "| RunnableLambda   | Single transformation |\n",
    "| RunnableSequence | Ordered composition   |\n",
    "\n",
    "RunnableSequence is built **from** smaller runnables.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "* Input normalization → LLM\n",
    "* Retrieval → formatting → generation\n",
    "* Output parsing\n",
    "* Guardrails before model calls\n",
    "* Deterministic business logic\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "* Using sequence when parallelism is needed\n",
    "* Forgetting output type compatibility between steps\n",
    "* Mixing dict and string shapes unintentionally\n",
    "\n",
    "Rule:\n",
    "\n",
    "> Output of step *n* must match input of step *n+1*\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "RunnableSequence is the **“pipeline conveyor belt”** of LCEL.\n",
    "\n",
    "```\n",
    "One step finishes → next step starts\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Keep steps small and explicit\n",
    "* Use RunnableLambda for glue logic\n",
    "* Use RunnableParallel only when no dependency exists\n",
    "* Combine Sequence + Parallel for complex graphs\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* RunnableSequence defines **ordered execution**\n",
    "* Implemented with `|`\n",
    "* Core building block of LCEL\n",
    "* Essential for deterministic LLM pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
