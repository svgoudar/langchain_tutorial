{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa470ba",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Redis Cache \n",
    "\n",
    "\n",
    "**Redis** is an in-memory key–value data store used as a **high-performance distributed cache**.\n",
    "\n",
    "It stores data in RAM, making access extremely fast compared to databases or file systems.\n",
    "\n",
    "**Why it is used**\n",
    "\n",
    "* Microsecond-level latency\n",
    "* Shared cache across multiple servers\n",
    "* Built-in expiration (TTL)\n",
    "* Supports strings, hashes, lists, sets, vectors, JSON\n",
    "\n",
    "---\n",
    "\n",
    "### Where Redis Fits in an AI System\n",
    "\n",
    "```\n",
    "Client Request\n",
    "      ↓\n",
    "Redis Cache ── hit → Return Data\n",
    "      ↓ miss\n",
    "Database / LLM / Embedding Model → Store in Redis → Return\n",
    "```\n",
    "\n",
    "Redis becomes the **first lookup layer** before expensive operations.\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Redis Caching (Response Cache)\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "import redis\n",
    "import json\n",
    "\n",
    "r = redis.Redis(host=\"localhost\", port=6379, db=0, decode_responses=True)\n",
    "\n",
    "def get_response(prompt):\n",
    "    key = f\"resp:{prompt}\"\n",
    "\n",
    "    cached = r.get(key)\n",
    "    if cached:\n",
    "        print(\"Redis Cache Hit\")\n",
    "        return cached\n",
    "\n",
    "    print(\"Redis Cache Miss — Calling LLM\")\n",
    "    response = llm.invoke(prompt).content\n",
    "    r.set(key, response, ex=3600)   # TTL = 1 hour\n",
    "    return response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Embedding Cache Using Redis\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "import pickle, hashlib\n",
    "\n",
    "def embed_key(text):\n",
    "    return f\"emb:{hashlib.sha256(text.encode()).hexdigest()}\"\n",
    "\n",
    "def get_embedding(text):\n",
    "    key = embed_key(text)\n",
    "\n",
    "    cached = r.get(key)\n",
    "    if cached:\n",
    "        print(\"Embedding Cache Hit\")\n",
    "        return pickle.loads(bytes.fromhex(cached))\n",
    "\n",
    "    vector = embedding_model.encode(text)\n",
    "    r.set(key, pickle.dumps(vector).hex(), ex=86400)   # 24 hours\n",
    "    return vector\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Semantic Cache with Redis + Similarity\n",
    "\n",
    "#### Demonstration (Simplified)\n",
    "\n",
    "```python\n",
    "def get_semantic_response(prompt):\n",
    "    query_vec = get_embedding(prompt)\n",
    "\n",
    "    for key in r.scan_iter(\"emb:*\"):\n",
    "        cached_vec = pickle.loads(bytes.fromhex(r.get(key)))\n",
    "        similarity = cosine_similarity(query_vec, cached_vec)\n",
    "\n",
    "        if similarity > 0.9:\n",
    "            return r.get(key.replace(\"emb:\", \"resp:\"))\n",
    "\n",
    "    response = llm.invoke(prompt).content\n",
    "    r.set(f\"resp:{prompt}\", response)\n",
    "    return response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Cache Invalidation\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "def invalidate(key):\n",
    "    r.delete(key)\n",
    "```\n",
    "\n",
    "Invalidate when:\n",
    "\n",
    "* Prompt templates change\n",
    "* LLM model version changes\n",
    "* Knowledge base updates\n",
    "\n",
    "---\n",
    "\n",
    "### TTL & Eviction\n",
    "\n",
    "Redis supports:\n",
    "\n",
    "* TTL expiration\n",
    "* LRU / LFU eviction\n",
    "* Memory limits\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "r.set(\"key\", \"value\", ex=600)  # expires in 10 minutes\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "| Rule                         | Purpose               |\n",
    "| ---------------------------- | --------------------- |\n",
    "| Use namespaced keys          | Avoid collisions      |\n",
    "| Include model version in key | Prevent stale reuse   |\n",
    "| Apply TTL                    | Prevent outdated data |\n",
    "| Monitor hit ratio            | Measure performance   |\n",
    "| Use Redis Cluster            | Horizontal scaling    |\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "Redis Cache = Ultra-fast shared memory for your entire system\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Redis is the backbone cache for scalable AI systems\n",
    "* Supports response caching, embedding caching, retrieval caching\n",
    "* Enables low-latency and cost-efficient pipelines\n",
    "* Mandatory for production-grade LLM systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e1ac3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
