{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388cbc25",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## In-Memory Cache — Explanation\n",
    "\n",
    "An **in-memory cache** stores data directly in the application's RAM for **extremely fast access**.\n",
    "It avoids disk I/O, network calls, database queries, and repeated model computations.\n",
    "\n",
    "**Primary characteristics**\n",
    "\n",
    "* Nanosecond–microsecond access time\n",
    "* Process-local (not shared across machines unless explicitly synchronized)\n",
    "* Ideal for hot data and short-lived results\n",
    "\n",
    "---\n",
    "\n",
    "### Where It Fits in the Pipeline\n",
    "\n",
    "```\n",
    "Request\n",
    "  ↓\n",
    "In-Memory Cache ── hit → Return Data\n",
    "  ↓ miss\n",
    "Expensive Operation → Store in Cache → Return\n",
    "```\n",
    "\n",
    "**Demonstration**\n",
    "\n",
    "The cache becomes the **first checkpoint** before costly work.\n",
    "\n",
    "---\n",
    "\n",
    "### Simple In-Memory Cache (Dictionary)\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "cache = {}\n",
    "\n",
    "def get_data(key):\n",
    "    if key in cache:\n",
    "        print(\"Cache Hit\")\n",
    "        return cache[key]\n",
    "\n",
    "    print(\"Cache Miss — Computing\")\n",
    "    value = expensive_computation(key)\n",
    "    cache[key] = value\n",
    "    return value\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### LLM Response Cache in Memory\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "response_cache = {}\n",
    "\n",
    "def get_llm_response(prompt):\n",
    "    key = prompt.strip().lower()\n",
    "\n",
    "    if key in response_cache:\n",
    "        return response_cache[key]\n",
    "\n",
    "    response = llm.invoke(prompt).content\n",
    "    response_cache[key] = response\n",
    "    return response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Embedding Cache in Memory\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "embedding_cache = {}\n",
    "\n",
    "def get_embedding(text):\n",
    "    key = text.strip().lower()\n",
    "\n",
    "    if key in embedding_cache:\n",
    "        return embedding_cache[key]\n",
    "\n",
    "    vector = embedding_model.encode(text)\n",
    "    embedding_cache[key] = vector\n",
    "    return vector\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### TTL-Based In-Memory Cache\n",
    "\n",
    "Automatically expires old entries.\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "from cachetools import TTLCache\n",
    "\n",
    "cache = TTLCache(maxsize=10000, ttl=3600)\n",
    "\n",
    "def get_value(key):\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "\n",
    "    value = expensive_computation(key)\n",
    "    cache[key] = value\n",
    "    return value\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### LRU Eviction In-Memory Cache\n",
    "\n",
    "Evicts least-recently-used items.\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "from cachetools import LRUCache\n",
    "\n",
    "cache = LRUCache(maxsize=5000)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Invalidation Strategy\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "def invalidate(key):\n",
    "    cache.pop(key, None)\n",
    "```\n",
    "\n",
    "Invalidate when:\n",
    "\n",
    "* Input changes\n",
    "* Model version changes\n",
    "* Business rules change\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use In-Memory Cache\n",
    "\n",
    "| Use Case          | Benefit               |\n",
    "| ----------------- | --------------------- |\n",
    "| Hot LLM responses | Ultra-fast replies    |\n",
    "| Embeddings        | Skip recomputation    |\n",
    "| RAG documents     | Faster retrieval      |\n",
    "| Session data      | Zero network overhead |\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "In-Memory Cache = Your application’s working memory\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Fastest cache layer in any system\n",
    "* Perfect for short-lived, high-frequency data\n",
    "* Must be combined with TTL or eviction to prevent memory leaks\n",
    "* Foundation layer before Redis or database caches"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
