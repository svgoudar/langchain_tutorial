{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb8f8c0",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Text Splitter\n",
    "\n",
    "\n",
    "\n",
    "A **Text Splitter** in LangChain is a component that **breaks large documents into smaller, overlapping chunks** that are suitable for:\n",
    "\n",
    "* Embedding\n",
    "* Retrieval\n",
    "* LLM context windows\n",
    "\n",
    "> Text splitters operate on **Document objects** and return **smaller Document objects**.\n",
    "\n",
    "They do **not** call LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Text Splitting Is Necessary\n",
    "\n",
    "LLMs have:\n",
    "\n",
    "* Context window limits\n",
    "* Degraded reasoning on very long inputs\n",
    "\n",
    "Without splitting:\n",
    "\n",
    "* Tokens overflow\n",
    "* Important context is lost\n",
    "* Retrieval becomes inaccurate\n",
    "\n",
    "Text splitting ensures:\n",
    "\n",
    "* Each chunk fits the model context\n",
    "* Semantic meaning is preserved\n",
    "* Retrieval quality improves\n",
    "\n",
    "---\n",
    "\n",
    "### Where Text Splitter Fits in RAG\n",
    "\n",
    "```\n",
    "Document Loader\n",
    "   ↓\n",
    "Documents\n",
    "   ↓\n",
    "Text Splitter\n",
    "   ↓\n",
    "Chunks\n",
    "   ↓\n",
    "Embeddings\n",
    "   ↓\n",
    "Vector Store\n",
    "   ↓\n",
    "Retriever\n",
    "```\n",
    "\n",
    "Text splitting is an **ingestion-time operation**.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "#### Chunk Size\n",
    "\n",
    "Maximum number of characters or tokens per chunk.\n",
    "\n",
    "Example:\n",
    "\n",
    "* `chunk_size = 500`\n",
    "\n",
    "---\n",
    "\n",
    "#### Chunk Overlap\n",
    "\n",
    "Number of characters or tokens shared between adjacent chunks.\n",
    "\n",
    "Example:\n",
    "\n",
    "* `chunk_overlap = 50`\n",
    "\n",
    "Purpose:\n",
    "\n",
    "* Prevents context loss at boundaries\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Text Splitter Demonstration\n",
    "\n",
    "#### RecursiveCharacterTextSplitter (Most Used)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782cae4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sangouda\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Sample long text for demonstration\n",
    "long_text = \"\"\"\n",
    "Large language models (LLMs) are very large deep learning models that are pre-trained on vast amounts of data. \n",
    "The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. \n",
    "The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.\n",
    "\n",
    "Transformer LLMs are capable of unsupervised training, although a more precise explanation is that transformers perform self-learning. \n",
    "It is through this process that transformers learn to understand basic grammar, languages, and knowledge.\n",
    "\n",
    "Unlike earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. \n",
    "This allows the data scientists to use GPUs for training transformer-based LLMs, significantly reducing the training time.\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(long_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a091879",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Splitting Documents (Recommended)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213515de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Large language models (LLMs) are very large deep learning models that are pre-trained on vast amounts of data. \n",
      "The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. \n",
      "The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.\n",
      "Metadata: {'source': 'llm_overview.txt', 'page': 1}\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "Transformer LLMs are capable of unsupervised training, although a more precise explanation is that transformers perform self-learning. \n",
      "It is through this process that transformers learn to understand basic grammar, languages, and knowledge.\n",
      "Metadata: {'source': 'llm_overview.txt', 'page': 1}\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "Unlike earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. \n",
      "This allows the data scientists to use GPUs for training transformer-based LLMs, significantly reducing the training time.\n",
      "Metadata: {'source': 'llm_overview.txt', 'page': 1}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create sample documents\n",
    "documents = [\n",
    "\tDocument(\n",
    "\t\tpage_content=long_text,\n",
    "\t\tmetadata={\"source\": \"llm_overview.txt\", \"page\": 1}\n",
    "\t)\n",
    "]\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "# Display the chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "\tprint(f\"Chunk {i + 1}:\")\n",
    "\tprint(chunk.page_content)\n",
    "\tprint(f\"Metadata: {chunk.metadata}\")\n",
    "\tprint(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f3b25",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Each output chunk is a `Document` with inherited metadata.\n",
    "\n",
    "---\n",
    "\n",
    "### How Recursive Splitting Works\n",
    "\n",
    "The splitter tries separators **in order**:\n",
    "\n",
    "1. Paragraph (`\\n\\n`)\n",
    "2. Line (`\\n`)\n",
    "3. Sentence (`.`)\n",
    "4. Word (` `)\n",
    "5. Character fallback\n",
    "\n",
    "This preserves **semantic boundaries** as much as possible.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Text Splitter Types\n",
    "\n",
    "#### CharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab692fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_classic.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec342fd0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Simple but may cut sentences.\n",
    "\n",
    "---\n",
    "\n",
    "### RecursiveCharacterTextSplitter (Recommended)\n",
    "\n",
    "Best balance of:\n",
    "\n",
    "* Simplicity\n",
    "* Semantic preservation\n",
    "\n",
    "---\n",
    "\n",
    "### TokenTextSplitter\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=32\n",
    ")\n",
    "```\n",
    "\n",
    "Uses tokens instead of characters.\n",
    "Important for strict token limits.\n",
    "\n",
    "---\n",
    "\n",
    "### Language-Specific Splitters\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "```\n",
    "\n",
    "Used for:\n",
    "\n",
    "* Source code\n",
    "* Structured formats\n",
    "\n",
    "---\n",
    "\n",
    "### Text Splitter vs Document Loader\n",
    "\n",
    "| Aspect    | Document Loader | Text Splitter     |\n",
    "| --------- | --------------- | ----------------- |\n",
    "| Purpose   | Read data       | Chunk data        |\n",
    "| Input     | Raw source      | Documents         |\n",
    "| Output    | Documents       | Smaller Documents |\n",
    "| LLM usage | ❌               | ❌                 |\n",
    "\n",
    "---\n",
    "\n",
    "### Text Splitter vs Retriever\n",
    "\n",
    "| Aspect       | Text Splitter  | Retriever  |\n",
    "| ------------ | -------------- | ---------- |\n",
    "| When         | Ingestion time | Query time |\n",
    "| Function     | Chunking       | Searching  |\n",
    "| LLM involved | ❌              | ❌          |\n",
    "\n",
    "---\n",
    "\n",
    "### Metadata Handling (Critical)\n",
    "\n",
    "Each chunk retains metadata:\n",
    "\n",
    "```python\n",
    "Document(\n",
    "    page_content=\"chunk text\",\n",
    "    metadata={\n",
    "        \"source\": \"file.pdf\",\n",
    "        \"page\": 2,\n",
    "        \"chunk_id\": 3\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "This enables:\n",
    "\n",
    "* Source attribution\n",
    "* Filtering\n",
    "* Debugging\n",
    "\n",
    "---\n",
    "\n",
    "### Choosing Chunk Size (Guidelines)\n",
    "\n",
    "| Use Case             | Chunk Size |\n",
    "| -------------------- | ---------- |\n",
    "| General RAG          | 300–800    |\n",
    "| Dense technical text | 200–400    |\n",
    "| Narrative text       | 800–1200   |\n",
    "| Code                 | 100–300    |\n",
    "\n",
    "---\n",
    "\n",
    "### Choosing Chunk Overlap\n",
    "\n",
    "Typical values:\n",
    "\n",
    "* 10–20% of chunk size\n",
    "\n",
    "Too small:\n",
    "\n",
    "* Context loss\n",
    "\n",
    "Too large:\n",
    "\n",
    "* Redundant embeddings\n",
    "* Higher cost\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "#### Very large chunks\n",
    "\n",
    "❌ Poor retrieval precision\n",
    "\n",
    "#### No overlap\n",
    "\n",
    "❌ Boundary context loss\n",
    "\n",
    "#### Splitting at query time\n",
    "\n",
    "❌ Should be ingestion-only\n",
    "\n",
    "#### Ignoring token limits\n",
    "\n",
    "❌ Runtime failures\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Use RecursiveCharacterTextSplitter by default\n",
    "* Tune chunk size per domain\n",
    "* Always preserve metadata\n",
    "* Split before embedding\n",
    "* Validate token counts\n",
    "\n",
    "---\n",
    "\n",
    "### Text Splitter in Production RAG\n",
    "\n",
    "Typical setup:\n",
    "\n",
    "```python\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "```\n",
    "\n",
    "Stable, predictable, and widely used.\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “A Text Splitter in LangChain breaks documents into smaller overlapping chunks to fit LLM context windows and improve retrieval quality. It operates at ingestion time and is a core component of RAG pipelines.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **Load → Split → Embed**\n",
    "* **Smaller chunks → better recall**\n",
    "* **Overlap → better continuity**\n",
    "* **Ingestion time only**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
