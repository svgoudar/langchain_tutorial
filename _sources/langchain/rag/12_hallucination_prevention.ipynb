{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "127eff48",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Hallucination Prevention and Control\n",
    "\n",
    "\n",
    "A **hallucination** occurs when an LLM generates **confident but incorrect, fabricated, or unverifiable information** that is **not grounded in the provided data or reality**.\n",
    "\n",
    "> Hallucinations are not bugs — they are a natural outcome of probabilistic language generation.\n",
    "\n",
    "Prevention and control require **system-level design**, not just better prompts.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Hallucinations Happen\n",
    "\n",
    "#### Root Causes\n",
    "\n",
    "1. **No grounding data**\n",
    "2. **Ambiguous or underspecified prompts**\n",
    "3. **Excessive context or noisy context**\n",
    "4. **High temperature / randomness**\n",
    "5. **Training bias toward fluent completion**\n",
    "6. **Model guessing to satisfy the prompt**\n",
    "\n",
    "---\n",
    "\n",
    "### Where Hallucination Control Fits\n",
    "\n",
    "```\n",
    "User Input\n",
    "  ↓\n",
    "Input Guardrails\n",
    "  ↓\n",
    "Retrieval (Grounding)\n",
    "  ↓\n",
    "Context Selection\n",
    "  ↓\n",
    "Prompt Constraints\n",
    "  ↓\n",
    "LLM\n",
    "  ↓\n",
    "Output Validation\n",
    "```\n",
    "\n",
    "Hallucination control must exist **at every stage**.\n",
    "\n",
    "---\n",
    "\n",
    "### Layered Hallucination Prevention Strategy\n",
    "\n",
    "#### Layer 1: Grounding with Retrieval (Primary Defense)\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** forces the model to answer **from real documents**.\n",
    "\n",
    "```\n",
    "Question → Retrieve Context → Answer from Context\n",
    "```\n",
    "\n",
    "Key rule:\n",
    "\n",
    "> If it’s not in the retrieved context, the model should say *“I don’t know”*.\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 2: Strong Prompt Constraints\n",
    "\n",
    "#### Example Prompt Guardrail\n",
    "\n",
    "```text\n",
    "Answer only using the provided context.\n",
    "If the answer is not present, say \"I don’t know\".\n",
    "Do not use outside knowledge.\n",
    "```\n",
    "\n",
    "This reduces, but does not eliminate hallucinations.\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 3: Context Quality Control\n",
    "\n",
    "### Techniques\n",
    "\n",
    "* Proper **chunking**\n",
    "* **Chunk overlap**\n",
    "* **Top-k limits**\n",
    "* **MMR (diversity)**\n",
    "* **Reranking (precision)**\n",
    "\n",
    "Bad context = hallucinated answers.\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 4: Similarity Thresholding\n",
    "\n",
    "Reject weak retrievals.\n",
    "\n",
    "```text\n",
    "If similarity score < threshold → do not answer\n",
    "```\n",
    "\n",
    "Production pattern:\n",
    "\n",
    "```\n",
    "Low confidence → Ask clarifying question or escalate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 5: Output Structure Enforcement\n",
    "\n",
    "Structured outputs reduce creative guessing.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "llm.with_structured_output(Schema)\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "\n",
    "* Forces explicit fields\n",
    "* Prevents free-form fabrication\n",
    "* Enables validation\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 6: Output Validation & Guardrails\n",
    "\n",
    "**Validation Examples**\n",
    "\n",
    "* Schema validation (Pydantic)\n",
    "* Allowed values (enums)\n",
    "* Numeric range checks\n",
    "* Regex checks\n",
    "\n",
    "Invalid outputs are:\n",
    "\n",
    "* Rejected\n",
    "* Re-prompted\n",
    "* Escalated\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 7: Temperature and Sampling Control\n",
    "\n",
    "**Recommended Settings**\n",
    "\n",
    "| Task             | Temperature |\n",
    "| ---------------- | ----------- |\n",
    "| Fact-based QA    | 0.0–0.2     |\n",
    "| RAG answers      | 0.0         |\n",
    "| Creative writing | 0.7+        |\n",
    "\n",
    "Lower temperature = fewer hallucinations.\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 8: Context Window Management\n",
    "\n",
    "Too much context causes:\n",
    "\n",
    "* Contradictions\n",
    "* Confusion\n",
    "* Fabrication\n",
    "\n",
    "Use:\n",
    "\n",
    "* Reranking\n",
    "* Context compression\n",
    "* Summarization\n",
    "* Memory pruning\n",
    "\n",
    "**Relevant context beats maximum context.**\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 9: Source Attribution (Detection Tool)\n",
    "\n",
    "Hallucinations become visible when:\n",
    "\n",
    "* Sources are missing\n",
    "* Sources don’t support claims\n",
    "\n",
    "Production rule:\n",
    "\n",
    "> No source → low confidence answer.\n",
    "\n",
    "---\n",
    "\n",
    "#### Layer 10: Human-in-the-Loop (Fail-Safe)\n",
    "\n",
    "For critical systems:\n",
    "\n",
    "* Low confidence → escalate to human\n",
    "* Hallucination risk → require approval\n",
    "\n",
    "Used in:\n",
    "\n",
    "* Legal\n",
    "* Medical\n",
    "* Compliance\n",
    "* IT incident response\n",
    "\n",
    "---\n",
    "\n",
    "### Hallucination Control by Stage\n",
    "\n",
    "#### Input Stage\n",
    "\n",
    "* Prompt injection detection\n",
    "* Input sanitization\n",
    "* Clarification requests\n",
    "\n",
    "#### Retrieval Stage\n",
    "\n",
    "* Hybrid search\n",
    "* Reranking\n",
    "* Similarity thresholds\n",
    "\n",
    "#### Generation Stage\n",
    "\n",
    "* Low temperature\n",
    "* Constrained prompts\n",
    "* Tool-only answers\n",
    "\n",
    "#### Output Stage\n",
    "\n",
    "* Validation\n",
    "* Schema enforcement\n",
    "* Source checks\n",
    "\n",
    "---\n",
    "\n",
    "### What Does NOT Prevent Hallucinations\n",
    "\n",
    "❌ Prompt engineering alone\n",
    "❌ Larger context windows\n",
    "❌ More retrieved documents\n",
    "❌ Asking the model to “be accurate”\n",
    "❌ Trusting confidence in tone\n",
    "\n",
    "---\n",
    "\n",
    "### Common Production Mistakes\n",
    "\n",
    "#### Over-retrieving context\n",
    "\n",
    "❌ Increases hallucinations\n",
    "\n",
    "#### No similarity threshold\n",
    "\n",
    "❌ Model answers without evidence\n",
    "\n",
    "#### Trusting LLM citations\n",
    "\n",
    "❌ Citations can be hallucinated\n",
    "\n",
    "#### No fallback logic\n",
    "\n",
    "❌ Users get wrong answers instead of “unknown”\n",
    "\n",
    "---\n",
    "\n",
    "### Production-Grade Hallucination Control Checklist\n",
    "\n",
    "* ✅ RAG with high-quality data\n",
    "* ✅ Hybrid retrieval + reranking\n",
    "* ✅ Similarity thresholds\n",
    "* ✅ Strong system prompts\n",
    "* ✅ Structured output validation\n",
    "* ✅ Source attribution\n",
    "* ✅ Human escalation path\n",
    "* ✅ Observability & logging\n",
    "\n",
    "---\n",
    "\n",
    "### Measuring Hallucinations (Detection)\n",
    "\n",
    "Indicators:\n",
    "\n",
    "* Answer without sources\n",
    "* Low retrieval scores\n",
    "* Contradictory statements\n",
    "* Overconfident language\n",
    "\n",
    "Metrics:\n",
    "\n",
    "* Answer groundedness\n",
    "* Retrieval confidence\n",
    "* Source coverage ratio\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “Hallucination prevention is a layered system design problem. It combines grounding via retrieval, prompt constraints, context control, low-temperature generation, structured outputs, validation, source attribution, and human-in-the-loop safeguards.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **No grounding → hallucinations**\n",
    "* **More context ≠ safer answers**\n",
    "* **Structure reduces guessing**\n",
    "* **If accuracy matters, validate everything**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
