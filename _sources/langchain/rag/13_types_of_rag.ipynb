{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ec11b0",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "### Types of RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a pattern where an LLM:\n",
    "\n",
    "1. **Retrieves external knowledge**\n",
    "2. **Uses that knowledge to generate grounded answers**\n",
    "\n",
    "Different **types of RAG** exist based on **how retrieval, context handling, and reasoning are done**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Naive RAG (Basic RAG)\n",
    "\n",
    "**Description**\n",
    "\n",
    "The simplest RAG form:\n",
    "\n",
    "* Retrieve top-k chunks\n",
    "* Stuff them into the prompt\n",
    "* Generate an answer\n",
    "\n",
    "```\n",
    "Query → Vector Search → Stuff Context → LLM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Demonstration (LangChain – Naive RAG)**\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer using only the context\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{question}\")\n",
    "])\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\"How does ticket escalation work?\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Pros / Cons**\n",
    "\n",
    "✅ Simple\n",
    "❌ Hallucinations if context is weak\n",
    "❌ No reranking or validation\n",
    "\n",
    "---\n",
    "\n",
    "### Stuff RAG\n",
    "\n",
    "**Description**\n",
    "\n",
    "All retrieved chunks are **stuffed into a single prompt**.\n",
    "\n",
    "```\n",
    "Retrieve → Concatenate → LLM\n",
    "```\n",
    "\n",
    "Used internally by many early RAG examples.\n",
    "\n",
    "---\n",
    "\n",
    "**Demonstration**\n",
    "\n",
    "```python\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* Small datasets\n",
    "* Few short chunks\n",
    "\n",
    "---\n",
    "\n",
    "### MapReduce RAG\n",
    "\n",
    "**Description**\n",
    "\n",
    "Processes large documents safely:\n",
    "\n",
    "* Map: summarize each chunk\n",
    "* Reduce: combine summaries\n",
    "\n",
    "```\n",
    "Retrieve → Map (per chunk) → Reduce → LLM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Demonstration**\n",
    "\n",
    "```python\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"map_reduce\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* Large documents\n",
    "* Long PDFs\n",
    "* Reports\n",
    "\n",
    "---\n",
    "\n",
    "### Refine RAG\n",
    "\n",
    "**Description**\n",
    "\n",
    "Builds the answer **incrementally**:\n",
    "\n",
    "* First chunk → initial answer\n",
    "* Next chunks → refine answer\n",
    "\n",
    "```\n",
    "Chunk 1 → Answer\n",
    "Chunk 2 → Refine\n",
    "Chunk 3 → Refine\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Demonstration**\n",
    "\n",
    "```python\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"refine\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* Ordered documents\n",
    "* High accuracy summaries\n",
    "\n",
    "---\n",
    "\n",
    "###  Conversational RAG\n",
    "\n",
    "**Description**\n",
    "\n",
    "Adds **chat memory**:\n",
    "\n",
    "* Rewrites follow-up questions\n",
    "* Retrieves context-aware results\n",
    "\n",
    "```\n",
    "Chat History → Question Condenser → Retrieve → Answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Demonstration**\n",
    "\n",
    "```python\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "conv_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* Chatbots\n",
    "* Multi-turn Q&A\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Hybrid RAG\n",
    "\n",
    "**Description**\n",
    "\n",
    "Combines:\n",
    "\n",
    "* **Keyword search (BM25)**\n",
    "* **Vector search**\n",
    "\n",
    "```\n",
    "BM25 + Vector → Merge → Rerank → LLM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Demonstration**\n",
    "\n",
    "```python\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25, vector_retriever],\n",
    "    weights=[0.4, 0.6]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* Enterprise RAG\n",
    "* Error codes, IDs, logs\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Reranked RAG (Production Standard)\n",
    "\n",
    "**Description**\n",
    "\n",
    "Adds **precision** using rerankers.\n",
    "\n",
    "```\n",
    "Retrieve (Top 20) → Rerank → Top 5 → LLM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Demonstration**\n",
    "\n",
    "```python\n",
    "reranker = CrossEncoderReranker(\n",
    "    model=HuggingFaceCrossEncoder(\"BAAI/bge-reranker-base\"),\n",
    "    top_n=5\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=retriever,\n",
    "    base_compressor=reranker\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* High-accuracy systems\n",
    "* IT support, legal, finance\n",
    "\n",
    "---\n",
    "\n",
    "### MMR-based RAG (Diversity-Aware)\n",
    "\n",
    "**Description**\n",
    "\n",
    "Uses **Maximal Marginal Relevance** to reduce redundancy.\n",
    "\n",
    "```\n",
    "Retrieve → MMR → Diverse Context → LLM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Demonstration**\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.6}\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* Repetitive documents\n",
    "* Chunk overlap heavy data\n",
    "\n",
    "---\n",
    "\n",
    "### Agentic RAG\n",
    "\n",
    "**Description**\n",
    "\n",
    "LLM **decides how and when to retrieve**:\n",
    "\n",
    "* Iterative retrieval\n",
    "* Tool usage\n",
    "* Multi-step reasoning\n",
    "\n",
    "```\n",
    "Reason → Retrieve → Reason → Retrieve → Answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Demonstration (Conceptual)**\n",
    "\n",
    "```python\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=[retriever_tool]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* Complex queries\n",
    "* Multi-hop reasoning\n",
    "\n",
    "---\n",
    "\n",
    "###  Contextual / Adaptive RAG\n",
    "\n",
    "**Description**\n",
    "\n",
    "Retrieval strategy changes dynamically:\n",
    "\n",
    "* Based on query type\n",
    "* Based on confidence\n",
    "* Based on token budget\n",
    "\n",
    "```\n",
    "Simple query → Small RAG\n",
    "Complex query → Hybrid + Rerank\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Demonstration (Conceptual LCEL)**\n",
    "\n",
    "```python\n",
    "def route(query):\n",
    "    return hybrid_chain if len(query) > 50 else simple_chain\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### RAG Type Comparison\n",
    "\n",
    "| RAG Type       | Accuracy  | Cost      | Complexity |\n",
    "| -------------- | --------- | --------- | ---------- |\n",
    "| Naive          | Low       | Low       | Low        |\n",
    "| Stuff          | Low       | Low       | Low        |\n",
    "| MapReduce      | Medium    | Medium    | Medium     |\n",
    "| Refine         | High      | High      | Medium     |\n",
    "| Conversational | Medium    | Medium    | Medium     |\n",
    "| Hybrid         | High      | Medium    | High       |\n",
    "| Reranked       | Very High | High      | High       |\n",
    "| Agentic        | Very High | Very High | Very High  |\n",
    "\n",
    "---\n",
    "\n",
    "### Production-Grade RAG (Recommended Stack)\n",
    "\n",
    "```\n",
    "Hybrid Retrieval\n",
    "   ↓\n",
    "MMR\n",
    "   ↓\n",
    "Reranker\n",
    "   ↓\n",
    "Context Compression\n",
    "   ↓\n",
    "LLM\n",
    "   ↓\n",
    "Source Attribution\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “RAG systems range from naive stuffing approaches to advanced hybrid, reranked, and agentic architectures. Production-grade RAG typically combines hybrid retrieval, reranking, MMR, and strict context control to maximize accuracy and minimize hallucinations.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **Prototype → Naive / Stuff RAG**\n",
    "* **Large docs → MapReduce / Refine**\n",
    "* **Chat → Conversational RAG**\n",
    "* **Enterprise → Hybrid + Rerank**\n",
    "* **Complex reasoning → Agentic RAG**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
