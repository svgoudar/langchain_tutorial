{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d4f104",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Chunk Overlap \n",
    "\n",
    "\n",
    "**Chunk overlap** is the practice of **repeating a small portion of text between consecutive chunks** when splitting documents.\n",
    "\n",
    "> It ensures that **context at chunk boundaries is not lost** during embedding and retrieval.\n",
    "\n",
    "Chunk overlap is applied by **Text Splitters**, not by retrievers or LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Chunk Overlap Is Necessary\n",
    "\n",
    "Without overlap:\n",
    "\n",
    "* Sentences get cut in half\n",
    "* Important references are split\n",
    "* Retrieval misses relevant context\n",
    "* Answers become incomplete or wrong\n",
    "\n",
    "Overlap preserves **semantic continuity** across chunks.\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Example\n",
    "\n",
    "### Original Text\n",
    "\n",
    "```\n",
    "LangChain is a framework for building LLM-powered applications.\n",
    "It supports prompts, chains, agents, and retrieval.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Without Overlap\n",
    "\n",
    "```\n",
    "Chunk 1: LangChain is a framework for building LLM-powered\n",
    "Chunk 2: applications. It supports prompts, chains, agents, and retrieval.\n",
    "```\n",
    "\n",
    "Meaning is fragmented.\n",
    "\n",
    "---\n",
    "\n",
    "### With Overlap (Overlap = 5 words)\n",
    "\n",
    "```\n",
    "Chunk 1: LangChain is a framework for building LLM-powered applications\n",
    "Chunk 2: building LLM-powered applications. It supports prompts, chains, agents, and retrieval\n",
    "```\n",
    "\n",
    "Context is preserved.\n",
    "\n",
    "---\n",
    "\n",
    "### Where Chunk Overlap Fits in the Pipeline\n",
    "\n",
    "```\n",
    "Document Loader\n",
    "   ↓\n",
    "Text Splitter (chunk_size + chunk_overlap)\n",
    "   ↓\n",
    "Chunks\n",
    "   ↓\n",
    "Embeddings\n",
    "   ↓\n",
    "Vector Store\n",
    "```\n",
    "\n",
    "Chunk overlap is an **ingestion-time decision**.\n",
    "\n",
    "---\n",
    "\n",
    "### Chunk Size vs Chunk Overlap\n",
    "\n",
    "### Chunk Size\n",
    "\n",
    "* Maximum length of each chunk\n",
    "\n",
    "### Chunk Overlap\n",
    "\n",
    "* Portion of text reused in the next chunk\n",
    "\n",
    "Example:\n",
    "\n",
    "```text\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50\n",
    "```\n",
    "\n",
    "Each new chunk starts **50 characters/tokens before** the previous chunk ends.\n",
    "\n",
    "---\n",
    "\n",
    "### Demonstration (LangChain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3adcb49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sangouda\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Chunk 0:\n",
      "Large language models (LLMs) are very large deep learning models that are pre-trained on vast\n",
      "\n",
      "Chunk 1:\n",
      "pre-trained on vast amounts of data.\n",
      "\n",
      "Chunk 2:\n",
      "The underlying transformer is a set of neural networks that consist of an encoder and a decoder\n",
      "\n",
      "Chunk 3:\n",
      "and a decoder with self-attention capabilities.\n",
      "\n",
      "Chunk 4:\n",
      "The encoder and decoder extract meanings from a sequence of text and understand the relationships\n",
      "\n",
      "Chunk 5:\n",
      "the relationships between words and phrases in it.\n",
      "\n",
      "Chunk 6:\n",
      "Transformer LLMs are capable of unsupervised training, although a more precise explanation is that\n",
      "\n",
      "Chunk 7:\n",
      "explanation is that transformers perform self-learning.\n",
      "\n",
      "Chunk 8:\n",
      "It is through this process that transformers learn to understand basic grammar, languages, and\n",
      "\n",
      "Chunk 9:\n",
      "languages, and knowledge.\n",
      "\n",
      "Chunk 10:\n",
      "Unlike earlier recurrent neural networks (RNN) that sequentially process inputs, transformers\n",
      "\n",
      "Chunk 11:\n",
      "transformers process entire sequences in parallel.\n",
      "\n",
      "Chunk 12:\n",
      "This allows the data scientists to use GPUs for training transformer-based LLMs, significantly\n",
      "\n",
      "Chunk 13:\n",
      "LLMs, significantly reducing the training time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "long_text = \"\"\"\n",
    "Large language models (LLMs) are very large deep learning models that are pre-trained on vast amounts of data. \n",
    "The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. \n",
    "The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.\n",
    "\n",
    "Transformer LLMs are capable of unsupervised training, although a more precise explanation is that transformers perform self-learning. \n",
    "It is through this process that transformers learn to understand basic grammar, languages, and knowledge.\n",
    "\n",
    "Unlike earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. \n",
    "This allows the data scientists to use GPUs for training transformer-based LLMs, significantly reducing the training time.\n",
    "\"\"\"\n",
    "\n",
    "chunks = splitter.split_text(long_text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b0814",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You will observe:\n",
    "\n",
    "* Last 20 characters of Chunk N\n",
    "* Reappear at the start of Chunk N+1\n",
    "\n",
    "---\n",
    "\n",
    "### How Overlap Improves Retrieval\n",
    "\n",
    "### Without Overlap\n",
    "\n",
    "* Query matches partial concept\n",
    "* Chunk score too low\n",
    "* Not retrieved\n",
    "\n",
    "### With Overlap\n",
    "\n",
    "* Full concept appears in at least one chunk\n",
    "* Higher embedding similarity\n",
    "* Better recall\n",
    "\n",
    "---\n",
    "\n",
    "### Chunk Overlap in RAG Answers\n",
    "\n",
    "Overlap helps when:\n",
    "\n",
    "* Definitions span multiple sentences\n",
    "* Pronouns reference earlier context\n",
    "* Steps are explained across paragraphs\n",
    "* Code blocks cross chunk boundaries\n",
    "\n",
    "---\n",
    "\n",
    "### Choosing the Right Overlap Size\n",
    "\n",
    "### General Rule\n",
    "\n",
    "```\n",
    "chunk_overlap ≈ 10–20% of chunk_size\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Recommended Values\n",
    "\n",
    "| Chunk Size | Overlap |\n",
    "| ---------- | ------- |\n",
    "| 300        | 30–50   |\n",
    "| 500        | 50–80   |\n",
    "| 800        | 80–120  |\n",
    "\n",
    "---\n",
    "\n",
    "### Overlap: Character vs Token\n",
    "\n",
    "#### Character-Based Overlap\n",
    "\n",
    "* Simpler\n",
    "* Language-agnostic\n",
    "* Slightly imprecise for tokens\n",
    "\n",
    "#### Token-Based Overlap\n",
    "\n",
    "* Precise for model limits\n",
    "* More expensive\n",
    "* Used with `TokenTextSplitter`\n",
    "\n",
    "---\n",
    "\n",
    "#### Too Little Overlap (Problems)\n",
    "\n",
    "* Broken sentences\n",
    "* Lost references\n",
    "* Lower recall\n",
    "* Incomplete answers\n",
    "\n",
    "---\n",
    "\n",
    "### Too Much Overlap (Problems)\n",
    "\n",
    "* Duplicate embeddings\n",
    "* Higher storage cost\n",
    "* Redundant retrieval\n",
    "* Slower indexing\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "#### Overlap = 0\n",
    "\n",
    "❌ Almost always bad for RAG\n",
    "\n",
    "#### Overlap > 50%\n",
    "\n",
    "❌ Wasteful and noisy\n",
    "\n",
    "#### Changing overlap at query time\n",
    "\n",
    "❌ Overlap is ingestion-time only\n",
    "\n",
    "---\n",
    "\n",
    "### Chunk Overlap vs Sliding Window\n",
    "\n",
    "| Concept | Chunk Overlap       | Sliding Window        |\n",
    "| ------- | ------------------- | --------------------- |\n",
    "| Purpose | Preserve boundaries | Continuous context    |\n",
    "| Cost    | Low                 | High                  |\n",
    "| Usage   | RAG ingestion       | Time-series / streams |\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Always use some overlap\n",
    "* Tune overlap per document type\n",
    "* Increase overlap for dense text\n",
    "* Reduce overlap for repetitive text\n",
    "* Keep overlap stable once indexed\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “Chunk overlap is the intentional repetition of text between adjacent chunks to preserve semantic continuity. It improves retrieval recall and answer quality in RAG systems and is configured at ingestion time via text splitters.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **No overlap → broken context**\n",
    "* **10–20% overlap → optimal**\n",
    "* **More overlap → more cost**\n",
    "* **Tune once, index once**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
