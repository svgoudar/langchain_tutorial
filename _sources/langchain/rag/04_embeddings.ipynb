{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ddd5713",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Embeddings\n",
    "\n",
    "### What Embeddings Are\n",
    "\n",
    "**Embeddings** are **numerical vector representations of text** that capture **semantic meaning**.\n",
    "Similar meanings → vectors are **close in vector space**.\n",
    "\n",
    "> Embeddings convert text into numbers so machines can **search, compare, and retrieve meaning**, not just keywords.\n",
    "\n",
    "They are a **core primitive** for RAG, semantic search, clustering, and recommendation systems.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Embeddings Are Needed\n",
    "\n",
    "LLMs are generative.\n",
    "Embeddings are **representational**.\n",
    "\n",
    "Without embeddings:\n",
    "\n",
    "* Keyword search only\n",
    "* Poor semantic recall\n",
    "* No similarity matching\n",
    "\n",
    "With embeddings:\n",
    "\n",
    "* Semantic search\n",
    "* Context-aware retrieval\n",
    "* Scalable RAG pipelines\n",
    "* Efficient similarity comparisons\n",
    "\n",
    "---\n",
    "\n",
    "### Where Embeddings Fit in the RAG Pipeline\n",
    "\n",
    "```\n",
    "Raw Data\n",
    "   ↓\n",
    "Document Loader\n",
    "   ↓\n",
    "Text Splitter\n",
    "   ↓\n",
    "Chunks\n",
    "   ↓\n",
    "Embeddings  ← (THIS STEP)\n",
    "   ↓\n",
    "Vector Store\n",
    "   ↓\n",
    "Retriever\n",
    "   ↓\n",
    "LLM\n",
    "```\n",
    "\n",
    "Embeddings are created at **ingestion time** and reused at **query time**.\n",
    "\n",
    "---\n",
    "\n",
    "### What an Embedding Vector Looks Like\n",
    "\n",
    "Example (simplified):\n",
    "\n",
    "```text\n",
    "\"Email service is down\"\n",
    "→ [0.021, -0.113, 0.884, ..., -0.042]\n",
    "```\n",
    "\n",
    "Properties:\n",
    "\n",
    "* Fixed length (e.g., 384, 768, 1536 dimensions)\n",
    "* Dense floating-point numbers\n",
    "* Meaning encoded geometrically\n",
    "\n",
    "---\n",
    "\n",
    "### How Similarity Is Measured\n",
    "\n",
    "Common distance metrics:\n",
    "\n",
    "### Cosine Similarity (Most Common)\n",
    "\n",
    "```\n",
    "cosine(v1, v2) → similarity score\n",
    "```\n",
    "\n",
    "* Closer to 1 → more similar\n",
    "* Closer to 0 → unrelated\n",
    "\n",
    "### Other Metrics\n",
    "\n",
    "* Euclidean distance\n",
    "* Dot product\n",
    "\n",
    "Vector stores choose the metric.\n",
    "\n",
    "---\n",
    "\n",
    "### Embeddings in LangChain\n",
    "\n",
    "LangChain provides a **standard Embeddings interface** that abstracts providers.\n",
    "\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "```\n",
    "\n",
    "This object:\n",
    "\n",
    "* Converts text → vectors\n",
    "* Handles batching\n",
    "* Is provider-agnostic\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Embedding Demonstration\n",
    "\n",
    "#### Embedding a Single Query\n",
    "\n",
    "```python\n",
    "vector = embeddings.embed_query(\n",
    "    \"What is LangChain?\"\n",
    ")\n",
    "\n",
    "print(len(vector))\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "1536\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Embedding Multiple Documents\n",
    "\n",
    "```python\n",
    "texts = [\n",
    "    \"LangChain is a framework for LLM applications\",\n",
    "    \"RAG combines retrieval with generation\"\n",
    "]\n",
    "\n",
    "vectors = embeddings.embed_documents(texts)\n",
    "```\n",
    "\n",
    "Each text → one vector.\n",
    "\n",
    "---\n",
    "\n",
    "### Embeddings + Vector Store (Typical Usage)\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings\n",
    ")\n",
    "```\n",
    "\n",
    "Now the data is:\n",
    "\n",
    "* Embedded\n",
    "* Indexed\n",
    "* Searchable\n",
    "\n",
    "---\n",
    "\n",
    "### Query-Time Embedding\n",
    "\n",
    "At query time:\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\n",
    "    \"How does RAG work?\"\n",
    ")\n",
    "```\n",
    "\n",
    "Internally:\n",
    "\n",
    "1. Query is embedded\n",
    "2. Similar vectors are searched\n",
    "3. Top-k documents are returned\n",
    "\n",
    "---\n",
    "\n",
    "### Embeddings vs LLMs (Critical Difference)\n",
    "\n",
    "| Aspect      | Embeddings         | LLM        |\n",
    "| ----------- | ------------------ | ---------- |\n",
    "| Purpose     | Representation     | Generation |\n",
    "| Output      | Vectors            | Text       |\n",
    "| Determinism | High               | Variable   |\n",
    "| Cost        | Low                | Higher     |\n",
    "| Use case    | Search, clustering | Answering  |\n",
    "\n",
    "---\n",
    "\n",
    "### Embeddings vs TF-IDF / BM25\n",
    "\n",
    "| Feature          | Embeddings | Keyword Search |\n",
    "| ---------------- | ---------- | -------------- |\n",
    "| Semantic meaning | ✅          | ❌              |\n",
    "| Synonyms         | ✅          | ❌              |\n",
    "| Context-aware    | ✅          | ❌              |\n",
    "| Explainability   | ❌          | ✅              |\n",
    "\n",
    "Hybrid search often combines both.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Embedding Models\n",
    "\n",
    "#### OpenAI\n",
    "\n",
    "* `text-embedding-3-small`\n",
    "* `text-embedding-3-large`\n",
    "\n",
    "#### Open Source\n",
    "\n",
    "* SentenceTransformers (e.g., all-MiniLM)\n",
    "* BGE, E5 families\n",
    "\n",
    "#### Local Models\n",
    "\n",
    "* HuggingFace embeddings\n",
    "* Ollama embeddings\n",
    "\n",
    "LangChain supports all via adapters.\n",
    "\n",
    "---\n",
    "\n",
    "### Choosing an Embedding Model\n",
    "\n",
    "#### Key Factors\n",
    "\n",
    "* Dimensionality\n",
    "* Domain relevance\n",
    "* Latency\n",
    "* Cost\n",
    "* Language support\n",
    "\n",
    "---\n",
    "\n",
    "### General Recommendations\n",
    "\n",
    "| Use Case        | Recommendation        |\n",
    "| --------------- | --------------------- |\n",
    "| General RAG     | 768–1536 dims         |\n",
    "| Low latency     | Smaller models        |\n",
    "| Domain-specific | Fine-tuned embeddings |\n",
    "| On-prem         | Open-source models    |\n",
    "\n",
    "---\n",
    "\n",
    "### Embeddings and Chunking (Very Important)\n",
    "\n",
    "Embeddings work on **chunks**, not full documents.\n",
    "\n",
    "Bad chunking → bad embeddings.\n",
    "\n",
    "Rules:\n",
    "\n",
    "* Split before embedding\n",
    "* Use overlap\n",
    "* Keep chunks semantically coherent\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "#### Embedding full documents\n",
    "\n",
    "❌ Too large, poor retrieval\n",
    "\n",
    "#### No chunk overlap\n",
    "\n",
    "❌ Boundary context loss\n",
    "\n",
    "#### Re-embedding at query time\n",
    "\n",
    "❌ Wasteful\n",
    "\n",
    "#### Mixing embedding models\n",
    "\n",
    "❌ Incompatible vectors\n",
    "\n",
    "---\n",
    "\n",
    "### Embeddings Are NOT Knowledge\n",
    "\n",
    "Embeddings:\n",
    "\n",
    "* Do not store facts\n",
    "* Do not reason\n",
    "* Do not answer questions\n",
    "\n",
    "They only help **find relevant context**.\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Embed once, reuse many times\n",
    "* Keep embedding model consistent\n",
    "* Store metadata with vectors\n",
    "* Monitor vector drift when data updates\n",
    "* Re-embed if chunking strategy changes\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “Embeddings are dense vector representations of text that capture semantic meaning. In LangChain, embeddings are used to index document chunks in vector stores, enabling semantic retrieval for RAG systems.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **Search → Embeddings**\n",
    "* **Answer → LLM**\n",
    "* **Good RAG → Good chunking + good embeddings**\n",
    "* **Change chunking → Re-embed**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
