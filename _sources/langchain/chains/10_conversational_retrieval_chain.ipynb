{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871cc911",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## ConversationalRetrievalChain\n",
    "\n",
    "### What ConversationalRetrievalChain Is\n",
    "\n",
    "A **ConversationalRetrievalChain** is a LangChain abstraction that extends **RetrievalQA** to support **multi-turn conversations** by combining:\n",
    "\n",
    "* **Chat history (memory)**\n",
    "* **Question rewriting**\n",
    "* **Document retrieval**\n",
    "* **Answer generation**\n",
    "\n",
    "> It enables **context-aware RAG**, where follow-up questions depend on previous turns.\n",
    "\n",
    "---\n",
    "\n",
    "### Why ConversationalRetrievalChain Exists\n",
    "\n",
    "In real conversations, users ask:\n",
    "\n",
    "* “What is LangChain?”\n",
    "* “How does it work?”\n",
    "* “Does it support agents?”\n",
    "\n",
    "The last two questions are **ambiguous without context**.\n",
    "\n",
    "ConversationalRetrievalChain:\n",
    "\n",
    "* Rewrites follow-up questions into standalone queries\n",
    "* Retrieves relevant documents using that rewritten query\n",
    "* Produces grounded answers using conversation context\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Flow\n",
    "\n",
    "```\n",
    "User Question\n",
    "   ↓\n",
    "Chat History\n",
    "   ↓\n",
    "Question Condenser (LLM)\n",
    "   ↓\n",
    "Standalone Question\n",
    "   ↓\n",
    "Retriever\n",
    "   ↓\n",
    "Relevant Documents\n",
    "   ↓\n",
    "LLM\n",
    "   ↓\n",
    "Answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Core Components\n",
    "\n",
    "#### Chat History (Memory)\n",
    "\n",
    "Stores prior Human/AI messages and provides conversational context.\n",
    "\n",
    "---\n",
    "\n",
    "### Question Condenser\n",
    "\n",
    "An LLM prompt that converts a follow-up question into a **standalone query**.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Original: \"Does it support agents?\"\n",
    "Rewritten: \"Does LangChain support agents?\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Retriever\n",
    "\n",
    "Fetches relevant documents using the rewritten question.\n",
    "\n",
    "---\n",
    "\n",
    "### Answer Generation Chain\n",
    "\n",
    "Uses retrieved documents + question to generate the final answer.\n",
    "\n",
    "---\n",
    "\n",
    "### Basic ConversationalRetrievalChain Demonstration\n",
    "\n",
    "#### Step 1: Create a Retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8725f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "texts = [\n",
    "    \"LangChain is a framework for building LLM-powered applications.\",\n",
    "    \"LangChain supports agents, tools, and retrieval pipelines.\"\n",
    "]\n",
    "\n",
    "vectorstore = FAISS.from_texts(texts, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7456fd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Create Memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e07fd745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4371e3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Create the ConversationalRetrievalChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2f1d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_classic.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354694d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Run a Conversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a5ffba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Does it support agents?',\n",
       " 'chat_history': [HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='LangChain is a framework for building applications powered by large language models (LLMs). It supports various components such as agents, tools, and retrieval pipelines to facilitate the development of these applications.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Does it support agents?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Yes, LangChain supports agents.', additional_kwargs={}, response_metadata={})],\n",
       " 'answer': 'Yes, LangChain supports agents.',\n",
       " 'source_documents': [Document(id='c72ccfcb-188c-4eb0-9ce3-946df5995104', metadata={}, page_content='LangChain supports agents, tools, and retrieval pipelines.'),\n",
       "  Document(id='84cf6952-5d3b-431c-b442-d69d1cfe9665', metadata={}, page_content='LangChain is a framework for building LLM-powered applications.')]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke({\"question\": \"What is LangChain?\"})\n",
    "qa_chain.invoke({\"question\": \"Does it support agents?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d113b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The second question is automatically **contextualized**.\n",
    "\n",
    "---\n",
    "\n",
    "### What the Chain Returns\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"answer\": \"...\",\n",
    "  \"source_documents\": [...],\n",
    "  \"chat_history\": [...]\n",
    "}\n",
    "```\n",
    "\n",
    "Useful for:\n",
    "\n",
    "* UI display\n",
    "* Citations\n",
    "* Debugging\n",
    "\n",
    "---\n",
    "\n",
    "### ConversationalRetrievalChain vs RetrievalQA\n",
    "\n",
    "| Aspect             | RetrievalQA | ConversationalRetrievalChain |\n",
    "| ------------------ | ----------- | ---------------------------- |\n",
    "| Chat history       | ❌           | ✅                            |\n",
    "| Multi-turn         | ❌           | ✅                            |\n",
    "| Question rewriting | ❌           | ✅                            |\n",
    "| Context-aware      | ❌           | ✅                            |\n",
    "\n",
    "---\n",
    "\n",
    "### Chain Types for Answer Generation\n",
    "\n",
    "Internally, it supports the same document-combining strategies:\n",
    "\n",
    "* `stuff`\n",
    "* `map_reduce`\n",
    "* `refine`\n",
    "* `map_rerank`\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations of ConversationalRetrievalChain\n",
    "\n",
    "* ❌ Legacy abstraction\n",
    "* ❌ Less control over internals\n",
    "* ❌ Harder to customize prompts\n",
    "* ❌ Not fully Runnable-based\n",
    "\n",
    "---\n",
    "\n",
    "### Conversational Retrieval vs LCEL (Modern RAG)\n",
    "\n",
    "### Legacy Approach\n",
    "\n",
    "```python\n",
    "ConversationalRetrievalChain.from_llm(...)\n",
    "```\n",
    "\n",
    "### LCEL-Based Context-Aware RAG (Recommended)\n",
    "\n",
    "```python\n",
    "standalone_q = condense_prompt | llm\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"input\": RunnablePassthrough()}\n",
    "    | answer_prompt\n",
    "    | llm\n",
    ")\n",
    "```\n",
    "\n",
    "LCEL offers:\n",
    "\n",
    "* Full control\n",
    "* Streaming\n",
    "* Custom guardrails\n",
    "* Better observability\n",
    "\n",
    "---\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "* Chatbots over documents\n",
    "* Internal knowledge assistants\n",
    "* Helpdesk bots\n",
    "* Enterprise search chat\n",
    "* Multi-turn Q&A systems\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Use low temperature (0–0.2)\n",
    "* Limit retrieved chunks\n",
    "* Enable source documents\n",
    "* Monitor question rewriting quality\n",
    "* Plan migration to LCEL for production\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use ConversationalRetrievalChain\n",
    "\n",
    "* Learning conversational RAG\n",
    "* Rapid prototyping\n",
    "* Legacy LangChain systems\n",
    "\n",
    "---\n",
    "\n",
    "### When NOT to Use It\n",
    "\n",
    "* New production systems\n",
    "* Complex agentic workflows\n",
    "* Advanced reranking pipelines\n",
    "* Streaming-first APIs\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “ConversationalRetrievalChain is a LangChain abstraction for multi-turn, context-aware RAG. It rewrites follow-up questions using chat history, retrieves relevant documents, and generates grounded answers, but is a legacy construct superseded by LCEL-based pipelines.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **Single-turn QA → RetrievalQA**\n",
    "* **Multi-turn document chat → ConversationalRetrievalChain**\n",
    "* **Production conversational RAG → LCEL or LangGraph**\n",
    "\n",
    "---\n",
    "\n",
    "If you want next:\n",
    "\n",
    "* LCEL conversational RAG from scratch\n",
    "* Question condensation prompt design\n",
    "* Memory strategies for chat RAG\n",
    "* Reranking in conversational search\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
