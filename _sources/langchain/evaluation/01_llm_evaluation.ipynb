{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ebeb57",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## LLM Evaluation \n",
    "\n",
    "\n",
    "**LLM Evaluation** is the systematic process of **measuring the quality, reliability, and usefulness** of LLM outputs against defined criteria such as correctness, relevance, faithfulness, safety, latency, and cost.\n",
    "\n",
    "Evaluation answers:\n",
    "\n",
    "* *Is the model giving correct answers?*\n",
    "* *Is RAG grounded in sources?*\n",
    "* *Is one prompt/model better than another?*\n",
    "* *Did performance regress after a change?*\n",
    "\n",
    "It is natively supported in LangChain and visualized/managed in LangSmith.\n",
    "\n",
    "---\n",
    "\n",
    "### Why LLM Evaluation Is Mandatory\n",
    "\n",
    "Without evaluation:\n",
    "\n",
    "* You rely on intuition\n",
    "* Regressions go unnoticed\n",
    "* Hallucinations ship to prod\n",
    "\n",
    "With evaluation:\n",
    "\n",
    "* Objective quality metrics\n",
    "* Safe prompt/model iteration\n",
    "* Continuous improvement\n",
    "* Production confidence\n",
    "\n",
    "---\n",
    "\n",
    "### What Can Be Evaluated\n",
    "\n",
    "| Dimension    | What It Measures                 |\n",
    "| ------------ | -------------------------------- |\n",
    "| Correctness  | Is the answer right?             |\n",
    "| Relevance    | Does it answer the question?     |\n",
    "| Faithfulness | Is it grounded in context (RAG)? |\n",
    "| Safety       | Is output compliant?             |\n",
    "| Consistency  | Is behavior stable across runs?  |\n",
    "| Latency      | How fast is it?                  |\n",
    "| Cost         | How expensive is it?             |\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Architecture\n",
    "\n",
    "![Image](https://www.researchgate.net/publication/383918543/figure/fig3/AS%3A11431281277260711%401726025711738/Evaluation-pipeline-Each-green-square-represents-a-call-to-an-LLM-while-the-blue-dotted.png)\n",
    "\n",
    "![Image](https://images.ctfassets.net/otwaplf7zuwf/77WwI4e8hpjjIzazrSEdTp/95391e9b0e5d9c39a1d57690ebdcdea9/image.png)\n",
    "\n",
    "![Image](https://i0.wp.com/gradientflow.com/wp-content/uploads/2024/08/RAG-Architecture.png?fit=1568%2C909\\&ssl=1)\n",
    "\n",
    "```\n",
    "Dataset → Prompt/Chain → LLM Output → Evaluators → Scores → Report\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Dataset-Based Evaluation (Offline)\n",
    "\n",
    "#### Prepare an Evaluation Dataset\n",
    "\n",
    "```python\n",
    "dataset = [\n",
    "    {\n",
    "        \"question\": \"What is RAG?\",\n",
    "        \"expected\": \"Retrieval Augmented Generation combines retrieval with generation.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is token streaming?\",\n",
    "        \"expected\": \"Streaming returns tokens incrementally as they are generated.\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Define the Chain Under Test\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question clearly:\\n{question}\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Run the Chain on the Dataset\n",
    "\n",
    "```python\n",
    "predictions = []\n",
    "for item in dataset:\n",
    "    out = chain.invoke({\"question\": item[\"question\"]})\n",
    "    predictions.append({\n",
    "        \"question\": item[\"question\"],\n",
    "        \"answer\": out.content,\n",
    "        \"expected\": item[\"expected\"]\n",
    "    })\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Automatic LLM-as-a-Judge Evaluation\n",
    "\n",
    "#### Create a Correctness Evaluator\n",
    "\n",
    "```python\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\n",
    "    \"labeled_criteria\",\n",
    "    criteria=\"correctness\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Score Predictions\n",
    "\n",
    "```python\n",
    "for p in predictions:\n",
    "    result = evaluator.evaluate_strings(\n",
    "        prediction=p[\"answer\"],\n",
    "        reference=p[\"expected\"],\n",
    "        input=p[\"question\"]\n",
    "    )\n",
    "    print(p[\"question\"], \"→\", result[\"score\"])\n",
    "```\n",
    "\n",
    "**Typical Output**\n",
    "\n",
    "```\n",
    "What is RAG? → 0.92\n",
    "What is token streaming? → 0.89\n",
    "```\n",
    "\n",
    "Scores are normalized (0–1).\n",
    "\n",
    "---\n",
    "\n",
    "### RAG Faithfulness Evaluation\n",
    "\n",
    "#### Faithfulness \n",
    "\n",
    "```python\n",
    "faithfulness = load_evaluator(\"faithfulness\")\n",
    "\n",
    "result = faithfulness.evaluate_strings(\n",
    "    prediction=\"RAG retrieves documents then generates answers.\",\n",
    "    input=\"Explain RAG\",\n",
    "    reference=\"Retrieved docs explain RAG combines retrieval + generation.\"\n",
    ")\n",
    "\n",
    "print(result)\n",
    "```\n",
    "\n",
    "This detects **hallucinations** when answers are not supported by context.\n",
    "\n",
    "---\n",
    "\n",
    "### Pairwise Model / Prompt Comparison\n",
    "\n",
    "#### A/B Evaluation\n",
    "\n",
    "```python\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "pairwise = load_evaluator(\"pairwise_string\")\n",
    "\n",
    "pairwise.evaluate_string_pairs(\n",
    "    input=\"Explain RAG\",\n",
    "    prediction=\"RAG uses retrieval + LLM generation.\",\n",
    "    prediction_b=\"RAG just searches the web.\"\n",
    ")\n",
    "```\n",
    "\n",
    "Returns which answer is **better and why**.\n",
    "\n",
    "---\n",
    "\n",
    "### Latency & Cost Evaluation (Quantitative)\n",
    "\n",
    "#### Measure Latency and Cost\n",
    "\n",
    "```python\n",
    "import time\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    start = time.time()\n",
    "    chain.invoke({\"question\": \"Explain evaluation\"})\n",
    "    latency = time.time() - start\n",
    "\n",
    "print(\"Latency:\", latency)\n",
    "print(\"Cost:\", cb.total_cost)\n",
    "```\n",
    "\n",
    "Now you evaluate **quality + performance + cost** together.\n",
    "\n",
    "---\n",
    "\n",
    "### Continuous Evaluation with Tracing\n",
    "\n",
    "#### Enable Tracing for Eval Runs\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=true\n",
    "export LANGCHAIN_PROJECT=llm-evaluation\n",
    "```\n",
    "\n",
    "Each evaluation run records:\n",
    "\n",
    "* Inputs/outputs\n",
    "* Scores\n",
    "* Tokens\n",
    "* Latency\n",
    "* Regressions\n",
    "\n",
    "Reviewed visually in LangSmith.\n",
    "\n",
    "---\n",
    "\n",
    "### Manual (Human-in-the-Loop) Evaluation\n",
    "\n",
    "When required (compliance/high risk):\n",
    "\n",
    "* Human scores outputs\n",
    "* Labels stored\n",
    "* Used as gold standard\n",
    "* Combined with automated evals\n",
    "\n",
    "---\n",
    "\n",
    "### Common Evaluation Pitfalls\n",
    "\n",
    "* Evaluating without a dataset\n",
    "* Using only one metric\n",
    "* Ignoring faithfulness in RAG\n",
    "* No baseline for comparison\n",
    "* Evaluating only quality, not cost/latency\n",
    "\n",
    "---\n",
    "\n",
    "### Best-Practice Evaluation Strategy\n",
    "\n",
    "| Stage        | Evaluation         |\n",
    "| ------------ | ------------------ |\n",
    "| Prompt dev   | LLM-as-judge       |\n",
    "| RAG tuning   | Faithfulness       |\n",
    "| Model choice | Pairwise           |\n",
    "| Pre-prod     | Dataset regression |\n",
    "| Prod         | Sampling + tracing |\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "LLM Evaluation is **unit testing for intelligence**.\n",
    "\n",
    "```\n",
    "Change prompt/model → run eval → compare scores → ship safely\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* LLM evaluation makes quality measurable\n",
    "* Supports correctness, faithfulness, safety, cost, latency\n",
    "* Can be automated or human-reviewed\n",
    "* Essential for production-grade AI systems"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
