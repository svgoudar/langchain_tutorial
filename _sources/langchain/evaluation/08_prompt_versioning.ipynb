{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f61e389",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Regression Testing\n",
    "\n",
    "**Regression testing** ensures that **existing behavior does not degrade** when you change something in your system—such as:\n",
    "\n",
    "* Prompt updates\n",
    "* Model upgrades\n",
    "* Retriever / chunking changes\n",
    "* Tool or agent logic changes\n",
    "\n",
    "In LLM systems, regression testing answers:\n",
    "\n",
    "> **Did this change make answers worse than before?**\n",
    "\n",
    "It is a **mandatory practice** for production LLM, RAG, and agent systems.\n",
    "\n",
    "Implemented commonly using LangChain and tracked via LangSmith.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Regression Testing Is Critical for LLMs\n",
    "\n",
    "Unlike traditional software:\n",
    "\n",
    "* LLM outputs are **non-deterministic**\n",
    "* Small prompt changes can cause **large behavior shifts**\n",
    "* Model upgrades can silently break quality\n",
    "\n",
    "Without regression testing:\n",
    "\n",
    "* Hallucinations slip into prod\n",
    "* Answer quality degrades unnoticed\n",
    "* Trust is lost\n",
    "\n",
    "---\n",
    "\n",
    "### What Can Regress in LLM Systems\n",
    "\n",
    "| Area         | Example Regression         |\n",
    "| ------------ | -------------------------- |\n",
    "| Prompt       | More verbose, less precise |\n",
    "| RAG          | Wrong documents retrieved  |\n",
    "| Faithfulness | New hallucinations         |\n",
    "| Relevance    | Topic drift                |\n",
    "| Correctness  | Factually wrong answers    |\n",
    "| Latency      | Slower responses           |\n",
    "| Cost         | Token explosion            |\n",
    "\n",
    "---\n",
    "\n",
    "### Regression Testing Architecture\n",
    "\n",
    "![Image](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/66d91e930996182ebc2b81f4_668296a3c80f1adbb05ede7c_01_llm_regression_testing_process-min.png)\n",
    "\n",
    "![Image](https://exactpro.com/sites/default/files/inline-images/4%20SOR%20RAG.png)\n",
    "\n",
    "![Image](https://arize.com/wp-content/uploads/2024/12/image1-1024x256.png)\n",
    "\n",
    "```\n",
    "Baseline Version (v1)\n",
    "        ↓\n",
    " Evaluation Dataset\n",
    "        ↓\n",
    " Metrics (saved)\n",
    "        ↓\n",
    " New Version (v2)\n",
    "        ↓\n",
    " Same Dataset\n",
    "        ↓\n",
    " Compare Metrics → Pass / Fail\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Create a Fixed Regression Dataset\n",
    "\n",
    "#### Regression Dataset (Golden Set)\n",
    "\n",
    "```python\n",
    "regression_data = [\n",
    "    {\n",
    "        \"question\": \"What is RAG?\",\n",
    "        \"expected\": \"RAG combines document retrieval with language model generation.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is token streaming?\",\n",
    "        \"expected\": \"Token streaming sends output incrementally as it is generated.\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "This dataset must:\n",
    "\n",
    "* Stay **stable**\n",
    "* Represent **real user queries**\n",
    "* Cover **edge cases**\n",
    "\n",
    "---\n",
    "\n",
    "### Define Baseline and New Chains\n",
    "\n",
    "#### Baseline Chain (v1)\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm_v1 = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt_v1 = ChatPromptTemplate.from_template(\n",
    "    \"Answer clearly:\\n{question}\"\n",
    ")\n",
    "\n",
    "chain_v1 = prompt_v1 | llm_v1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### New Chain (v2 – After Change)\n",
    "\n",
    "```python\n",
    "llm_v2 = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "prompt_v2 = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question concisely and accurately:\\n{question}\"\n",
    ")\n",
    "\n",
    "chain_v2 = prompt_v2 | llm_v2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Run Both Versions on the Same Dataset\n",
    "\n",
    "#### Collect Outputs\n",
    "\n",
    "```python\n",
    "def run_chain(chain, data):\n",
    "    results = []\n",
    "    for row in data:\n",
    "        out = chain.invoke({\"question\": row[\"question\"]})\n",
    "        results.append({\n",
    "            \"question\": row[\"question\"],\n",
    "            \"answer\": out.content,\n",
    "            \"expected\": row[\"expected\"]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "baseline_results = run_chain(chain_v1, regression_data)\n",
    "new_results = run_chain(chain_v2, regression_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluate and Compare (Correctness Example)\n",
    "\n",
    "#### Correctness Evaluation\n",
    "\n",
    "```python\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "correctness_eval = load_evaluator(\n",
    "    \"labeled_criteria\",\n",
    "    criteria=\"correctness\"\n",
    ")\n",
    "\n",
    "def score(results):\n",
    "    scores = []\n",
    "    for r in results:\n",
    "        s = correctness_eval.evaluate_strings(\n",
    "            input=r[\"question\"],\n",
    "            prediction=r[\"answer\"],\n",
    "            reference=r[\"expected\"]\n",
    "        )\n",
    "        scores.append(s[\"score\"])\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "baseline_score = score(baseline_results)\n",
    "new_score = score(new_results)\n",
    "\n",
    "print(\"Baseline score:\", baseline_score)\n",
    "print(\"New score:\", new_score)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Regression Decision\n",
    "\n",
    "```python\n",
    "if new_score < baseline_score - 0.05:\n",
    "    raise Exception(\"❌ Regression detected\")\n",
    "else:\n",
    "    print(\"✅ No regression\")\n",
    "```\n",
    "\n",
    "This introduces a **tolerance band** (industry standard).\n",
    "\n",
    "---\n",
    "\n",
    "### Regression Testing for RAG (Faithfulness)\n",
    "\n",
    "#### Faithfulness Regression Check\n",
    "\n",
    "```python\n",
    "faithfulness_eval = load_evaluator(\"faithfulness\")\n",
    "\n",
    "def faithfulness_score(answer, docs, question):\n",
    "    context = \"\\n\".join(d.page_content for d in docs)\n",
    "    return faithfulness_eval.evaluate_strings(\n",
    "        input=question,\n",
    "        prediction=answer,\n",
    "        reference=context\n",
    "    )[\"score\"]\n",
    "```\n",
    "\n",
    "Fail if:\n",
    "\n",
    "* Faithfulness score drops\n",
    "* New hallucinations appear\n",
    "\n",
    "---\n",
    "\n",
    "### Latency & Cost Regression\n",
    "\n",
    "#### Performance Regression Check\n",
    "\n",
    "```python\n",
    "import time\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    start = time.time()\n",
    "    chain_v2.invoke({\"question\": \"Explain RAG\"})\n",
    "    latency = time.time() - start\n",
    "\n",
    "print(\"Latency:\", latency)\n",
    "print(\"Cost:\", cb.total_cost)\n",
    "```\n",
    "\n",
    "Fail if:\n",
    "\n",
    "* Latency ↑ beyond SLA\n",
    "* Cost ↑ beyond budget\n",
    "\n",
    "---\n",
    "\n",
    "#### CI/CD Integration (Real-World)\n",
    "\n",
    "Typical pipeline:\n",
    "\n",
    "```\n",
    "PR opened\n",
    "  ↓\n",
    "Run regression dataset\n",
    "  ↓\n",
    "Evaluate metrics\n",
    "  ↓\n",
    "Compare with baseline\n",
    "  ↓\n",
    "Fail build if regression\n",
    "```\n",
    "\n",
    "LangSmith can:\n",
    "\n",
    "* Store baselines\n",
    "* Compare versions visually\n",
    "* Track regressions over time\n",
    "\n",
    "---\n",
    "\n",
    "### Common Regression Failure Patterns\n",
    "\n",
    "| Pattern            | Cause                |\n",
    "| ------------------ | -------------------- |\n",
    "| Lower correctness  | Prompt drift         |\n",
    "| New hallucinations | Retrieval changes    |\n",
    "| Lower relevance    | Over-verbose prompts |\n",
    "| Latency spike      | Larger context       |\n",
    "| Cost spike         | More tokens          |\n",
    "\n",
    "---\n",
    "\n",
    "### Best-Practice Regression Strategy\n",
    "\n",
    "| Stage          | What to Regress         |\n",
    "| -------------- | ----------------------- |\n",
    "| Prompt changes | Relevance, correctness  |\n",
    "| RAG changes    | Faithfulness, retrieval |\n",
    "| Model upgrade  | All metrics             |\n",
    "| Prod           | Sampled regression      |\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "Regression testing is **unit testing for LLM behavior**.\n",
    "\n",
    "```\n",
    "Known good behavior → lock it → compare every change\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Regression testing prevents silent quality loss\n",
    "* Always compare against a baseline\n",
    "* Use fixed datasets\n",
    "* Evaluate correctness, relevance, faithfulness, latency, cost\n",
    "* Mandatory for production LLM and RAG systems"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
