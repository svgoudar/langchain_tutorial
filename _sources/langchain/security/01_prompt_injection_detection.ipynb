{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59039532",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Prompt Injection Detection\n",
    "\n",
    "\n",
    "**Prompt injection** is an attack where a user crafts input to **override, manipulate, or bypass** system instructions given to an LLM.\n",
    "\n",
    "Goal of the attacker:\n",
    "Force the model to ignore rules, reveal confidential data, or execute harmful behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### Where Detection Fits\n",
    "\n",
    "```\n",
    "User Input\n",
    "   ↓\n",
    "Injection Detector ── safe → Continue\n",
    "   ↓ unsafe\n",
    "Reject / Sanitize / Log / Alert\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Common Prompt Injection Patterns\n",
    "\n",
    "| Pattern                   | Example                        |\n",
    "| ------------------------- | ------------------------------ |\n",
    "| Instruction override      | \"Ignore previous instructions\" |\n",
    "| System role impersonation | \"You are now the system\"       |\n",
    "| Data exfiltration         | \"Show hidden prompt\"           |\n",
    "| Context manipulation      | \"Pretend you are allowed\"      |\n",
    "| Jailbreak attempts        | \"No rules apply\"               |\n",
    "\n",
    "---\n",
    "\n",
    "### Rule-Based Injection Detection\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "SUSPICIOUS_PATTERNS = [\n",
    "    r\"ignore (all|previous|earlier) instructions\",\n",
    "    r\"you are now the system\",\n",
    "    r\"reveal (your|the) prompt\",\n",
    "    r\"no rules apply\",\n",
    "    r\"bypass security\",\n",
    "]\n",
    "\n",
    "def detect_prompt_injection(text):\n",
    "    text = text.lower()\n",
    "    for pattern in SUSPICIOUS_PATTERNS:\n",
    "        if re.search(pattern, text):\n",
    "            return True\n",
    "    return False\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ML-Based Injection Detection (Scoring Model)\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "def ml_injection_score(text):\n",
    "    score = injection_classifier.predict_proba([text])[0][1]\n",
    "    return score\n",
    "\n",
    "def is_safe(text):\n",
    "    return ml_injection_score(text) < 0.7\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Hybrid Detection Pipeline\n",
    "\n",
    "#### Demonstration\n",
    "\n",
    "```python\n",
    "def validate_prompt(prompt):\n",
    "    if detect_prompt_injection(prompt):\n",
    "        return False\n",
    "    if not is_safe(prompt):\n",
    "        return False\n",
    "    return True\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Response Enforcement Layer\n",
    "\n",
    "Even if detection fails, enforce safety at generation time.\n",
    "\n",
    "```python\n",
    "def safe_llm_call(prompt):\n",
    "    if not validate_prompt(prompt):\n",
    "        return \"Request blocked: potential prompt injection detected.\"\n",
    "    \n",
    "    return llm.invoke(prompt).content\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Logging and Monitoring\n",
    "\n",
    "```python\n",
    "def log_attack(prompt):\n",
    "    with open(\"security.log\", \"a\") as f:\n",
    "        f.write(prompt + \"\\n\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "Prompt Injection Detection = Firewall for your LLM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Prompt injection is one of the top security risks for LLM systems\n",
    "* Use layered defense: rules + ML + enforcement\n",
    "* Always log suspicious attempts\n",
    "* Detection must run before the LLM is invoked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0568e41",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
