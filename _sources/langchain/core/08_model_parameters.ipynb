{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "113ccd42",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Model Parameters\n",
    "\n",
    "**Model parameters** are **runtime controls** that influence *how* an LLM generates responses.\n",
    "They do **not change the model weights**—only the **generation behavior**.\n",
    "\n",
    "In LangChain, these parameters are passed to the **ChatModel / LLM abstraction** and are **provider-agnostic** (as much as possible).\n",
    "\n",
    "---\n",
    "\n",
    "### Why Model Parameters Matter\n",
    "\n",
    "Model parameters control:\n",
    "\n",
    "* Creativity vs determinism\n",
    "* Factuality vs diversity\n",
    "* Latency and cost\n",
    "* Safety and verbosity\n",
    "\n",
    "Poor tuning leads to:\n",
    "\n",
    "* Hallucinations\n",
    "* Inconsistent answers\n",
    "* Higher cost\n",
    "* Slow responses\n",
    "\n",
    "---\n",
    "\n",
    "### Core Model Parameters (Most Important)\n",
    "\n",
    "#### temperature\n",
    "\n",
    "Controls **randomness** of token selection.\n",
    "\n",
    "**Behavior**\n",
    "\n",
    "* `0.0` → deterministic, factual\n",
    "* `0.7` → balanced\n",
    "* `>1.0` → creative, risky\n",
    "\n",
    "**Demonstration**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "llm.invoke(\"Explain RAG\").content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec63db",
   "metadata": {},
   "source": [
    "\n",
    "**Rule of thumb**\n",
    "\n",
    "* RAG / classification → `0.0–0.2`\n",
    "* Chat / ideation → `0.6–0.9`\n",
    "\n",
    "---\n",
    "\n",
    "### top_p (Nucleus Sampling)\n",
    "\n",
    "Limits token selection to the **smallest probability mass**.\n",
    "\n",
    "**Behavior**\n",
    "\n",
    "* `top_p=1.0` → consider all tokens\n",
    "* `top_p=0.9` → consider only top 90% probability mass\n",
    "\n",
    "**Demonstration**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc66214",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "llm.invoke(\"Explain RAG\").content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f064647a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Important**\n",
    "\n",
    "* Use **either** `temperature` **or** `top_p`\n",
    "* Avoid tuning both aggressively\n",
    "\n",
    "---\n",
    "\n",
    "### max_tokens\n",
    "\n",
    "Limits the **length of the generated output**.\n",
    "\n",
    "#### Demonstration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03acc7d4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Controls:\n",
    "\n",
    "* Cost\n",
    "* Response verbosity\n",
    "* Latency\n",
    "\n",
    "---\n",
    "\n",
    "### Token Control Parameters\n",
    "\n",
    "#### stop\n",
    "\n",
    "Stops generation when a token or sequence is encountered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa152ad6",
   "metadata": {},
   "source": [
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    stop=[\"\\n\\n\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d03176",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Used in:\n",
    "\n",
    "* Structured outputs\n",
    "* Tool responses\n",
    "* Controlled generation\n",
    "\n",
    "---\n",
    "\n",
    "### frequency_penalty\n",
    "\n",
    "Reduces repetition of tokens already used.\n",
    "\n",
    "**Demonstration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    frequency_penalty=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524de545",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### presence_penalty\n",
    "\n",
    "Encourages **new topics**.\n",
    "\n",
    "**Demonstration**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11faad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    presence_penalty=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df1b60",
   "metadata": {},
   "source": [
    "\n",
    "### seed (Determinism)\n",
    "\n",
    "Ensures **repeatable outputs** (if provider supports it).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b239508",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b9d58",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### response_format (Structured Output)\n",
    "\n",
    "Used internally by LangChain for structured parsing.\n",
    "\n",
    "```python\n",
    "llm.with_structured_output(MySchema)\n",
    "```\n",
    "\n",
    "LangChain auto-manages this.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Parameters in LangChain Chains\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06edab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain = prompt | ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce8ac8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Parameters are bound to the **Runnable**, not the prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Parameters in Agents\n",
    "\n",
    "```python\n",
    "agent_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "```\n",
    "\n",
    "Best practice:\n",
    "\n",
    "* **Agents → temperature = 0**\n",
    "* Deterministic tool selection\n",
    "\n",
    "---\n",
    "\n",
    "## Model Parameters in RAG\n",
    "\n",
    "### Recommended RAG Settings\n",
    "\n",
    "```python\n",
    "ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=300\n",
    ")\n",
    "```\n",
    "\n",
    "Why:\n",
    "\n",
    "* Reduce hallucinations\n",
    "* Improve grounding on retrieved context\n",
    "\n",
    "---\n",
    "\n",
    "## Common Parameter Mistakes\n",
    "\n",
    "### High temperature in RAG\n",
    "\n",
    "❌ Causes hallucinations\n",
    "\n",
    "### Unlimited max_tokens\n",
    "\n",
    "❌ Increases cost\n",
    "\n",
    "### Tuning temperature + top_p together\n",
    "\n",
    "❌ Unpredictable behavior\n",
    "\n",
    "### Using defaults blindly\n",
    "\n",
    "❌ Suboptimal results\n",
    "\n",
    "---\n",
    "\n",
    "## Parameter Interaction Summary\n",
    "\n",
    "| Parameter         | Affects         | Typical Use        |\n",
    "| ----------------- | --------------- | ------------------ |\n",
    "| temperature       | Randomness      | Creativity control |\n",
    "| top_p             | Diversity       | Token filtering    |\n",
    "| max_tokens        | Length          | Cost & latency     |\n",
    "| stop              | Termination     | Structured output  |\n",
    "| frequency_penalty | Repetition      | Reduce loops       |\n",
    "| presence_penalty  | Topic diversity | Brainstorming      |\n",
    "\n",
    "---\n",
    "\n",
    "### Recommended Presets (Production)\n",
    "\n",
    "#### RAG / QA\n",
    "\n",
    "```python\n",
    "temperature=0.0\n",
    "top_p=1.0\n",
    "max_tokens=300\n",
    "```\n",
    "\n",
    "#### Chatbot\n",
    "\n",
    "```python\n",
    "temperature=0.7\n",
    "top_p=0.9\n",
    "max_tokens=500\n",
    "```\n",
    "\n",
    "#### Agents\n",
    "\n",
    "```python\n",
    "temperature=0.0\n",
    "max_tokens=200\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### How LangChain Helps with Parameters\n",
    "\n",
    "* Unified API across providers\n",
    "* Safe defaults\n",
    "* Retry & fallback support\n",
    "* Structured output enforcement\n",
    "* Centralized tuning\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “Model parameters in LangChain control generation behavior at runtime. They tune creativity, determinism, length, and cost without changing the model. Proper parameter tuning is critical for reliable RAG, agents, and production systems.”\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "* **Accuracy → temperature ↓**\n",
    "* **Creativity → temperature ↑**\n",
    "* **Production → explicit parameters**\n",
    "* **Never rely on defaults blindly**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
