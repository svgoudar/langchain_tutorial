{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2286880",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## LLM \n",
    "\n",
    "In **LangChain**, an LLM is:\n",
    "\n",
    "> A **pluggable, provider-agnostic text generation interface** that can be composed with prompts, tools, memory, and retrieval pipelines.\n",
    "\n",
    "LangChain **does not build models**.\n",
    "It **standardizes how you call them**.\n",
    "\n",
    "---\n",
    "\n",
    "### Two Core LLM Abstractions in LangChain\n",
    "\n",
    "#### A. `LLM` (Legacy / Text-only)\n",
    "\n",
    "* Input: `str`\n",
    "* Output: `str`\n",
    "* Example: older OpenAI completions\n",
    "\n",
    "#### B. `ChatModel` (Primary / Modern)\n",
    "\n",
    "* Input: list of messages\n",
    "* Output: AIMessage\n",
    "* Supports:\n",
    "\n",
    "  * Tool calling\n",
    "  * Function calling\n",
    "  * System messages\n",
    "  * Multi-turn chat\n",
    "\n",
    "**Most LangChain code today uses ChatModels**\n",
    "\n",
    "---\n",
    "\n",
    "### Mental Model\n",
    "\n",
    "```\n",
    "Prompt → LLM → Output\n",
    "Prompt + Tools → Agent → LLM → Tool → LLM → Output\n",
    "Prompt + Docs → Retriever → LLM → Answer\n",
    "```\n",
    "\n",
    "LLM is **just one node** in a larger graph.\n",
    "\n",
    "---\n",
    "\n",
    "### Basic LLM Usage (Chat Model)\n",
    "\n",
    "#### Example: OpenAI via LangChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de83a6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG, or Retrieval-Augmented Generation, is a machine learning approach that combines retrieval of relevant information from a knowledge base with generative models to produce more accurate and contextually relevant responses.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"Explain RAG in one sentence\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f5bde8",
   "metadata": {},
   "source": [
    "#### What LangChain adds\n",
    "\n",
    "* Unified API\n",
    "* Retry logic\n",
    "* Streaming\n",
    "* Async support\n",
    "* Easy swapping of providers\n",
    "\n",
    "---\n",
    "\n",
    "### Prompt Templates + LLM (LangChain Style)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3e1eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed to simplify the development of applications that utilize large language models (LLMs). It provides tools and components to help developers build applications that can leverage the capabilities of LLMs for various tasks, such as natural language understanding, text generation, and conversational agents.\n",
      "\n",
      "Key features of LangChain include:\n",
      "\n",
      "1. **Modularity**: LangChain is built with a modular architecture, allowing developers to mix and match components according to their needs. This includes various modules for handling prompts, memory, and chains of operations.\n",
      "\n",
      "2. **Chains**: The framework allows for the creation of \"chains,\" which are sequences of operations that can be executed in order. This is useful for building complex workflows that involve multiple steps or interactions with the language model.\n",
      "\n",
      "3. **Memory**: LangChain supports memory management, enabling applications to maintain context over multiple interactions. This is particularly useful for conversational agents that need to remember previous exchanges.\n",
      "\n",
      "4. **Integrations**: LangChain can integrate with\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an IT support assistant\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "result = chain.invoke({\"question\": \"What is LangChain?\"})\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b981b69",
   "metadata": {},
   "source": [
    "\n",
    "Key idea:\n",
    "\n",
    "* Prompts are **first-class objects**\n",
    "* LLM is **composable**\n",
    "\n",
    "---\n",
    "\n",
    "### LCEL (LangChain Expression Language)\n",
    "\n",
    "LCEL is how LangChain **wires LLMs into pipelines**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c3226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"question\": lambda x: x[\"query\"]}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394d967",
   "metadata": {},
   "source": [
    "Benefits:\n",
    "\n",
    "* Declarative\n",
    "* Async by default\n",
    "* Streaming-ready\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd14726",
   "metadata": {},
   "source": [
    "\n",
    "### LLM with Structured Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb82e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category='Technical Issue' priority='High'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class TicketInfo(BaseModel):\n",
    "    category: str\n",
    "    priority: str\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(TicketInfo)\n",
    "\n",
    "result = structured_llm.invoke(\n",
    "    \"Email service is down for CEO\"\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dd3ab1",
   "metadata": {},
   "source": [
    "LangChain ensures:\n",
    "\n",
    "* JSON validity\n",
    "* Schema compliance\n",
    "* Retry on failure\n",
    "\n",
    "---\n",
    "\n",
    "### LLM with Tools (Key Differentiator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c0b0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ticket_count(source: str) -> int:\n",
    "    return 120\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([ticket_count])\n",
    "\n",
    "response = llm_with_tools.invoke(\n",
    "    \"How many tickets are there in Jira?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f73fd37",
   "metadata": {},
   "source": [
    "LangChain:\n",
    "\n",
    "* Converts function → JSON schema\n",
    "* Parses tool calls\n",
    "* Routes execution\n",
    "\n",
    "---\n",
    "\n",
    "### LLM in RAG (Retriever + LLM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b69f57d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_classic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrieval_qa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[32m      2\u001b[39m qa = RetrievalQA.from_chain_type(\n\u001b[32m      3\u001b[39m     llm=llm,\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     retriever=\u001b[43mvectorstore\u001b[49m.as_retriever()\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m qa.invoke(\u001b[33m\"\u001b[39m\u001b[33mWhat is our password policy?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "qa.invoke(\"What is our password policy?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a938b",
   "metadata": {},
   "source": [
    "\n",
    "LLM role:\n",
    "\n",
    "* Read retrieved chunks\n",
    "* Synthesize answer\n",
    "* Cite sources (if enabled)\n",
    "\n",
    "---\n",
    "\n",
    "### Streaming Tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f388d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed to facilitate the development of applications that leverage large language models (LLMs). It provides a structured way to build applications that can utilize LLMs for various tasks, such as natural language understanding, text generation, and more. The framework is particularly useful for developers looking to create complex applications that require the integration of LLMs with other components, such as databases, APIs, and user interfaces.\n",
      "\n",
      "### Key Features of LangChain:\n",
      "\n",
      "1. **Modular Components**: LangChain is built around modular components that can be easily combined and reused. This allows developers to create custom workflows tailored to their specific needs.\n",
      "\n",
      "2. **Chains**: At the core of LangChain are \"chains,\" which are sequences of operations that can include prompts, model calls, and other processing steps. Chains can be simple or complex, depending on the application's requirements.\n",
      "\n",
      "3. **Agents**: LangChain supports the concept of agents, which can make decisions based on user input and dynamically choose"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"Explain LangChain\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ab8ac",
   "metadata": {},
   "source": [
    "Useful for:\n",
    "\n",
    "* Chat UIs\n",
    "* SSE / WebSocket\n",
    "\n",
    "---\n",
    "\n",
    "### LLM Lifecycle in LangChain\n",
    "\n",
    "```\n",
    "Prompt\n",
    " → Validation\n",
    " → Model Call\n",
    " → Retry / Backoff\n",
    " → Output Parsing\n",
    " → Callbacks / Tracing\n",
    "```\n",
    "\n",
    "You get all of this **without writing glue code**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why LangChain Uses LLM Abstractions\n",
    "\n",
    "| Problem           | LangChain Solution    |\n",
    "| ----------------- | --------------------- |\n",
    "| Provider lock-in  | Unified LLM interface |\n",
    "| Prompt reuse      | Prompt templates      |\n",
    "| Complex workflows | LCEL                  |\n",
    "| Tool calling      | Automatic schema      |\n",
    "| Streaming         | Built-in              |\n",
    "| Production safety | Output parsing        |\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Summary\n",
    "\n",
    "> “In LangChain, an LLM is a composable runtime component, not a standalone model. It is wired into prompts, retrievers, tools, and agents using LCEL to build deterministic, production-grade pipelines.”\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "| Use Case          | LangChain LLM Pattern      |\n",
    "| ----------------- | -------------------------- |\n",
    "| Simple Q&A        | Prompt → LLM               |\n",
    "| Chatbot           | ChatModel                  |\n",
    "| RAG               | Retriever → LLM            |\n",
    "| Automation        | Agent + Tools              |\n",
    "| Structured output | `with_structured_output()` |\n",
    "| Streaming UI      | `stream()`                 |\n",
    "\n",
    "\n",
    "### LLM v/s ChatModel\n",
    "\n",
    "> **`LLM` is a text-completion interface.\n",
    "> `ChatModel` is a message-based, tool-aware conversational interface.**\n",
    "\n",
    "LangChain treats them as **distinct abstractions**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Abstraction Level\n",
    "\n",
    "| Aspect           | LLM             | ChatModel           |\n",
    "| ---------------- | --------------- | ------------------- |\n",
    "| Input            | `str`           | `List[BaseMessage]` |\n",
    "| Output           | `str`           | `AIMessage`         |\n",
    "| Conversation     | ❌ No            | ✅ Yes               |\n",
    "| System messages  | ❌ No            | ✅ Yes               |\n",
    "| Tool calling     | ❌ No            | ✅ Yes               |\n",
    "| Function calling | ❌ No            | ✅ Yes               |\n",
    "| Agents           | ❌ Not supported | ✅ Required          |\n",
    "| Streaming        | Limited         | Full                |\n",
    "| Future-proof     | ❌ Legacy        | ✅ Primary           |\n",
    "\n",
    "**LangChain recommends ChatModel for all new systems.**\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Model\n",
    "\n",
    "#### LLM\n",
    "\n",
    "```\n",
    "\"text in\" → model → \"text out\"\n",
    "```\n",
    "\n",
    "#### ChatModel\n",
    "\n",
    "```\n",
    "[System, Human, AI] → model → AIMessage\n",
    "                      ↳ tool_calls\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### LLM (Text Completion) – Demonstration\n",
    "\n",
    "> **Legacy / compatibility abstraction**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"Explain RAG in one sentence\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2079be8",
   "metadata": {},
   "source": [
    "\n",
    "### Limitations\n",
    "\n",
    "* No system role\n",
    "* No memory\n",
    "* No tools\n",
    "* No agents\n",
    "* No structured output\n",
    "\n",
    "Used only for:\n",
    "\n",
    "* Migration\n",
    "* Simple batch tasks\n",
    "* Non-chat legacy models\n",
    "\n",
    "---\n",
    "\n",
    "### ChatModel – Demonstration\n",
    "\n",
    "> **Primary LangChain abstraction**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da1c5c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG, or Retrieval-Augmented Generation, is a machine learning approach that combines retrieval of relevant information from a knowledge base with generative models to produce more accurate and contextually relevant responses.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "response = chat.invoke(\"Explain RAG in one sentence\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01faf4",
   "metadata": {},
   "source": [
    "### What you gain immediately\n",
    "\n",
    "* Message roles\n",
    "* Tool calling\n",
    "* Function schemas\n",
    "* Streaming\n",
    "* Agents\n",
    "* Multi-turn chat\n",
    "\n",
    "---\n",
    "\n",
    "### Message Objects\n",
    "\n",
    "ChatModels operate on **messages**, not strings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d1bad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an IT support assistant\"),\n",
    "    HumanMessage(content=\"Email is down\")\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34191b8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This enables:\n",
    "\n",
    "* Instruction hierarchy\n",
    "* Context control\n",
    "* Safety rules\n",
    "\n",
    "---\n",
    "\n",
    "### Tool Calling (ChatModel-only)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0359f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'ticket_count', 'args': {'source': 'Jira'}, 'id': 'call_PdonKV34RhsI1eJZxSkrES4h', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ticket_count(source: str) -> int:\n",
    "    return 120\n",
    "\n",
    "chat_with_tools = chat.bind_tools([ticket_count])\n",
    "\n",
    "response = chat_with_tools.invoke(\n",
    "    \"How many tickets are there in Jira?\"\n",
    ")\n",
    "\n",
    "print(response.tool_calls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45affc6f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "LLMs **cannot do this**.\n",
    "\n",
    "---\n",
    "\n",
    "### Structured Output (ChatModel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd19a69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category='Technical Support' priority='High'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Ticket(BaseModel):\n",
    "    category: str\n",
    "    priority: str\n",
    "\n",
    "structured_chat = chat.with_structured_output(Ticket)\n",
    "\n",
    "result = structured_chat.invoke(\"CEO cannot login to VPN\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0fd40",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "LangChain:\n",
    "\n",
    "* Generates schema\n",
    "* Validates output\n",
    "* Retries on failure\n",
    "\n",
    "---\n",
    "\n",
    "### Agents Require ChatModels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "166fe14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticket_count(source: str) -> int:\n",
    "    return 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00c9b377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ticket_count` with `Jira`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m120\u001b[0m\u001b[32;1m\u001b[1;3mThere are 120 tickets in Jira.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "There are 120 tickets in Jira.\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.agents import (\n",
    "    AgentExecutor,\n",
    "    create_openai_tools_agent,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "# Define the tool\n",
    "def ticket_count(source: str) -> int:\n",
    "    \"\"\"Returns the number of tickets in the system.\"\"\"\n",
    "    return 120\n",
    "\n",
    "ticket_tool = Tool(\n",
    "    name=\"ticket_count\",\n",
    "    func=ticket_count,\n",
    "    description=\"Counts the number of tickets in the system.\"\n",
    ")\n",
    "\n",
    "# Initialize the Chat Model\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant. Use tools if needed.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"assistant\", \"{agent_scratchpad}\")\n",
    "])\n",
    "\n",
    "# Create the agent\n",
    "agent = create_openai_tools_agent(\n",
    "    llm=chat,\n",
    "    tools=[ticket_tool],\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Create the executor\n",
    "executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=[ticket_tool],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Invoke the agent\n",
    "response = executor.invoke(\n",
    "    {\"input\": \"How many tickets are there in Jira?\"}\n",
    ")\n",
    "\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d505bb7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Agents will not work with LLMs.**\n",
    "\n",
    "---\n",
    "\n",
    "###  Streaming Comparison\n",
    "\n",
    "**LLM (limited)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54d398eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object BaseChatModel.stream at 0x000002988EAC6020>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.stream(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b5ffc",
   "metadata": {},
   "source": [
    "### ChatModel (full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b72f09b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed to facilitate the development of applications that utilize large language models (LLMs). It provides a structured way to build applications that can leverage the capabilities of LLMs for various tasks, such as natural language understanding, text generation, and more. Here are some key components and features of LangChain:\n",
      "\n",
      "1. **Modular Design**: LangChain is built with a modular architecture, allowing developers to easily integrate different components such as LLMs, data sources, and tools. This modularity makes it easier to customize and extend applications.\n",
      "\n",
      "2. **Chains**: At the core of LangChain is the concept of \"chains,\" which are sequences of operations that can be executed in order. For example, a chain might involve taking user input, processing it with an LLM, and then returning a response. Chains can be simple or complex, depending on the application's requirements.\n",
      "\n",
      "3. **Agents**: LangChain supports the creation of agents that can make decisions based on user input and context. Agents can use LLMs to interpret commands and determine the best course of action, making them suitable for building conversational agents or chatbots.\n",
      "\n",
      "4. **Memory**: LangChain includes features for managing memory, allowing applications to maintain context over multiple interactions. This is particularly useful for conversational applications where maintaining context is crucial for providing relevant responses.\n",
      "\n",
      "5. **Integrations**: LangChain can integrate with various data sources, APIs, and tools, enabling developers to create applications that can pull in external information or perform specific tasks beyond just text generation.\n",
      "\n",
      "6. **Use Cases**: LangChain can be used for a wide range of applications, including chatbots, virtual assistants, content generation, question-answering systems, and more. Its flexibility allows developers to tailor solutions to specific needs.\n",
      "\n",
      "7. **Community and Ecosystem**: LangChain has an active community and ecosystem, with resources, documentation, and examples available to help developers get started and share their experiences.\n",
      "\n",
      "Overall, LangChain aims to simplify the process of building applications that harness the power of large language models, making it easier for developers to create sophisticated and interactive experiences."
     ]
    }
   ],
   "source": [
    "for chunk in chat.stream(\"Explain LangChain\"):\n",
    "    print(chunk.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e6baa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ChatModels support:\n",
    "\n",
    "* Token streaming\n",
    "* Tool streaming\n",
    "* Partial responses\n",
    "\n",
    "---\n",
    "\n",
    "- LCEL Compatibility\n",
    "\n",
    "| Feature   | LLM        | ChatModel |\n",
    "| --------- | ---------- | --------- |\n",
    "| Runnable  | ⚠️ Limited | ✅ Full    |\n",
    "| Parallel  | ❌          | ✅         |\n",
    "| Retry     | ❌          | ✅         |\n",
    "| Fallbacks | ❌          | ✅         |\n",
    "\n",
    "---\n",
    "\n",
    "### When Should You Use LLM?\n",
    "\n",
    "**Rare cases only**:\n",
    "\n",
    "* Old completion-only models\n",
    "* Static text generation\n",
    "* Offline batch processing\n",
    "* Migration support\n",
    "\n",
    "---\n",
    "\n",
    "### When Should You Use ChatModel?\n",
    "\n",
    "**Always, if:**\n",
    "\n",
    "* Chatbot\n",
    "* RAG\n",
    "* Tools\n",
    "* Agents\n",
    "* Streaming UI\n",
    "* Production systems\n",
    "\n",
    "---\n",
    "\n",
    "### Why LangChain Split Them\n",
    "\n",
    "LangChain reflects **model evolution**:\n",
    "\n",
    "| Era      | API Type        |\n",
    "| -------- | --------------- |\n",
    "| Pre-2023 | Text completion |\n",
    "| 2023+    | Chat / messages |\n",
    "| 2024+    | Tools + agents  |\n",
    "\n",
    "---\n",
    "\n",
    "**Interview-Ready Answer**\n",
    "\n",
    "> “LLM is a legacy text-completion abstraction. ChatModel is LangChain’s primary interface for modern, conversational, tool-aware, agentic LLM systems. All agent, RAG, and production workflows require ChatModels.”\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb3df23",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
