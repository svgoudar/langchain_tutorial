{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ccb73f",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Chat Model\n",
    "\n",
    "In **LangChain**, a **Chat Model** is:\n",
    "\n",
    "> A message-based LLM interface that accepts structured conversation turns (roles), supports tool/function calling, and enables agentic and RAG workflows.\n",
    "\n",
    "It is **not just “chat”** — it is the **core execution engine** for modern LLM systems.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Chat Models Exist\n",
    "\n",
    "Earlier LLM APIs:\n",
    "\n",
    "```\n",
    "\"text prompt\" → \"text completion\"\n",
    "```\n",
    "\n",
    "Modern LLM systems require:\n",
    "\n",
    "* Role separation\n",
    "* Tool calling\n",
    "* Multi-turn context\n",
    "* Structured outputs\n",
    "* Streaming\n",
    "\n",
    "Chat Models were introduced to support this.\n",
    "\n",
    "---\n",
    "\n",
    "### Chat Model Interface (Key Difference)\n",
    "\n",
    "#### Input\n",
    "\n",
    "```python\n",
    "List[BaseMessage]\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```python\n",
    "AIMessage(\n",
    "  content=\"answer\",\n",
    "  tool_calls=[...]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Message Types (Very Important)\n",
    "\n",
    "LangChain defines explicit message objects:\n",
    "\n",
    "| Message Type  | Purpose               |\n",
    "| ------------- | --------------------- |\n",
    "| SystemMessage | Global instructions   |\n",
    "| HumanMessage  | User input            |\n",
    "| AIMessage     | Model response        |\n",
    "| ToolMessage   | Tool execution result |\n",
    "\n",
    "This is what enables **control and safety**.\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Chat Model Usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5b9371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed to facilitate the development of applications that utilize large language models (LLMs). It provides a structured way to build applications that can leverage the capabilities of LLMs for various tasks, such as natural language understanding, text generation, and conversational agents. Here are some key components and features of LangChain:\n",
      "\n",
      "1. **Modular Design**: LangChain is built with a modular architecture, allowing developers to easily integrate different components such as language models, data sources, and tools. This modularity enables flexibility in designing applications tailored to specific use cases.\n",
      "\n",
      "2. **Chains**: One of the core concepts in LangChain is the idea of \"chains.\" A chain is a sequence of operations that can involve multiple steps, such as retrieving data, processing it with an LLM, and then returning a response. Chains can be simple or complex, depending on the application's requirements.\n",
      "\n",
      "3. **Agents**: LangChain supports the creation of agents that can make decisions based on user input and context. Agents can utilize LLMs to interpret user queries and determine the best course of action, such as fetching information from a database or generating a response.\n",
      "\n",
      "4. **Memory**: LangChain includes features for managing memory, allowing applications to maintain context over multiple interactions. This is particularly useful for conversational agents that need to remember previous exchanges to provide coherent and contextually relevant responses.\n",
      "\n",
      "5. **Integrations**: LangChain can integrate with various external tools and APIs, enabling developers to enhance their applications with additional functionalities, such as web scraping, database access, or third-party services.\n",
      "\n",
      "6. **Use Cases**: LangChain can be applied to a wide range of use cases, including chatbots, virtual assistants, content generation, data analysis, and more. Its flexibility makes it suitable for both simple and complex applications.\n",
      "\n",
      "7. **Community and Ecosystem**: LangChain has an active community and ecosystem, with resources such as documentation, tutorials, and examples that help developers get started and share their experiences.\n",
      "\n",
      "Overall, LangChain aims to simplify the process of building applications that harness the power of language models, making it easier for developers to create intelligent and interactive solutions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "response = chat.invoke(\"Explain LangChain\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f02e9e",
   "metadata": {},
   "source": [
    "\n",
    "Internally, LangChain converts the string into a `HumanMessage`.\n",
    "\n",
    "---\n",
    "\n",
    "### Explicit Message Control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0adfedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an IT support assistant\"),\n",
    "    HumanMessage(content=\"VPN is not working\")\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cea1ce7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This allows:\n",
    "\n",
    "* Role-based behavior\n",
    "* Priority ordering\n",
    "* Instruction hierarchy\n",
    "\n",
    "---\n",
    "\n",
    "### Tool Calling (Chat Models Only)\n",
    "\n",
    "Chat Models can **decide to call tools**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ticket_count(source: str) -> int:\n",
    "    return 120\n",
    "\n",
    "chat_with_tools = chat.bind_tools([ticket_count])\n",
    "\n",
    "response = chat_with_tools.invoke(\n",
    "    \"How many tickets are there in Jira?\"\n",
    ")\n",
    "\n",
    "print(response.tool_calls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69abde81",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "What happens:\n",
    "\n",
    "1. Model emits a tool call\n",
    "2. LangChain parses it\n",
    "3. Executor runs the tool\n",
    "4. Tool result is fed back\n",
    "\n",
    "---\n",
    "\n",
    "### Structured Output (JSON Enforcement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Ticket(BaseModel):\n",
    "    category: str\n",
    "    priority: str\n",
    "\n",
    "structured_chat = chat.with_structured_output(Ticket)\n",
    "\n",
    "result = structured_chat.invoke(\"CEO cannot access email\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a429e76",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Benefits:\n",
    "\n",
    "* No hallucinated formats\n",
    "* Type-safe output\n",
    "* Retry on validation failure\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Streaming Tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chat.stream(\"Explain Chat Models\"):\n",
    "    print(chunk.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489456b1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Used for:\n",
    "\n",
    "* Chat UIs\n",
    "* SSE\n",
    "* WebSockets\n",
    "\n",
    "---\n",
    "\n",
    "### Chat Models in RAG\n",
    "\n",
    "```python\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=retriever\n",
    ")\n",
    "```\n",
    "\n",
    "Chat Model role:\n",
    "\n",
    "* Read retrieved documents\n",
    "* Synthesize answer\n",
    "* Respect system instructions\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Chat Models in Agents\n",
    "\n",
    "Agents **require** Chat Models.\n",
    "\n",
    "```python\n",
    "agent = create_openai_tools_agent(\n",
    "    llm=chat,\n",
    "    tools=[ticket_count]\n",
    ")\n",
    "```\n",
    "\n",
    "LLMs (text completion) **cannot do this**.\n",
    "\n",
    "---\n",
    "\n",
    "### Lifecycle of a Chat Model Call\n",
    "\n",
    "```\n",
    "Messages\n",
    " → Prompt assembly\n",
    " → Tool schema injection\n",
    " → Model invocation\n",
    " → Tool call parsing\n",
    " → Output parsing\n",
    " → Callbacks / tracing\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why LangChain Uses Chat Models Everywhere\n",
    "\n",
    "| Requirement       | Chat Model |\n",
    "| ----------------- | ---------- |\n",
    "| Multi-turn chat   | ✅          |\n",
    "| Tool calling      | ✅          |\n",
    "| Agents            | ✅          |\n",
    "| Structured output | ✅          |\n",
    "| Streaming         | ✅          |\n",
    "| Safety & control  | ✅          |\n",
    "\n",
    "---\n",
    "\n",
    "### When NOT to Use Chat Models\n",
    "\n",
    "Rare cases only:\n",
    "\n",
    "* Legacy completion-only models\n",
    "* Static batch text generation\n",
    "* Migration scenarios\n",
    "\n",
    "---\n",
    "\n",
    "### Chat Model Providers\n",
    "\n",
    "LangChain supports:\n",
    "\n",
    "* OpenAI (`ChatOpenAI`)\n",
    "* Anthropic (`ChatAnthropic`)\n",
    "* Ollama (`ChatOllama`)\n",
    "* Hugging Face (`ChatHuggingFace`)\n",
    "* Azure OpenAI\n",
    "\n",
    "Same interface, different backends.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "| Mistake                          | Result                |\n",
    "| -------------------------------- | --------------------- |\n",
    "| Using `LLM` instead of ChatModel | Limited functionality |\n",
    "| Passing raw strings everywhere   | Loss of role control  |\n",
    "| Ignoring system messages         | Unstable behavior     |\n",
    "| Not using tool calling           | Manual routing logic  |\n",
    "\n",
    "---\n",
    "\n",
    "**Interview-Ready Summary**\n",
    "\n",
    "> “Chat Models in LangChain are message-based, tool-aware LLM interfaces that enable agents, RAG, structured outputs, and streaming. They are the primary abstraction for building production-grade LLM systems.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a32d0e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
