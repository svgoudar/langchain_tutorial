{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e14312",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Output Parser\n",
    "\n",
    "In **LangChain**, an **Output Parser** is:\n",
    "\n",
    "> A component that **converts raw LLM output into a structured, validated Python object**.\n",
    "\n",
    "LLMs always return **text**.\n",
    "Applications need **data**.\n",
    "\n",
    "Output parsers bridge that gap.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Output Parsers Are Necessary\n",
    "\n",
    "Without parsers:\n",
    "\n",
    "* Free-form text\n",
    "* Fragile string parsing\n",
    "* Runtime bugs\n",
    "* No guarantees\n",
    "\n",
    "With parsers:\n",
    "\n",
    "* Typed outputs\n",
    "* Schema validation\n",
    "* Automatic retries\n",
    "* Production safety\n",
    "\n",
    "---\n",
    "\n",
    "### Where Output Parsers Sit in the Pipeline\n",
    "\n",
    "```\n",
    "Prompt\n",
    "  ↓\n",
    "LLM\n",
    "  ↓\n",
    "Raw Text\n",
    "  ↓\n",
    "Output Parser\n",
    "  ↓\n",
    "Typed Python Object\n",
    "```\n",
    "\n",
    "In LCEL:\n",
    "\n",
    "```python\n",
    "prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Core Output Parser Types in LangChain\n",
    "\n",
    "| Parser                           | Purpose                 |\n",
    "| -------------------------------- | ----------------------- |\n",
    "| `StrOutputParser`                | Plain text              |\n",
    "| `JsonOutputParser`               | JSON output             |\n",
    "| `PydanticOutputParser`           | Typed schema            |\n",
    "| `EnumOutputParser`               | Controlled labels       |\n",
    "| `CommaSeparatedListOutputParser` | Lists                   |\n",
    "| `OutputFixingParser`             | Auto-repair bad outputs |\n",
    "| `StructuredOutputParser`         | JSON schema (legacy)    |\n",
    "\n",
    "---\n",
    "\n",
    "###  `StrOutputParser` (Default / Simplest)\n",
    "\n",
    "Used when you just want text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46148aa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'text'}.  Expected: ['text'] Received: ['input']\\nNote: if you intended {text} to be part of the string and not a variable, please escape it with double curly braces like: '{{text}}'.\\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m parser = StrOutputParser()\n\u001b[32m      5\u001b[39m chain = prompt | llm | parser\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m result = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExplain RAG\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sangouda\\Python3.1210\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3149\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3147\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3149\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3150\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3151\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sangouda\\Python3.1210\\Lib\\site-packages\\langchain_core\\prompts\\base.py:217\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    216\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sangouda\\Python3.1210\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2058\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2054\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2056\u001b[39m         output = cast(\n\u001b[32m   2057\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2058\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2059\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2060\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2061\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2062\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2066\u001b[39m         )\n\u001b[32m   2067\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2068\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sangouda\\Python3.1210\\Lib\\site-packages\\langchain_core\\runnables\\config.py:433\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    432\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sangouda\\Python3.1210\\Lib\\site-packages\\langchain_core\\prompts\\base.py:190\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     inner_input_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**inner_input_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sangouda\\Python3.1210\\Lib\\site-packages\\langchain_core\\prompts\\base.py:184\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    178\u001b[39m     example_key = missing.pop()\n\u001b[32m    179\u001b[39m     msg += (\n\u001b[32m    180\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m to be part of the string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    185\u001b[39m         create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n\u001b[32m    186\u001b[39m     )\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[31mKeyError\u001b[39m: \"Input to PromptTemplate is missing variables {'text'}.  Expected: ['text'] Received: ['input']\\nNote: if you intended {text} to be part of the string and not a variable, please escape it with double curly braces like: '{{text}}'.\\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/INVALID_PROMPT_INPUT \""
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({\"input\": \"Explain RAG\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa20308b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "```text\n",
    "\"Retrieval-Augmented Generation combines search with generation.\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `PydanticOutputParser` (Most Important)\n",
    "\n",
    "#### Define a schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8843d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Ticket(BaseModel):\n",
    "    category: str = Field(description=\"Issue category\")\n",
    "    priority: str = Field(description=\"High, Medium, or Low\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eeb0f4",
   "metadata": {},
   "source": [
    "#### Create parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b178b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Ticket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb198f2",
   "metadata": {},
   "source": [
    "\n",
    "#### Use with prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb8216",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category='VPN' priority='High'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract structured ticket info.\"),\n",
    "    (\"human\", \"{input}\\n\\n{format_instructions}\")\n",
    "])\n",
    "\n",
    "chain = prompt | ChatOpenAI(temperature=0.2) | parser\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"input\": \"CEO cannot access VPN\",\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9576a4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `with_structured_output()` (Modern Shortcut)\n",
    "\n",
    "This is the **recommended approach**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdabfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sangouda\\Python3.1210\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:2067: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category='Email' priority='High'\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.2)\n",
    "structured_llm = llm.with_structured_output(Ticket)\n",
    "\n",
    "result = structured_llm.invoke(\n",
    "    \"Email service down for finance team\"\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb927f",
   "metadata": {},
   "source": [
    "What LangChain does internally:\n",
    "\n",
    "* Generates JSON schema\n",
    "* Injects format instructions\n",
    "* Parses + validates\n",
    "* Retries on failure\n",
    "\n",
    "---\n",
    "\n",
    "### JSON Output Parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543f774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acronym': 'RAG',\n",
       " 'meaning': 'Red, Amber, Green',\n",
       " 'description': 'RAG stands for Red, Amber, Green and is a color-coded system used to indicate the status or health of a project, task, or situation. Red typically signifies a critical or at-risk status, amber indicates a warning or potential issues, and green represents a good or on-track status.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"input\": \"Explain RAG  \",\"format_instructions\": parser.get_format_instructions()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac9e0e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Use when:\n",
    "\n",
    "* Schema is dynamic\n",
    "* No strict typing needed\n",
    "\n",
    "---\n",
    "\n",
    "### Enum Output Parser (Classification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57670deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Severity.HIGH: 'high'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.output_parsers.enum import EnumOutputParser\n",
    "from enum import Enum\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "class Severity(Enum):\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "\n",
    "parser = EnumOutputParser(enum=Severity)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Classify severity: {text}\\n{format_instructions}\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"text\": \"Database is down\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c2ce4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### OutputFixingParser (Auto-Repair)\n",
    "\n",
    "Used when models sometimes return invalid JSON.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.output_parsers.fix import OutputFixingParser\n",
    "\n",
    "fixing_parser = OutputFixingParser.from_llm(\n",
    "    llm=llm,\n",
    "    parser=parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6dca54",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "LangChain will:\n",
    "\n",
    "1. Detect parse failure\n",
    "2. Re-prompt the LLM\n",
    "3. Fix output automatically\n",
    "\n",
    "---\n",
    "\n",
    "### Output Parsers in Agents\n",
    "\n",
    "Agents **do not expose raw parsers** directly.\n",
    "\n",
    "Instead:\n",
    "\n",
    "* Tool outputs are parsed\n",
    "* Final agent output is text\n",
    "* Structured parsing is applied **after agent execution**\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "executor.invoke(...)\n",
    "result[\"output\"]\n",
    "```\n",
    "\n",
    "If you need structured agent output → wrap with `with_structured_output()` or post-parse.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Mistakes (Critical)\n",
    "\n",
    "| Mistake                             | Result          |\n",
    "| ----------------------------------- | --------------- |\n",
    "| Forgetting `format_instructions`    | Invalid output  |\n",
    "| Manual JSON parsing                 | Fragile         |\n",
    "| Parsing chain-of-thought            | Security risk   |\n",
    "| Using legacy StructuredOutputParser | Not recommended |\n",
    "| Skipping validation                 | Runtime bugs    |\n",
    "\n",
    "---\n",
    "\n",
    "### Security & Safety Note\n",
    "\n",
    "LangChain **does NOT expose chain-of-thought**.\n",
    "Output parsers extract **answers only**, which is production-safe.\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Which Parser\n",
    "\n",
    "| Use Case              | Parser                     |\n",
    "| --------------------- | -------------------------- |\n",
    "| Free-form chat        | `StrOutputParser`          |\n",
    "| Typed API response    | `PydanticOutputParser`     |\n",
    "| Classification        | `EnumOutputParser`         |\n",
    "| Unstable model output | `OutputFixingParser`       |\n",
    "| Modern systems        | `with_structured_output()` |\n",
    "\n",
    "---\n",
    "\n",
    "**Interview-Ready Summary**\n",
    "\n",
    "> “Output parsers in LangChain convert raw LLM text into validated, typed Python objects. They are essential for production systems because they enforce schemas, reduce hallucinations, and enable deterministic downstream processing.”\n",
    "\n",
    "---\n",
    "\n",
    "**Rule of Thumb**\n",
    "\n",
    "* **UI → text**\n",
    "* **API / agents / automation → structured output**\n",
    "* **Production → always parse**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
