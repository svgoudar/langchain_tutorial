

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>LLM &#8212; Machine Learning Handbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/js/hoverxref.js"></script>
    <script src="../../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'langchain/core/01_llms';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Chat Model" href="02_chat_models.html" />
    <link rel="prev" title="LangChain Core" href="overview.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Machine Learning Handbook - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Machine Learning Handbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    LangChain Tutorial
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="overview.html">LangChain Core</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">LLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_chat_models.html">Chat Model</a></li>


<li class="toctree-l2"><a class="reference internal" href="03_model_providers.html">Model Provider</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_prompt_templates.html">Prompt Template</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_output_parsers.html">Output Parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_runnable_interface.html">Runnable Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_lcel.html">LangChain Expression Language (LCEL)</a></li>

<li class="toctree-l2"><a class="reference internal" href="08_model_parameters.html">Model Parameters</a></li>



</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../prompt_engineering/overview.html">Prompt Engineering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../prompt_engineering/01_prompt_template.html">PromptTemplate (LangChain)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../prompt_engineering/02_chat_prompt_template.html">ChatPromptTemplate (LangChain)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../prompt_engineering/03_system_human_ai_messages.html">System / Human / AI Messages (LangChain)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../prompt_engineering/04_few_shot_prompting.html">Few-shot Prompting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../prompt_engineering/05_chain_of_thought.html">Chain-of-Thought (CoT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../prompt_engineering/06_self_consistency.html">Self-Consistency (LangChain Perspective)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../prompt_engineering/07_instruction_prompting.html">Instruction Tuning (LangChain &amp; LLM Perspective)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../prompt_engineering/08_structured_output_prompts.html">Structured Output Prompts (LangChain Perspective)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../prompt_engineering/09_guardrails.html">Guardrails (LangChain Perspective)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chains/overview.html">LangChain Chains</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chains/01_llmchain.html">LLMChain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chains/02_sequential_chain.html">SequentialChain (LangChain)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chains/03_simple_sequential_chain.html">SimpleSequentialChain (LangChain)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chains/04_router_chain.html">Router Chain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chains/05_map_reduce_chains.html">MapReduce Chain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chains/06_refine_chain.html">Refine Chain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chains/07_stuff_chain.html">Stuff Chain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chains/08_transform_chain.html">TransformChain (LangChain)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chains/09_retrievalqa_chain.html">RetrievalQA Chain (LangChain)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chains/10_conversational_retrieval_chain.html">ConversationalRetrievalChain</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../rag/overview.html">Retrieval-Augmented Generation (RAG)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../rag/01_document_loaders.html">Document Loader</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/02_text_splitters.html">Text Splitter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/03_chunk_overlap.html">Chunk Overlap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/04_embeddings.html">Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/05_vector_stores.html">Vector Stores</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/06_similarity_search.html">Similarity Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/07_mmr.html">Maximal Marginal Relevance (MMR)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/08_hybrid_search.html">Hybrid Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/09_reranking.html">Reranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/10_context_window_management.html">Context Window Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/11_source_attribution.html">Source Attribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/12_hallucination_prevention.html">Hallucination Prevention and Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rag/13_types_of_rag.html">Types of RAG (Retrieval-Augmented Generation)</a></li>














</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../agents/overview.html">LangChain Agents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../agents/01_tool_calling.html">Tool Calling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/02_react_agents.html">ReAct Agent (Reason + Act)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/03_openai_tools_agent.html">OpenAI Tools Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/04_conversational_agents.html">Conversational Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/05_multi_tool_agents.html">Multi-Tool Agents</a></li>
















<li class="toctree-l2"><a class="reference internal" href="../agents/06_function_calling.html">Function Calling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/07_agent_scratchpad.html">agent_scratchpad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/08_agent_executor.html">AgentExecutor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/09_agent_memory.html">Agent Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../agents/10_planning_vs_execution_agents.html">Planning vs Execution Agents</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tools/overview.html">Tools</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../tools/01_custom_tools.html">Custom Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tools/02_structured_tools.html">Structured Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tools/03_tool_schemas.html">Tool Schema</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tools/04_api_tools.html">API Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tools/05_database_tools.html">Database Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tools/06_file_system_tools.html">File System Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tools/07_web_search_tools.html">Web Search Tools — Explanation with Demonstration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tools/08_mcp_tool_adapters.html">MCP Tool Adapters</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../memory/overview.html">Memory Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../memory/01_conversation_buffer_memory.html">ConversationBufferMemory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../memory/02_conversation_summary_memory.html">ConversationSummaryMemory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../memory/03_conversation_buffer_window_memory.html">ConversationBufferWindowMemory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../memory/04_vectorstore_backed_memory.html">VectorStore-Backed Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../memory/05_entity_memory.html">Entity Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../memory/06_session_based_memory.html">Session-Based Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../memory/07_long_term_vs_short_term_memory.html">Long-Term vs Short-Term Memory</a></li>



</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../lcel/overview.html">LangChain Expression Language (LCEL)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../lcel/01_runnable_passthrough.html">RunnablePassthrough</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lcel/02_runnable_lambda.html">RunnableLambda</a></li>

<li class="toctree-l2"><a class="reference internal" href="../lcel/03_runnable_parallel.html">RunnableParallel</a></li>

<li class="toctree-l2"><a class="reference internal" href="../lcel/04_runnable_sequence.html">RunnableSequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lcel/05_streaming_runnables.html">Streaming Runnables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lcel/06_batch_execution.html">Batch Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lcel/07_async_execution.html">Async Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lcel/08_retry_and_fallbacks.html">Retry and Fallback</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../streaming_callbacks/overview.html">Streaming and Callbacks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../streaming_callbacks/01_token_streaming.html">Token Streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../streaming_callbacks/02_callback_handlers.html">Callback Handlers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../streaming_callbacks/03_tracing.html">Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../streaming_callbacks/04_logging.html">Logging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../streaming_callbacks/05_observability_hooks.html">Observability Hooks</a></li>



<li class="toctree-l2"><a class="reference internal" href="../streaming_callbacks/06_cost_tracking.html">Cost Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../streaming_callbacks/07_latency_tracking.html">Latency Tracking</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../evaluation/overview.html">Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../evaluation/01_llm_evaluation.html">LLM Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../evaluation/02_rag_evaluation.html">RAG Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../evaluation/03_faithfulness.html">Faithfulness</a></li>
<li class="toctree-l2"><a class="reference internal" href="../evaluation/04_relevance.html">Relevance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../evaluation/05_answer_correctness.html">Answer Correctness</a></li>
<li class="toctree-l2"><a class="reference internal" href="../evaluation/06_synthetic_data_generation.html">Synthetic Data Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../evaluation/07_regression_testing.html">Regression Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../evaluation/08_prompt_versioning.html">Regression Testing</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../data_ingestion/overview.html">Data Ingestion</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../data_ingestion/01_pdf_csv_html_loaders.html">Document Loaders — PDF / CSV / HTML</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_ingestion/02_web_scraping.html">Web Scraping</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_ingestion/03_api_based_loaders.html">API-Based Loader</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_ingestion/04_data_cleaning.html">Data Cleaning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_ingestion/05_metadata_enrichment.html">Metadata Enrichment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_ingestion/06_document_versioning.html">Document Versioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_ingestion/07_incremental_ingestion.html">Incremental Ingestion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_ingestion/08_cdc_style_updates.html">CDC (Change Data Capture) Style Updates</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../caching/overview.html">LangChain Caching</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../caching/01_prompt_caching.html">Prompt Caching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../caching/02_llm_response_caching.html">LLM Response Caching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../caching/03_embedding_caching.html">Embeddings Cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="../caching/04_redis_cache.html">Redis Cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="../caching/05_in_memory_cache.html">In-Memory Cache — Explanation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../caching/06_cache_invalidation.html">Cache Invalidation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../caching/07_rate_limiting.html">Rate Limiting</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../security/overview.html">Security</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../security/01_prompt_injection_detection.html">Prompt Injection Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../security/02_input_sanitization.html">Input Sanitization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../security/03_output_validation.html">Output Validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../security/04_content_filtering.html">Content Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../security/05_secrets_management.html">Secrets Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../security/06_role_based_access.html">Role-Based Access Control (RBAC)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../deployment/overview.html">Deployment</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../deployment/01_fastapi_integration.html">FastAPI Integration with LangChain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deployment/02_gradio_streamlit_ui.html">Gradio and Streamlit UI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deployment/03_background_tasks.html">Background Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deployment/04_async_workers.html">Async Workers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deployment/05_server_sent_events.html">Server-Sent Events (SSE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deployment/06_websockets.html">WebSockets —</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deployment/07_load_balancing.html">Load Balancing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deployment/08_horizontal_scaling.html">Horizontal Scaling</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../interoperability/overview.html">Interoperability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../interoperability/01_langgraph_integration.html">LangGraph Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../interoperability/02_llamaindex_integration.html">LlamaIndex Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../interoperability/03_mcp_integration.html">MCP (Model Context Protocol) Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../interoperability/04_external_vector_dbs.html">External Vector Database</a></li>
<li class="toctree-l2"><a class="reference internal" href="../interoperability/05_cloud_services.html">Cloud Services</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../production/overview.html">Production Best Practices</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../production/01_config_management.html">Configuration Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../production/02_environment_separation.html">Environment Separation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../production/03_cicd_for_prompts.html">Environment Separation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../production/04_versioned_pipelines.html">Versioned Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="../production/05_rollbacks.html">Rollbacks in LLM Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../production/06_monitoring_and_alerts.html">Monitoring and Alerts — Detailed Explanation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../production/07_cost_optimization.html">Cost Optimization — Detailed Explanation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../metrics/overview.html">Metrics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../metrics/01_intrinsic_metrics.html">Intrinsic (Model-Centric) Metrics</a></li>

















<li class="toctree-l2"><a class="reference internal" href="../metrics/02_task_metrics.html">Task &amp; Quality Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../metrics/03_RAG_metrics.html">RAG and Knowledge Grounding Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../metrics/04_safety_metrics.html">Safety &amp; Alignment Metrics</a></li>

<li class="toctree-l2"><a class="reference internal" href="../metrics/05_perf_metrics.html">Performance Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../metrics/06_cost_effeciency_metrics.html">Cost &amp; Efficiency Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../metrics/07_reliability_stability_metrics.html">Reliability &amp; Stability Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../metrics/08_user_experience_metrics.html">User Experience Metrics</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../advanced/overview.html">Advanced LangChain Patterns</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/01_agentic_rag.html">Agentic RAG — Detailed Explanation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/02_context_aware_rag.html">Context-Aware RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/03_hierarchical_agents.html">Hierarchical Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/04_self_reflection.html">Self-Reflection in LLM Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/05_tool_reranking.html">Tool Re-Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/06_online_learning_from_feedback.html">Online Learning from Feedback</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/07_multi_modal_chains.html">Multi-Modal Chain — Brief Explanation</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/svgoudar/langchain_tutorial/blob/master/langchain/core/01_llms.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/svgoudar/langchain_tutorial" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/svgoudar/langchain_tutorial/issues/new?title=Issue%20on%20page%20%2Flangchain/core/01_llms.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/langchain/core/01_llms.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LLM</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-core-llm-abstractions-in-langchain">Two Core LLM Abstractions in LangChain</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-llm-legacy-text-only">A. <code class="docutils literal notranslate"><span class="pre">LLM</span></code> (Legacy / Text-only)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-chatmodel-primary-modern">B. <code class="docutils literal notranslate"><span class="pre">ChatModel</span></code> (Primary / Modern)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mental-model">Mental Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-llm-usage-chat-model">Basic LLM Usage (Chat Model)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-openai-via-langchain">Example: OpenAI via LangChain</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-langchain-adds">What LangChain adds</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-templates-llm-langchain-style">Prompt Templates + LLM (LangChain Style)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lcel-langchain-expression-language">LCEL (LangChain Expression Language)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-with-structured-output">LLM with Structured Output</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-with-tools-key-differentiator">LLM with Tools (Key Differentiator)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-in-rag-retriever-llm">LLM in RAG (Retriever + LLM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#streaming-tokens">Streaming Tokens</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-lifecycle-in-langchain">LLM Lifecycle in LangChain</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-langchain-uses-llm-abstractions">Why LangChain Uses LLM Abstractions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interview-ready-summary">Interview-Ready Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-what">When to Use What</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-v-s-chatmodel">LLM v/s ChatModel</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#abstraction-level">Abstraction Level</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conceptual-model">Conceptual Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">LLM</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chatmodel">ChatModel</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-text-completion-demonstration">LLM (Text Completion) – Demonstration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations">Limitations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chatmodel-demonstration">ChatModel – Demonstration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-you-gain-immediately">What you gain immediately</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#message-objects">Message Objects</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tool-calling-chatmodel-only">Tool Calling (ChatModel-only)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#structured-output-chatmodel">Structured Output (ChatModel)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#agents-require-chatmodels">Agents Require ChatModels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#streaming-comparison">Streaming Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chatmodel-full">ChatModel (full)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-should-you-use-llm">When Should You Use LLM?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-should-you-use-chatmodel">When Should You Use ChatModel?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-langchain-split-them">Why LangChain Split Them</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#llm" id="id2">LLM</a></p>
<ul>
<li><p><a class="reference internal" href="#two-core-llm-abstractions-in-langchain" id="id3">Two Core LLM Abstractions in LangChain</a></p>
<ul>
<li><p><a class="reference internal" href="#a-llm-legacy-text-only" id="id4">A. <code class="docutils literal notranslate"><span class="pre">LLM</span></code> (Legacy / Text-only)</a></p></li>
<li><p><a class="reference internal" href="#b-chatmodel-primary-modern" id="id5">B. <code class="docutils literal notranslate"><span class="pre">ChatModel</span></code> (Primary / Modern)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#mental-model" id="id6">Mental Model</a></p></li>
<li><p><a class="reference internal" href="#basic-llm-usage-chat-model" id="id7">Basic LLM Usage (Chat Model)</a></p>
<ul>
<li><p><a class="reference internal" href="#example-openai-via-langchain" id="id8">Example: OpenAI via LangChain</a></p></li>
<li><p><a class="reference internal" href="#what-langchain-adds" id="id9">What LangChain adds</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#prompt-templates-llm-langchain-style" id="id10">Prompt Templates + LLM (LangChain Style)</a></p></li>
<li><p><a class="reference internal" href="#lcel-langchain-expression-language" id="id11">LCEL (LangChain Expression Language)</a></p></li>
<li><p><a class="reference internal" href="#llm-with-structured-output" id="id12">LLM with Structured Output</a></p></li>
<li><p><a class="reference internal" href="#llm-with-tools-key-differentiator" id="id13">LLM with Tools (Key Differentiator)</a></p></li>
<li><p><a class="reference internal" href="#llm-in-rag-retriever-llm" id="id14">LLM in RAG (Retriever + LLM)</a></p></li>
<li><p><a class="reference internal" href="#streaming-tokens" id="id15">Streaming Tokens</a></p></li>
<li><p><a class="reference internal" href="#llm-lifecycle-in-langchain" id="id16">LLM Lifecycle in LangChain</a></p></li>
<li><p><a class="reference internal" href="#why-langchain-uses-llm-abstractions" id="id17">Why LangChain Uses LLM Abstractions</a></p></li>
<li><p><a class="reference internal" href="#interview-ready-summary" id="id18">Interview-Ready Summary</a></p></li>
<li><p><a class="reference internal" href="#when-to-use-what" id="id19">When to Use What</a></p></li>
<li><p><a class="reference internal" href="#llm-v-s-chatmodel" id="id20">LLM v/s ChatModel</a></p>
<ul>
<li><p><a class="reference internal" href="#abstraction-level" id="id21">Abstraction Level</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#conceptual-model" id="id22">Conceptual Model</a></p>
<ul>
<li><p><a class="reference internal" href="#id1" id="id23">LLM</a></p></li>
<li><p><a class="reference internal" href="#chatmodel" id="id24">ChatModel</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#llm-text-completion-demonstration" id="id25">LLM (Text Completion) – Demonstration</a></p></li>
<li><p><a class="reference internal" href="#limitations" id="id26">Limitations</a></p></li>
<li><p><a class="reference internal" href="#chatmodel-demonstration" id="id27">ChatModel – Demonstration</a></p></li>
<li><p><a class="reference internal" href="#what-you-gain-immediately" id="id28">What you gain immediately</a></p></li>
<li><p><a class="reference internal" href="#message-objects" id="id29">Message Objects</a></p></li>
<li><p><a class="reference internal" href="#tool-calling-chatmodel-only" id="id30">Tool Calling (ChatModel-only)</a></p></li>
<li><p><a class="reference internal" href="#structured-output-chatmodel" id="id31">Structured Output (ChatModel)</a></p></li>
<li><p><a class="reference internal" href="#agents-require-chatmodels" id="id32">Agents Require ChatModels</a></p></li>
<li><p><a class="reference internal" href="#streaming-comparison" id="id33">Streaming Comparison</a></p></li>
<li><p><a class="reference internal" href="#chatmodel-full" id="id34">ChatModel (full)</a></p></li>
<li><p><a class="reference internal" href="#when-should-you-use-llm" id="id35">When Should You Use LLM?</a></p></li>
<li><p><a class="reference internal" href="#when-should-you-use-chatmodel" id="id36">When Should You Use ChatModel?</a></p></li>
<li><p><a class="reference internal" href="#why-langchain-split-them" id="id37">Why LangChain Split Them</a></p></li>
</ul>
</li>
</ul>
</div>
<section id="llm">
<h1><a class="toc-backref" href="#id2">LLM</a><a class="headerlink" href="#llm" title="Permalink to this heading">#</a></h1>
<p>In <strong>LangChain</strong>, an LLM is:</p>
<blockquote>
<div><p>A <strong>pluggable, provider-agnostic text generation interface</strong> that can be composed with prompts, tools, memory, and retrieval pipelines.</p>
</div></blockquote>
<p>LangChain <strong>does not build models</strong>.
It <strong>standardizes how you call them</strong>.</p>
<hr class="docutils" />
<section id="two-core-llm-abstractions-in-langchain">
<h2><a class="toc-backref" href="#id3">Two Core LLM Abstractions in LangChain</a><a class="headerlink" href="#two-core-llm-abstractions-in-langchain" title="Permalink to this heading">#</a></h2>
<section id="a-llm-legacy-text-only">
<h3><a class="toc-backref" href="#id4">A. <code class="docutils literal notranslate"><span class="pre">LLM</span></code> (Legacy / Text-only)</a><a class="headerlink" href="#a-llm-legacy-text-only" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Input: <code class="docutils literal notranslate"><span class="pre">str</span></code></p></li>
<li><p>Output: <code class="docutils literal notranslate"><span class="pre">str</span></code></p></li>
<li><p>Example: older OpenAI completions</p></li>
</ul>
</section>
<section id="b-chatmodel-primary-modern">
<h3><a class="toc-backref" href="#id5">B. <code class="docutils literal notranslate"><span class="pre">ChatModel</span></code> (Primary / Modern)</a><a class="headerlink" href="#b-chatmodel-primary-modern" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Input: list of messages</p></li>
<li><p>Output: AIMessage</p></li>
<li><p>Supports:</p>
<ul>
<li><p>Tool calling</p></li>
<li><p>Function calling</p></li>
<li><p>System messages</p></li>
<li><p>Multi-turn chat</p></li>
</ul>
</li>
</ul>
<p><strong>Most LangChain code today uses ChatModels</strong></p>
</section>
</section>
<hr class="docutils" />
<section id="mental-model">
<h2><a class="toc-backref" href="#id6">Mental Model</a><a class="headerlink" href="#mental-model" title="Permalink to this heading">#</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Prompt → LLM → Output
Prompt + Tools → Agent → LLM → Tool → LLM → Output
Prompt + Docs → Retriever → LLM → Answer
</pre></div>
</div>
<p>LLM is <strong>just one node</strong> in a larger graph.</p>
</section>
<hr class="docutils" />
<section id="basic-llm-usage-chat-model">
<h2><a class="toc-backref" href="#id7">Basic LLM Usage (Chat Model)</a><a class="headerlink" href="#basic-llm-usage-chat-model" title="Permalink to this heading">#</a></h2>
<section id="example-openai-via-langchain">
<h3><a class="toc-backref" href="#id8">Example: OpenAI via LangChain</a><a class="headerlink" href="#example-openai-via-langchain" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">200</span>
<span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Explain RAG in one sentence&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RAG, or Retrieval-Augmented Generation, is a machine learning approach that combines retrieval of relevant information from a knowledge base with generative models to produce more accurate and contextually relevant responses.
</pre></div>
</div>
</div>
</div>
</section>
<section id="what-langchain-adds">
<h3><a class="toc-backref" href="#id9">What LangChain adds</a><a class="headerlink" href="#what-langchain-adds" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unified API</p></li>
<li><p>Retry logic</p></li>
<li><p>Streaming</p></li>
<li><p>Async support</p></li>
<li><p>Easy swapping of providers</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="prompt-templates-llm-langchain-style">
<h2><a class="toc-backref" href="#id10">Prompt Templates + LLM (LangChain Style)</a><a class="headerlink" href="#prompt-templates-llm-langchain-style" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatPromptTemplate</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;You are an IT support assistant&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{question}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is LangChain?&quot;</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LangChain is a framework designed to simplify the development of applications that utilize large language models (LLMs). It provides tools and components to help developers build applications that can leverage the capabilities of LLMs for various tasks, such as natural language understanding, text generation, and conversational agents.

Key features of LangChain include:

1. **Modularity**: LangChain is built with a modular architecture, allowing developers to mix and match components according to their needs. This includes various modules for handling prompts, memory, and chains of operations.

2. **Chains**: The framework allows for the creation of &quot;chains,&quot; which are sequences of operations that can be executed in order. This is useful for building complex workflows that involve multiple steps or interactions with the language model.

3. **Memory**: LangChain supports memory management, enabling applications to maintain context over multiple interactions. This is particularly useful for conversational agents that need to remember previous exchanges.

4. **Integrations**: LangChain can integrate with
</pre></div>
</div>
</div>
</div>
<p>Key idea:</p>
<ul class="simple">
<li><p>Prompts are <strong>first-class objects</strong></p></li>
<li><p>LLM is <strong>composable</strong></p></li>
</ul>
</section>
<hr class="docutils" />
<section id="lcel-langchain-expression-language">
<h2><a class="toc-backref" href="#id11">LCEL (LangChain Expression Language)</a><a class="headerlink" href="#lcel-langchain-expression-language" title="Permalink to this heading">#</a></h2>
<p>LCEL is how LangChain <strong>wires LLMs into pipelines</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">]}</span>
    <span class="o">|</span> <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">llm</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Benefits:</p>
<ul class="simple">
<li><p>Declarative</p></li>
<li><p>Async by default</p></li>
<li><p>Streaming-ready</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="llm-with-structured-output">
<h2><a class="toc-backref" href="#id12">LLM with Structured Output</a><a class="headerlink" href="#llm-with-structured-output" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TicketInfo</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">category</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">priority</span><span class="p">:</span> <span class="nb">str</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">)</span>

<span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">TicketInfo</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">structured_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="s2">&quot;Email service is down for CEO&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>category=&#39;Technical Issue&#39; priority=&#39;High&#39;
</pre></div>
</div>
</div>
</div>
<p>LangChain ensures:</p>
<ul class="simple">
<li><p>JSON validity</p></li>
<li><p>Schema compliance</p></li>
<li><p>Retry on failure</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="llm-with-tools-key-differentiator">
<h2><a class="toc-backref" href="#id13">LLM with Tools (Key Differentiator)</a><a class="headerlink" href="#llm-with-tools-key-differentiator" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">ticket_count</span><span class="p">(</span><span class="n">source</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">return</span> <span class="mi">120</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">)</span>

<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">([</span><span class="n">ticket_count</span><span class="p">])</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="s2">&quot;How many tickets are there in Jira?&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>LangChain:</p>
<ul class="simple">
<li><p>Converts function → JSON schema</p></li>
<li><p>Parses tool calls</p></li>
<li><p>Routes execution</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="llm-in-rag-retriever-llm">
<h2><a class="toc-backref" href="#id14">LLM in RAG (Retriever + LLM)</a><a class="headerlink" href="#llm-in-rag-retriever-llm" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_classic.chains.retrieval_qa.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">RetrievalQA</span>
<span class="n">qa</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_chain_type</span><span class="p">(</span>
    <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
    <span class="n">retriever</span><span class="o">=</span><span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">qa</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;What is our password policy?&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">7</span><span class="p">],</span> <span class="n">line</span> <span class="mi">4</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="kn">from</span><span class="w"> </span><span class="nn">langchain_classic.chains.retrieval_qa.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">RetrievalQA</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">qa</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_chain_type</span><span class="p">(</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span>     <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
<span class="ne">----&gt; </span><span class="mi">4</span>     <span class="n">retriever</span><span class="o">=</span><span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="n">qa</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;What is our password policy?&quot;</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;vectorstore&#39; is not defined
</pre></div>
</div>
</div>
</div>
<p>LLM role:</p>
<ul class="simple">
<li><p>Read retrieved chunks</p></li>
<li><p>Synthesize answer</p></li>
<li><p>Cite sources (if enabled)</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="streaming-tokens">
<h2><a class="toc-backref" href="#id15">Streaming Tokens</a><a class="headerlink" href="#streaming-tokens" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">llm</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="s2">&quot;Explain LangChain&quot;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LangChain is a framework designed to facilitate the development of applications that leverage large language models (LLMs). It provides a structured way to build applications that can utilize LLMs for various tasks, such as natural language understanding, text generation, and more. The framework is particularly useful for developers looking to create complex applications that require the integration of LLMs with other components, such as databases, APIs, and user interfaces.

### Key Features of LangChain:

1. **Modular Components**: LangChain is built around modular components that can be easily combined and reused. This allows developers to create custom workflows tailored to their specific needs.

2. **Chains**: At the core of LangChain are &quot;chains,&quot; which are sequences of operations that can include prompts, model calls, and other processing steps. Chains can be simple or complex, depending on the application&#39;s requirements.

3. **Agents**: LangChain supports the concept of agents, which can make decisions based on user input and dynamically choose
</pre></div>
</div>
</div>
</div>
<p>Useful for:</p>
<ul class="simple">
<li><p>Chat UIs</p></li>
<li><p>SSE / WebSocket</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="llm-lifecycle-in-langchain">
<h2><a class="toc-backref" href="#id16">LLM Lifecycle in LangChain</a><a class="headerlink" href="#llm-lifecycle-in-langchain" title="Permalink to this heading">#</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Prompt
 → Validation
 → Model Call
 → Retry / Backoff
 → Output Parsing
 → Callbacks / Tracing
</pre></div>
</div>
<p>You get all of this <strong>without writing glue code</strong>.</p>
</section>
<hr class="docutils" />
<section id="why-langchain-uses-llm-abstractions">
<h2><a class="toc-backref" href="#id17">Why LangChain Uses LLM Abstractions</a><a class="headerlink" href="#why-langchain-uses-llm-abstractions" title="Permalink to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Problem</p></th>
<th class="head"><p>LangChain Solution</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Provider lock-in</p></td>
<td><p>Unified LLM interface</p></td>
</tr>
<tr class="row-odd"><td><p>Prompt reuse</p></td>
<td><p>Prompt templates</p></td>
</tr>
<tr class="row-even"><td><p>Complex workflows</p></td>
<td><p>LCEL</p></td>
</tr>
<tr class="row-odd"><td><p>Tool calling</p></td>
<td><p>Automatic schema</p></td>
</tr>
<tr class="row-even"><td><p>Streaming</p></td>
<td><p>Built-in</p></td>
</tr>
<tr class="row-odd"><td><p>Production safety</p></td>
<td><p>Output parsing</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="interview-ready-summary">
<h2><a class="toc-backref" href="#id18">Interview-Ready Summary</a><a class="headerlink" href="#interview-ready-summary" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>“In LangChain, an LLM is a composable runtime component, not a standalone model. It is wired into prompts, retrievers, tools, and agents using LCEL to build deterministic, production-grade pipelines.”</p>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="when-to-use-what">
<h2><a class="toc-backref" href="#id19">When to Use What</a><a class="headerlink" href="#when-to-use-what" title="Permalink to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Use Case</p></th>
<th class="head"><p>LangChain LLM Pattern</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Simple Q&amp;A</p></td>
<td><p>Prompt → LLM</p></td>
</tr>
<tr class="row-odd"><td><p>Chatbot</p></td>
<td><p>ChatModel</p></td>
</tr>
<tr class="row-even"><td><p>RAG</p></td>
<td><p>Retriever → LLM</p></td>
</tr>
<tr class="row-odd"><td><p>Automation</p></td>
<td><p>Agent + Tools</p></td>
</tr>
<tr class="row-even"><td><p>Structured output</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">with_structured_output()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Streaming UI</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">stream()</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="llm-v-s-chatmodel">
<h2><a class="toc-backref" href="#id20">LLM v/s ChatModel</a><a class="headerlink" href="#llm-v-s-chatmodel" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p><strong><code class="docutils literal notranslate"><span class="pre">LLM</span></code> is a text-completion interface.
<code class="docutils literal notranslate"><span class="pre">ChatModel</span></code> is a message-based, tool-aware conversational interface.</strong></p>
</div></blockquote>
<p>LangChain treats them as <strong>distinct abstractions</strong>.</p>
<hr class="docutils" />
<section id="abstraction-level">
<h3><a class="toc-backref" href="#id21">Abstraction Level</a><a class="headerlink" href="#abstraction-level" title="Permalink to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>LLM</p></th>
<th class="head"><p>ChatModel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Input</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">List[BaseMessage]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Output</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">AIMessage</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Conversation</p></td>
<td><p>❌ No</p></td>
<td><p>✅ Yes</p></td>
</tr>
<tr class="row-odd"><td><p>System messages</p></td>
<td><p>❌ No</p></td>
<td><p>✅ Yes</p></td>
</tr>
<tr class="row-even"><td><p>Tool calling</p></td>
<td><p>❌ No</p></td>
<td><p>✅ Yes</p></td>
</tr>
<tr class="row-odd"><td><p>Function calling</p></td>
<td><p>❌ No</p></td>
<td><p>✅ Yes</p></td>
</tr>
<tr class="row-even"><td><p>Agents</p></td>
<td><p>❌ Not supported</p></td>
<td><p>✅ Required</p></td>
</tr>
<tr class="row-odd"><td><p>Streaming</p></td>
<td><p>Limited</p></td>
<td><p>Full</p></td>
</tr>
<tr class="row-even"><td><p>Future-proof</p></td>
<td><p>❌ Legacy</p></td>
<td><p>✅ Primary</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>LangChain recommends ChatModel for all new systems.</strong></p>
</section>
</section>
<hr class="docutils" />
<section id="conceptual-model">
<h2><a class="toc-backref" href="#id22">Conceptual Model</a><a class="headerlink" href="#conceptual-model" title="Permalink to this heading">#</a></h2>
<section id="id1">
<h3><a class="toc-backref" href="#id23">LLM</a><a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&quot;text in&quot; → model → &quot;text out&quot;
</pre></div>
</div>
</section>
<section id="chatmodel">
<h3><a class="toc-backref" href="#id24">ChatModel</a><a class="headerlink" href="#chatmodel" title="Permalink to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[System, Human, AI] → model → AIMessage
                      ↳ tool_calls
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="llm-text-completion-demonstration">
<h2><a class="toc-backref" href="#id25">LLM (Text Completion) – Demonstration</a><a class="headerlink" href="#llm-text-completion-demonstration" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p><strong>Legacy / compatibility abstraction</strong></p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo-instruct&quot;</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Explain RAG in one sentence&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="limitations">
<h2><a class="toc-backref" href="#id26">Limitations</a><a class="headerlink" href="#limitations" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>No system role</p></li>
<li><p>No memory</p></li>
<li><p>No tools</p></li>
<li><p>No agents</p></li>
<li><p>No structured output</p></li>
</ul>
<p>Used only for:</p>
<ul class="simple">
<li><p>Migration</p></li>
<li><p>Simple batch tasks</p></li>
<li><p>Non-chat legacy models</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="chatmodel-demonstration">
<h2><a class="toc-backref" href="#id27">ChatModel – Demonstration</a><a class="headerlink" href="#chatmodel-demonstration" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p><strong>Primary LangChain abstraction</strong></p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">chat</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Explain RAG in one sentence&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RAG, or Retrieval-Augmented Generation, is a machine learning approach that combines retrieval of relevant information from a knowledge base with generative models to produce more accurate and contextually relevant responses.
</pre></div>
</div>
</div>
</div>
</section>
<section id="what-you-gain-immediately">
<h2><a class="toc-backref" href="#id28">What you gain immediately</a><a class="headerlink" href="#what-you-gain-immediately" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Message roles</p></li>
<li><p>Tool calling</p></li>
<li><p>Function schemas</p></li>
<li><p>Streaming</p></li>
<li><p>Agents</p></li>
<li><p>Multi-turn chat</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="message-objects">
<h2><a class="toc-backref" href="#id29">Message Objects</a><a class="headerlink" href="#message-objects" title="Permalink to this heading">#</a></h2>
<p>ChatModels operate on <strong>messages</strong>, not strings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;You are an IT support assistant&quot;</span><span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Email is down&quot;</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This enables:</p>
<ul class="simple">
<li><p>Instruction hierarchy</p></li>
<li><p>Context control</p></li>
<li><p>Safety rules</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="tool-calling-chatmodel-only">
<h2><a class="toc-backref" href="#id30">Tool Calling (ChatModel-only)</a><a class="headerlink" href="#tool-calling-chatmodel-only" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">ticket_count</span><span class="p">(</span><span class="n">source</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">return</span> <span class="mi">120</span>

<span class="n">chat_with_tools</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">([</span><span class="n">ticket_count</span><span class="p">])</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">chat_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="s2">&quot;How many tickets are there in Jira?&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[{&#39;name&#39;: &#39;ticket_count&#39;, &#39;args&#39;: {&#39;source&#39;: &#39;Jira&#39;}, &#39;id&#39;: &#39;call_PdonKV34RhsI1eJZxSkrES4h&#39;, &#39;type&#39;: &#39;tool_call&#39;}]
</pre></div>
</div>
</div>
</div>
<p>LLMs <strong>cannot do this</strong>.</p>
</section>
<hr class="docutils" />
<section id="structured-output-chatmodel">
<h2><a class="toc-backref" href="#id31">Structured Output (ChatModel)</a><a class="headerlink" href="#structured-output-chatmodel" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Ticket</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">category</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">priority</span><span class="p">:</span> <span class="nb">str</span>

<span class="n">structured_chat</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">Ticket</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">structured_chat</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;CEO cannot login to VPN&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>category=&#39;Technical Support&#39; priority=&#39;High&#39;
</pre></div>
</div>
</div>
</div>
<p>LangChain:</p>
<ul class="simple">
<li><p>Generates schema</p></li>
<li><p>Validates output</p></li>
<li><p>Retries on failure</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="agents-require-chatmodels">
<h2><a class="toc-backref" href="#id32">Agents Require ChatModels</a><a class="headerlink" href="#agents-require-chatmodels" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">ticket_count</span><span class="p">(</span><span class="n">source</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">return</span> <span class="mi">120</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_classic.agents</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AgentExecutor</span><span class="p">,</span>
    <span class="n">create_openai_tools_agent</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tool</span>

<span class="c1"># Define the tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">ticket_count</span><span class="p">(</span><span class="n">source</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the number of tickets in the system.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">120</span>

<span class="n">ticket_tool</span> <span class="o">=</span> <span class="n">Tool</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ticket_count&quot;</span><span class="p">,</span>
    <span class="n">func</span><span class="o">=</span><span class="n">ticket_count</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Counts the number of tickets in the system.&quot;</span>
<span class="p">)</span>

<span class="c1"># Initialize the Chat Model</span>
<span class="n">chat</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Define the prompt template</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;You are an assistant. Use tools if needed.&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{input}</span><span class="s2">&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{agent_scratchpad}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Create the agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">create_openai_tools_agent</span><span class="p">(</span>
    <span class="n">llm</span><span class="o">=</span><span class="n">chat</span><span class="p">,</span>
    <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">ticket_tool</span><span class="p">],</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span>
<span class="p">)</span>

<span class="c1"># Create the executor</span>
<span class="n">executor</span> <span class="o">=</span> <span class="n">AgentExecutor</span><span class="p">(</span>
    <span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span>
    <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">ticket_tool</span><span class="p">],</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># Invoke the agent</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;How many tickets are there in Jira?&quot;</span><span class="p">}</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">&gt; Entering new AgentExecutor chain...</span>

<span class=" -Color -Color-Bold -Color-Bold-Green">Invoking: `ticket_count` with `Jira`</span>


<span class=" -Color -Color-Bold -Color-Bold-Cyan">120</span><span class=" -Color -Color-Bold -Color-Bold-Green">There are 120 tickets in Jira.</span>

<span class=" -Color -Color-Bold">&gt; Finished chain.</span>
There are 120 tickets in Jira.
</pre></div>
</div>
</div>
</div>
<p><strong>Agents will not work with LLMs.</strong></p>
</section>
<hr class="docutils" />
<section id="streaming-comparison">
<h2><a class="toc-backref" href="#id33">Streaming Comparison</a><a class="headerlink" href="#streaming-comparison" title="Permalink to this heading">#</a></h2>
<p><strong>LLM (limited)</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;generator object BaseChatModel.stream at 0x000002988EAC6020&gt;
</pre></div>
</div>
</div>
</div>
</section>
<section id="chatmodel-full">
<h2><a class="toc-backref" href="#id34">ChatModel (full)</a><a class="headerlink" href="#chatmodel-full" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chat</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="s2">&quot;Explain LangChain&quot;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LangChain is a framework designed to facilitate the development of applications that utilize large language models (LLMs). It provides a structured way to build applications that can leverage the capabilities of LLMs for various tasks, such as natural language understanding, text generation, and more. Here are some key components and features of LangChain:

1. **Modular Design**: LangChain is built with a modular architecture, allowing developers to easily integrate different components such as LLMs, data sources, and tools. This modularity makes it easier to customize and extend applications.

2. **Chains**: At the core of LangChain is the concept of &quot;chains,&quot; which are sequences of operations that can be executed in order. For example, a chain might involve taking user input, processing it with an LLM, and then returning a response. Chains can be simple or complex, depending on the application&#39;s requirements.

3. **Agents**: LangChain supports the creation of agents that can make decisions based on user input and context. Agents can use LLMs to interpret commands and determine the best course of action, making them suitable for building conversational agents or chatbots.

4. **Memory**: LangChain includes features for managing memory, allowing applications to maintain context over multiple interactions. This is particularly useful for conversational applications where maintaining context is crucial for providing relevant responses.

5. **Integrations**: LangChain can integrate with various data sources, APIs, and tools, enabling developers to create applications that can pull in external information or perform specific tasks beyond just text generation.

6. **Use Cases**: LangChain can be used for a wide range of applications, including chatbots, virtual assistants, content generation, question-answering systems, and more. Its flexibility allows developers to tailor solutions to specific needs.

7. **Community and Ecosystem**: LangChain has an active community and ecosystem, with resources, documentation, and examples available to help developers get started and share their experiences.

Overall, LangChain aims to simplify the process of building applications that harness the power of large language models, making it easier for developers to create sophisticated and interactive experiences.
</pre></div>
</div>
</div>
</div>
<p>ChatModels support:</p>
<ul class="simple">
<li><p>Token streaming</p></li>
<li><p>Tool streaming</p></li>
<li><p>Partial responses</p></li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>LCEL Compatibility</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>LLM</p></th>
<th class="head"><p>ChatModel</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Runnable</p></td>
<td><p>⚠️ Limited</p></td>
<td><p>✅ Full</p></td>
</tr>
<tr class="row-odd"><td><p>Parallel</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p>Retry</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p>Fallbacks</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="when-should-you-use-llm">
<h2><a class="toc-backref" href="#id35">When Should You Use LLM?</a><a class="headerlink" href="#when-should-you-use-llm" title="Permalink to this heading">#</a></h2>
<p><strong>Rare cases only</strong>:</p>
<ul class="simple">
<li><p>Old completion-only models</p></li>
<li><p>Static text generation</p></li>
<li><p>Offline batch processing</p></li>
<li><p>Migration support</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="when-should-you-use-chatmodel">
<h2><a class="toc-backref" href="#id36">When Should You Use ChatModel?</a><a class="headerlink" href="#when-should-you-use-chatmodel" title="Permalink to this heading">#</a></h2>
<p><strong>Always, if:</strong></p>
<ul class="simple">
<li><p>Chatbot</p></li>
<li><p>RAG</p></li>
<li><p>Tools</p></li>
<li><p>Agents</p></li>
<li><p>Streaming UI</p></li>
<li><p>Production systems</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="why-langchain-split-them">
<h2><a class="toc-backref" href="#id37">Why LangChain Split Them</a><a class="headerlink" href="#why-langchain-split-them" title="Permalink to this heading">#</a></h2>
<p>LangChain reflects <strong>model evolution</strong>:</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Era</p></th>
<th class="head"><p>API Type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Pre-2023</p></td>
<td><p>Text completion</p></td>
</tr>
<tr class="row-odd"><td><p>2023+</p></td>
<td><p>Chat / messages</p></td>
</tr>
<tr class="row-even"><td><p>2024+</p></td>
<td><p>Tools + agents</p></td>
</tr>
</tbody>
</table>
</div>
<hr class="docutils" />
<p><strong>Interview-Ready Answer</strong></p>
<blockquote>
<div><p>“LLM is a legacy text-completion abstraction. ChatModel is LangChain’s primary interface for modern, conversational, tool-aware, agentic LLM systems. All agent, RAG, and production workflows require ChatModels.”</p>
</div></blockquote>
<hr class="docutils" />
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./langchain\core"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="overview.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">LangChain Core</p>
      </div>
    </a>
    <a class="right-next"
       href="02_chat_models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chat Model</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-core-llm-abstractions-in-langchain">Two Core LLM Abstractions in LangChain</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-llm-legacy-text-only">A. <code class="docutils literal notranslate"><span class="pre">LLM</span></code> (Legacy / Text-only)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-chatmodel-primary-modern">B. <code class="docutils literal notranslate"><span class="pre">ChatModel</span></code> (Primary / Modern)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mental-model">Mental Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-llm-usage-chat-model">Basic LLM Usage (Chat Model)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-openai-via-langchain">Example: OpenAI via LangChain</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-langchain-adds">What LangChain adds</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-templates-llm-langchain-style">Prompt Templates + LLM (LangChain Style)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lcel-langchain-expression-language">LCEL (LangChain Expression Language)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-with-structured-output">LLM with Structured Output</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-with-tools-key-differentiator">LLM with Tools (Key Differentiator)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-in-rag-retriever-llm">LLM in RAG (Retriever + LLM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#streaming-tokens">Streaming Tokens</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-lifecycle-in-langchain">LLM Lifecycle in LangChain</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-langchain-uses-llm-abstractions">Why LangChain Uses LLM Abstractions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interview-ready-summary">Interview-Ready Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-what">When to Use What</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-v-s-chatmodel">LLM v/s ChatModel</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#abstraction-level">Abstraction Level</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conceptual-model">Conceptual Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">LLM</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chatmodel">ChatModel</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-text-completion-demonstration">LLM (Text Completion) – Demonstration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations">Limitations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chatmodel-demonstration">ChatModel – Demonstration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-you-gain-immediately">What you gain immediately</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#message-objects">Message Objects</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tool-calling-chatmodel-only">Tool Calling (ChatModel-only)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#structured-output-chatmodel">Structured Output (ChatModel)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#agents-require-chatmodels">Agents Require ChatModels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#streaming-comparison">Streaming Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chatmodel-full">ChatModel (full)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-should-you-use-llm">When Should You Use LLM?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-should-you-use-chatmodel">When Should You Use ChatModel?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-langchain-split-them">Why LangChain Split Them</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By svgoudar
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>